{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "title: \"(연구&재인) MBTI -- 감성분석\"\n",
    "author: \"신록예찬\"\n",
    "date: \"10/11/2023\"\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43787</th>\n",
       "      <td>very pleasing at its best moments</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16159</th>\n",
       "      <td>, american chai is enough to make you put away...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59015</th>\n",
       "      <td>too much like an infomercial for ram dass 's l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5108</th>\n",
       "      <td>a stirring visual sequence</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67052</th>\n",
       "      <td>cool visual backmasking</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35938</th>\n",
       "      <td>hard ground</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49879</th>\n",
       "      <td>the striking , quietly vulnerable personality ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51591</th>\n",
       "      <td>pan nalin 's exposition is beautiful and myste...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56780</th>\n",
       "      <td>wonderfully loopy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28518</th>\n",
       "      <td>most beautiful , evocative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  label\n",
       "43787                 very pleasing at its best moments       1\n",
       "16159  , american chai is enough to make you put away...      0\n",
       "59015  too much like an infomercial for ram dass 's l...      0\n",
       "5108                         a stirring visual sequence       1\n",
       "67052                           cool visual backmasking       1\n",
       "35938                                       hard ground       0\n",
       "49879  the striking , quietly vulnerable personality ...      1\n",
       "51591  pan nalin 's exposition is beautiful and myste...      1\n",
       "56780                                 wonderfully loopy       1\n",
       "28518                        most beautiful , evocative       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogluon.core.utils.loaders import load_pd\n",
    "train_data = load_pd.load('https://autogluon-text.s3-accelerate.amazonaws.com/glue/sst/train.parquet')\n",
    "test_data = load_pd.load('https://autogluon-text.s3-accelerate.amazonaws.com/glue/sst/dev.parquet')\n",
    "subsample_size = 1000  # subsample data for faster demo, try setting this to larger values\n",
    "train_data = train_data.sample(n=subsample_size, random_state=0)\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_cuda.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/core/utils/utils.py:564: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context(\"mode.use_inf_as_na\", True):  # treat None, NaN, INF, NINF as NA\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [0, 1]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Global seed set to 0\n",
      "AutoMM starts to create your model. ✨\n",
      "\n",
      "- AutoGluon version is 0.8.2.\n",
      "\n",
      "- Pytorch version is 2.0.0.post2.\n",
      "\n",
      "- Model will be saved to \"/home/cgb2/Dropbox/03_yechan3/tmp/ec6bab7c48a341ed99029ebb6094a01a-automm_sst\".\n",
      "\n",
      "- Validation metric is \"acc\".\n",
      "\n",
      "- To track the learning progress, you can open a terminal and launch Tensorboard:\n",
      "    ```shell\n",
      "    # Assume you have installed tensorboard\n",
      "    tensorboard --logdir /home/cgb2/Dropbox/03_yechan3/tmp/ec6bab7c48a341ed99029ebb6094a01a-automm_sst\n",
      "    ```\n",
      "\n",
      "Enjoy your coffee, and let AutoMM do the job ☕☕☕ Learn more at https://auto.gluon.ai\n",
      "\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 666/666 [00:00<00:00, 1.35MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 440M/440M [00:21<00:00, 20.0MB/s] \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 27.0/27.0 [00:00<00:00, 59.8kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.16MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 58.6MB/s]\n",
      "0 GPUs are detected, and 0 GPUs will be used.\n",
      "\n",
      "/home/cgb2/anaconda3/envs/ag/lib/python3.10/site-packages/autogluon/multimodal/utils/environment.py:91: UserWarning: Only CPU is detected in the instance. This may result in slow speed for MultiModalPredictor. Consider using an instance with GPU support.\n",
      "  warnings.warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name              | Type                         | Params\n",
      "-------------------------------------------------------------------\n",
      "0 | model             | HFAutoModelForTextPrediction | 108 M \n",
      "1 | validation_metric | MulticlassAccuracy           | 0     \n",
      "2 | loss_func         | CrossEntropyLoss             | 0     \n",
      "-------------------------------------------------------------------\n",
      "108 M     Trainable params\n",
      "0         Non-trainable params\n",
      "108 M     Total params\n",
      "435.573   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  50%|█████     | 50/100 [00:38<00:38,  1.29it/s]                  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 3: 'val_acc' reached 0.46500 (best 0.46500), saving model to '/home/cgb2/Dropbox/03_yechan3/tmp/ec6bab7c48a341ed99029ebb6094a01a-automm_sst/epoch=0-step=3.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 100/100 [01:25<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 7: 'val_acc' reached 0.57000 (best 0.57000), saving model to '/home/cgb2/Dropbox/03_yechan3/tmp/ec6bab7c48a341ed99029ebb6094a01a-automm_sst/epoch=0-step=7.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  50%|█████     | 50/100 [00:43<00:43,  1.14it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 10: 'val_acc' reached 0.77500 (best 0.77500), saving model to '/home/cgb2/Dropbox/03_yechan3/tmp/ec6bab7c48a341ed99029ebb6094a01a-automm_sst/epoch=1-step=10.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 100/100 [01:30<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 14: 'val_acc' reached 0.79500 (best 0.79500), saving model to '/home/cgb2/Dropbox/03_yechan3/tmp/ec6bab7c48a341ed99029ebb6094a01a-automm_sst/epoch=1-step=14.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/100 [00:00<?, ?it/s]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time limit reached. Elapsed time is 0:03:01. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:   1%|          | 1/100 [00:06<10:45,  6.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Start to fuse 3 checkpoints via the greedy soup algorithm.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting DataLoader 0: 100%|██████████| 7/7 [00:05<00:00,  1.25it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 7/7 [00:05<00:00,  1.29it/s]\n",
      "Predicting DataLoader 0: 100%|██████████| 7/7 [00:05<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AutoMM has created your model 🎉🎉🎉\n",
      "\n",
      "- To load the model, use the code below:\n",
      "    ```python\n",
      "    from autogluon.multimodal import MultiModalPredictor\n",
      "    predictor = MultiModalPredictor.load(\"/home/cgb2/Dropbox/03_yechan3/tmp/ec6bab7c48a341ed99029ebb6094a01a-automm_sst\")\n",
      "    ```\n",
      "\n",
      "- You can open a terminal and launch Tensorboard to visualize the training log:\n",
      "    ```shell\n",
      "    # Assume you have installed tensorboard\n",
      "    tensorboard --logdir /home/cgb2/Dropbox/03_yechan3/tmp/ec6bab7c48a341ed99029ebb6094a01a-automm_sst\n",
      "    ```\n",
      "\n",
      "- If you are not satisfied with the model, try to increase the training time, \n",
      "adjust the hyperparameters (https://auto.gluon.ai/stable/tutorials/multimodal/advanced_topics/customization.html),\n",
      "or post issues on GitHub: https://github.com/autogluon/autogluon\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.multimodal.predictor.MultiModalPredictor at 0x7f8acd4349a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogluon.multimodal import MultiModalPredictor\n",
    "import uuid\n",
    "model_path = f\"./tmp/{uuid.uuid4().hex}-automm_sst\"\n",
    "predictor = MultiModalPredictor(label='label', eval_metric='acc', path=model_path)\n",
    "predictor.fit(train_data, time_limit=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Sentence\": it's a charming and often affecting journey. \"Predicted Sentiment\": 1\n",
      "\"Sentence\": It's slow, very, very, very slow. \"Predicted Sentiment\": 0\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"it's a charming and often affecting journey.\"\n",
    "sentence2 = \"It's slow, very, very, very slow.\"\n",
    "predictions = predictor.predict({'sentence': [sentence1, sentence2]})\n",
    "print('\"Sentence\":', sentence1, '\"Predicted Sentiment\":', predictions[0])\n",
    "print('\"Sentence\":', sentence2, '\"Predicted Sentiment\":', predictions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
