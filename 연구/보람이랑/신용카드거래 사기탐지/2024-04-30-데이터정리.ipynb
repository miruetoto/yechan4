{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d882f611-5dae-4866-af25-e032df6cd57f",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"(연구&보람) 데이터정리\"\n",
    "author: \"김보람\"\n",
    "date: \"04/24/2024\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a61e0ef-99f8-4745-ae02-fce5900ba4fe",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d41752-1bab-4f48-b98c-e2de3fe495d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "import itertools\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00669038-e135-4610-92ce-a419c2cf51ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def throw(df, fraud_rate, random_state=42):\n",
    "    # Separate fraud and non-fraud transactions and make copies\n",
    "    df1 = df[df['is_fraud'] == 1].copy()  # Fraud transactions\n",
    "    df0 = df[df['is_fraud'] == 0].copy()  # Non-fraud transactions\n",
    "    \n",
    "    # Downsample non-fraud transactions\n",
    "    # Downsample the non-fraud transactions considering the fraud rate\n",
    "    df0_downsample = (len(df1) * (1 - fraud_rate)) / (len(df0) * fraud_rate)\n",
    "    df0_down = df0.sample(frac=df0_downsample, random_state=random_state)\n",
    "    \n",
    "    # Concatenate fraud transactions and downsampled non-fraud transactions\n",
    "    df_p = pd.concat([df1, df0_down])\n",
    "    \n",
    "    # Return the concatenated DataFrame\n",
    "    return df_p\n",
    "    \n",
    "def split_dataframe(data_frame, test_fraud_rate, test_rate=0.3, random_state=42):\n",
    "    # Split the data into 70:30 ratio with test_fraud_rate as input\n",
    "    n = len(data_frame)\n",
    "\n",
    "    # Separate fraud and non-fraud transactions\n",
    "    fraud_data = data_frame[data_frame['is_fraud'] == 1]\n",
    "    normal_data = data_frame[data_frame['is_fraud'] == 0]\n",
    "\n",
    "    # Calculate the size of the test data\n",
    "    test_samples = int(test_fraud_rate * (n * test_rate))\n",
    "    remaining_test_samples = int(n * test_rate) - test_samples\n",
    "\n",
    "    # Randomly sample test data from fraud and non-fraud transactions\n",
    "    test_fraud_data = fraud_data.sample(n=test_samples, replace=False, random_state=random_state)\n",
    "    test_normal_data = normal_data.sample(n=remaining_test_samples, replace=False, random_state=random_state)\n",
    "\n",
    "    # Concatenate test data\n",
    "    test_data = pd.concat([test_normal_data, test_fraud_data])\n",
    "\n",
    "    # Create training data\n",
    "    train_data = data_frame[~data_frame.index.isin(test_data.index)]\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "860810ec-5375-4faa-8945-00ffcbc4a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN에서 mask를 위한 함수 \n",
    "def _concat(df_tr, df_tst):   \n",
    "    df = pd.concat([df_tr, df_tst])\n",
    "    train_mask = np.concatenate((np.full(len(df_tr), True), np.full(len(df_tst), False)))    # index꼬이는거 방지하기 위해서? ★ (이거,, 훔,,?(\n",
    "    test_mask =  np.concatenate((np.full(len(df_tr), False), np.full(len(df_tst), True))) \n",
    "    mask = (train_mask, test_mask)\n",
    "    return df, mask\n",
    "\n",
    "# 거래시간차이 계산 \n",
    "def _compute_time_difference(sub_df):\n",
    "    t = sub_df.unix_time.to_numpy() \n",
    "    t_diff = abs(t.reshape(-1,1) - t.reshape(1,-1)).reshape(-1,1)\n",
    "    idx = np.array(list(itertools.product(sub_df.index,sub_df.index)))\n",
    "    result = np.concatenate([idx,t_diff],axis=1)\n",
    "    return result\n",
    "\n",
    "def _parallel_apply(func, data_frames, num_processes=None):\n",
    "    \"\"\"\n",
    "    Apply a function to a list of DataFrames in parallel.\n",
    "    \n",
    "    :param func: The function to apply.\n",
    "    :param data_frames: The list of DataFrames to process.\n",
    "    :param num_processes: The number of processes to use.\n",
    "    :return: A list containing the processed results.\n",
    "    \"\"\"\n",
    "    if num_processes is None:\n",
    "        num_processes = multiprocessing.cpu_count()  # Number of available CPU cores\n",
    "\n",
    "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "        results = pool.map(func, data_frames)\n",
    "\n",
    "    return results\n",
    "    \n",
    "def df2geodata(df_train,df_test,theta,gamma):\n",
    "    df2, mask = _concat(df_train, df_test)   \n",
    "    df = df2.reset_index()\n",
    "    groups = df.groupby('cc_num')\n",
    "    data_frames = [sub_df for _,sub_df in groups]  # 여기에 데이터프레임들을 리스트로 \n",
    "    processed_results = _parallel_apply(_compute_time_difference, data_frames)\n",
    "    processed_results = np.concatenate(processed_results).astype(np.float64)\n",
    "    edge_index = processed_results[:,:2]\n",
    "    dist = processed_results[:,2]\n",
    "    weight = (dist != 0) * np.exp(-dist/theta)\n",
    "    edge_index = torch.tensor(edge_index[weight > gamma]).long().t()    \n",
    "    x = torch.tensor(df['amt'].values, dtype=torch.float).reshape(-1,1)\n",
    "    y = torch.tensor(df['is_fraud'].values,dtype=torch.int64)\n",
    "    data = torch_geometric.data.Data(x=x, edge_index = edge_index, y=y, train_mask = mask[0], test_mask= mask[1])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f073dc-30e1-491b-8a0b-df96bd5c4b6e",
   "metadata": {},
   "source": [
    "# 2. Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dd4b71e-0153-4379-89b2-0ce9eb57ebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fraudTrain.pkl', 'rb') as file:\n",
    "    fraudTrain = pickle.load(file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccf123fc-8a88-4a38-a6d1-89bf8c46fffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6006"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraudTrain.is_fraud.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff9b81a4-3c26-4510-a8e2-b3fc081870f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005727773406766326"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FRAUD_RATE = fraudTrain.is_fraud.mean()\n",
    "FRAUD_RATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da03a2ac-910d-462f-a343-88b5e5c9bb0d",
   "metadata": {},
   "source": [
    "# 3. 데이터의 정리 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f22168-ab48-4424-912a-47a508c8c8a2",
   "metadata": {},
   "source": [
    "## A. 분리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e488c1a-a2bc-425c-82df-68a6a3de1ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train1, df_test = split_dataframe(fraudTrain,test_fraud_rate=FRAUD_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f834cee-fa3c-47e7-8d6a-725d146093fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2 = throw(df_train1, fraud_rate=0.01) \n",
    "df_train3 = throw(df_train1, fraud_rate=0.05) \n",
    "df_train4 = throw(df_train1, fraud_rate=0.1) \n",
    "df_train5 = throw(df_train1, fraud_rate=0.2) \n",
    "df_train6 = throw(df_train1, fraud_rate=0.3) \n",
    "df_train7 = throw(df_train1, fraud_rate=0.4) \n",
    "df_train8 = throw(df_train1, fraud_rate=0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d0847b5-9713-4c01-8be1-53c8718d0c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<df1>\n",
      "training set := (734003, 22), 0.0057, 4205\n",
      "test set := (314572, 22), 0.0057, 1801\n",
      "<df2>\n",
      "training set := (420500, 22), 0.0100, 4205\n",
      "test set := (314572, 22), 0.0057, 1801\n",
      "<df3>\n",
      "training set := (84100, 22), 0.0500, 4205\n",
      "test set := (314572, 22), 0.0057, 1801\n",
      "<df4>\n",
      "training set := (42050, 22), 0.1000, 4205\n",
      "test set := (314572, 22), 0.0057, 1801\n",
      "<df5>\n",
      "training set := (21025, 22), 0.2000, 4205\n",
      "test set := (314572, 22), 0.0057, 1801\n",
      "<df6>\n",
      "training set := (14017, 22), 0.3000, 4205\n",
      "test set := (314572, 22), 0.0057, 1801\n",
      "<df2>\n",
      "training set := (10512, 22), 0.4000, 4205\n",
      "test set := (314572, 22), 0.0057, 1801\n",
      "<df8>\n",
      "training set := (8410, 22), 0.5000, 4205\n",
      "test set := (314572, 22), 0.0057, 1801\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"<df1>\n",
    "training set := {df_train1.shape}, {df_train1.is_fraud.mean():.4f}, {df_train1.is_fraud.sum()}\n",
    "test set := {df_test.shape}, {df_test.is_fraud.mean():.4f}, {df_test.is_fraud.sum()}\n",
    "<df2>\n",
    "training set := {df_train2.shape}, {df_train2.is_fraud.mean():.4f}, {df_train1.is_fraud.sum()}\n",
    "test set := {df_test.shape}, {df_test.is_fraud.mean():.4f}, {df_test.is_fraud.sum()}\n",
    "<df3>\n",
    "training set := {df_train3.shape}, {df_train3.is_fraud.mean():.4f}, {df_train1.is_fraud.sum()}\n",
    "test set := {df_test.shape}, {df_test.is_fraud.mean():.4f}, {df_test.is_fraud.sum()}\n",
    "<df4>\n",
    "training set := {df_train4.shape}, {df_train4.is_fraud.mean():.4f}, {df_train1.is_fraud.sum()}\n",
    "test set := {df_test.shape}, {df_test.is_fraud.mean():.4f}, {df_test.is_fraud.sum()}\n",
    "<df5>\n",
    "training set := {df_train5.shape}, {df_train5.is_fraud.mean():.4f}, {df_train1.is_fraud.sum()}\n",
    "test set := {df_test.shape}, {df_test.is_fraud.mean():.4f}, {df_test.is_fraud.sum()}\n",
    "<df6>\n",
    "training set := {df_train6.shape}, {df_train6.is_fraud.mean():.4f}, {df_train1.is_fraud.sum()}\n",
    "test set := {df_test.shape}, {df_test.is_fraud.mean():.4f}, {df_test.is_fraud.sum()}\n",
    "<df2>\n",
    "training set := {df_train7.shape}, {df_train7.is_fraud.mean():.4f}, {df_train1.is_fraud.sum()}\n",
    "test set := {df_test.shape}, {df_test.is_fraud.mean():.4f}, {df_test.is_fraud.sum()}\n",
    "<df8>\n",
    "training set := {df_train8.shape}, {df_train8.is_fraud.mean():.4f}, {df_train1.is_fraud.sum()}\n",
    "test set := {df_test.shape}, {df_test.is_fraud.mean():.4f}, {df_test.is_fraud.sum()}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d91942-b88d-48d7-9435-102aab2d01a6",
   "metadata": {},
   "source": [
    "## B. DF저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e680e6e-e51b-4d56-9a46-749fc78f5839",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train1.to_csv(\"./data/df_train1.csv\",index=False)\n",
    "df_train2.to_csv(\"./data/df_train2.csv\",index=False)\n",
    "df_train3.to_csv(\"./data/df_train3.csv\",index=False)\n",
    "df_train4.to_csv(\"./data/df_train4.csv\",index=False)\n",
    "df_train5.to_csv(\"./data/df_train5.csv\",index=False)\n",
    "df_train6.to_csv(\"./data/df_train6.csv\",index=False)\n",
    "df_train7.to_csv(\"./data/df_train7.csv\",index=False)\n",
    "df_train8.to_csv(\"./data/df_train8.csv\",index=False)\n",
    "df_test.to_csv(\"./data/df_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6940a14b-edab-49a9-9807-9456808c0874",
   "metadata": {},
   "source": [
    "## C. Make_Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efdcebc4-316b-4e91-98a9-35e0a16b2600",
   "metadata": {},
   "outputs": [],
   "source": [
    "THETA = 1e7\n",
    "GAMMA = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "608500bb-3231-41b7-995a-da31b8367c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_geometric_data1 = df2geodata(df_train1,df_test,theta=THETA,gamma=GAMMA)\n",
    "torch_geometric_data2 = df2geodata(df_train2,df_test,theta=THETA,gamma=GAMMA)\n",
    "torch_geometric_data3 = df2geodata(df_train3,df_test,theta=THETA,gamma=GAMMA)\n",
    "torch_geometric_data4 = df2geodata(df_train4,df_test,theta=THETA,gamma=GAMMA)\n",
    "torch_geometric_data5 = df2geodata(df_train5,df_test,theta=THETA,gamma=GAMMA)\n",
    "torch_geometric_data6 = df2geodata(df_train6,df_test,theta=THETA,gamma=GAMMA)\n",
    "torch_geometric_data7 = df2geodata(df_train7,df_test,theta=THETA,gamma=GAMMA)\n",
    "torch_geometric_data8 = df2geodata(df_train8,df_test,theta=THETA,gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec66b44a-9ad0-4eef-81ef-cd93ba1a5c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_geometric_data_list = [\n",
    "    torch_geometric_data1,\n",
    "    torch_geometric_data2,\n",
    "    torch_geometric_data3,\n",
    "    torch_geometric_data4,\n",
    "    torch_geometric_data5,\n",
    "    torch_geometric_data6,\n",
    "    torch_geometric_data7,\n",
    "    torch_geometric_data8\n",
    "]\n",
    "df_train_list = [\n",
    "    df_train1,\n",
    "    df_train2,\n",
    "    df_train3,\n",
    "    df_train4,\n",
    "    df_train5,\n",
    "    df_train6,\n",
    "    df_train7,\n",
    "    df_train8\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6f4fde7-ba1f-4563-92e7-3e6d272548d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, df_train in zip(torch_geometric_data_list,df_train_list):\n",
    "    data._train_size = len(df_train)\n",
    "    data._train_frate = df_train.is_fraud.mean()\n",
    "    data._test_size = len(df_test)\n",
    "    data._test_frate = df_test.is_fraud.mean()\n",
    "    data._theta = THETA\n",
    "    data._gamma = GAMMA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b22a05ae-b999-4f25-889d-92dd330b0bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i,data) in enumerate(torch_geometric_data_list):\n",
    "    with open(f'data/torch_geometric_data{i+1}_{THETA:.1e}_{GAMMA}.pkl', 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
