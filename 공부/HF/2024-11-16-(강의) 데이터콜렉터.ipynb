{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"(강의) 데이터콜렉터\"\n",
    "author: \"신록예찬\"\n",
    "date: \"11/16/2024\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 2. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "```Python\n",
    "#!pip uninstall mp2024pkg -y\n",
    "#!pip install git+https://github.com/guebin/mp2024pkg.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets \n",
    "import transformers\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.utils\n",
    "import evaluate\n",
    "from rich import print as rprint\n",
    "from mp2024pkg import show, tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. `data_collator` 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. 외우세요 $(\\star\\star\\star)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` `data_collator`를 잘 설계하는 방법: `trainer_input`과 `model`이 주어졌을때 `data_collator`는 아래의 코드가 동작하도록 설계하면 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "trainer_input = ~~~\n",
    "model = ~~~~ \n",
    "batch_maker = transformers.Trainer(\n",
    "    model = model,\n",
    "    data_collator = lambda x: x\n",
    ") # 이 과정에서 model이 cuda로 감 \n",
    "_batched_data = batch_maker.get_test_dataloader(trainer_input) # 이 과정에서 trainer_input이 cuda로 감\n",
    "batched_data = list(_batched_data)\n",
    "single_batch = batched_data[0]\n",
    "model.to(\"cpu\") # 경우에 따라 생략해야할수도있음\n",
    "model(**data_collator(single_batch))\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 위의 코드가 오류없이 실행되었다면 아래의 코드를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "trainer = transformers.Trainer(\n",
    "    model = model,\n",
    "    data_collator = data_collator\n",
    ")\n",
    "trainer.predict(trainer_input)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 이걸 어떻게 알았냐고요? 코드뜯어봤습니다.. $\\to$ 숙제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "코랩사용자의 경우 아래와 같이 wandb(Weights & Biases) 로그인을 요구하는 문제가 있습니다. \n",
    "```bash\n",
    "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
    "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
    "wandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
    "wandb: You can find your API key in your browser here: https://wandb.ai/authorize\n",
    "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\n",
    "```\n",
    "이를 해결하기 위해서는 아래의 코드를 코랩처음에 실행하면 됩니다. \n",
    "\n",
    "```Python\n",
    "import os\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "\n",
    "주의: `data`의 type이 꼭 `Dataset` 일 필요는 없다..\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. IMDB -- 복습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref: <https://huggingface.co/docs/transformers/tasks/sequence_classification>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*1. 데이터준비: `\"guebin/imdb-tiny\"` $\\to$ `trainer_input`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = datasets.load_dataset(\"guebin/imdb-tiny\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\") \n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "tokenized_imdb = imdb.map(preprocess_function,batched=True)\n",
    "trainer_input = tokenized_imdb['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2. 모델준비: `\"distilbert/distilbert-base-uncased\"` $\\to$`model`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*3. 데이터콜렉터: `DataCollatorWithPadding()` $\\to$ `data_collator`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorWithPadding(tokenizer=DistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}, padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터콜렉터가 올바로 설정되었는지 체크하고, 적당한 `trainer`를 만들어 \n",
    "\n",
    "```Python\n",
    "trainer.predict(trainer_input)\n",
    "```\n",
    "\n",
    "이 정상동작하는지 확인하라. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(풀이)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.6714, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0168, -0.0577],\n",
       "        [ 0.0035, -0.0523],\n",
       "        [-0.0044, -0.0638],\n",
       "        [ 0.0191, -0.0579],\n",
       "        [ 0.0050, -0.0271],\n",
       "        [ 0.0182, -0.0258],\n",
       "        [ 0.0243, -0.0059],\n",
       "        [ 0.0056, -0.0080]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_maker = transformers.Trainer(\n",
    "    model = model,\n",
    "    data_collator = lambda x: x,\n",
    ") # 이 과정에서 model이 cuda로 감 \n",
    "_batched_data = batch_maker.get_test_dataloader(trainer_input) # 이 과정에서 trainer_input이 cuda로 감\n",
    "batched_data = list(_batched_data)\n",
    "single_batch = batched_data[0]\n",
    "model.to(\"cpu\") # 경우에 따라 생략해야할수도있음\n",
    "model(**data_collator(single_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 잘 돌아감..\n",
    "- 잘 설계된 data_collator라는 의미\n",
    "- 이걸 이용해서 진짜 trainer를 만들자.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-0.01682485, -0.05772787],\n",
       "       [ 0.00350759, -0.05234354],\n",
       "       [-0.0044021 , -0.06378944],\n",
       "       [ 0.01906627, -0.05794427],\n",
       "       [ 0.00501909, -0.02714018],\n",
       "       [ 0.0182183 , -0.02577444],\n",
       "       [ 0.02433074, -0.00588998],\n",
       "       [ 0.00561093, -0.00798136],\n",
       "       [ 0.015525  , -0.01578641],\n",
       "       [ 0.03086514, -0.00874608]], dtype=float32), label_ids=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), metrics={'test_loss': 0.6722059845924377, 'test_model_preparation_time': 0.0015, 'test_runtime': 0.2214, 'test_samples_per_second': 45.168, 'test_steps_per_second': 9.034})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model = model,\n",
    "    data_collator = data_collator\n",
    ")\n",
    "out = trainer.predict(trainer_input)\n",
    "out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-`  관찰1: 여기에서 `batched_data[-1]`은 하나의 배치를 의미, 그런데 모델의 입력으로 사용하기에는 형식이 맞지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batched_data[-1] -- 안될것같은 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): DistilBertSdpaAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n) argument after ** must be a mapping, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# 경우에 따라 생략해야할수도있음\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatched_data[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): DistilBertSdpaAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n) argument after ** must be a mapping, not list"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\") # 경우에 따라 생략해야할수도있음\n",
    "model(**batched_data[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 관찰2: 여기에서 `data_collator(batched_data[-1])` 역시 하나의 배치를 의미. 이번에는 형식이 잘 맞음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_collator(batched_data[-1]) -- 될 것 같은 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.6756, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0155, -0.0158],\n",
       "        [ 0.0309, -0.0087]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\") # 경우에 따라 생략해야할수도있음\n",
    "model(**data_collator(batched_data[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "### `data_collator` -- 심화이해\n",
    "\n",
    "아래의 형식으로 정리된 배치화된 자료가 있다고 하자. (주의: `batched_data`는 항상 list비슷한 오브젝트이어야함)\n",
    "\n",
    "```Python\n",
    "batched_data = [batch_1, batch_2, ...,batch_n]\n",
    "```\n",
    "\n",
    "`data_collator` 는 각각의 `single_batch`, 즉 `batch_1`, `batch_2` 등을 `model`이 처리가능한 형태로 \"형식\"을 맞춰주는 역할을 한다. 즉 아래가 실행되도록 만들어주는 역할을 한다. \n",
    "\n",
    "```Python\n",
    "model(**data_collator(batch_1))\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "### `trainer`와 `model`의 자료처리과정 비교\n",
    "\n",
    "***#. `model`의 자료처리과정*** \n",
    "\n",
    "-코드: `model.forward(model_input)`\n",
    "\n",
    "-처리과정: `model_input`에 정리된 입력을 단순히 `model.forward()` 함수가 처리. \n",
    "\n",
    "***#. `trainer`의 자료처리과정***\n",
    "\n",
    "-코드: `trainer.predict(trainer_input)`\n",
    "\n",
    "-처리과정: 크게 배치화 $\\to$ 데이터콜렉팅 $\\to$ 추론 의 과정을 거친다. \n",
    "\n",
    "1. `trainer_input`을 배치(batch)로 나눈다.\n",
    "2.\t각 배치(=`single_batch`)를 `data_collator`를 통해 형식을 맞춘다. \n",
    "3.\t형식이 조정된 데이터를 `model.forward`의 입력으로 전달한다. \n",
    "\n",
    "-슈도코드:\n",
    "```Python\n",
    "## 이 코드는.. \n",
    "trainer.predict(trainer_input)\n",
    "\n",
    "## 대략 아래의 느낌으로 해석하면 된다.. (동일X. 결과정리, GPU처리 등 세부로직이 더 있음)\n",
    "batched_data = some_function(trainer_input)\n",
    "for single_batch in batched_data:\n",
    "    collated_data = data_collator(single_batch)\n",
    "    model(**collated_data)\n",
    "```\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "### `trainer.predict()` 의 분해\n",
    "\n",
    "`trainer.predict()`의 동작은 개념적으로 (1) 배치화 (2) 데이터콜렝팅 (3) 추론의 과정으로 분해할 수 있지만, 실제이러한 과정으로 코드를 정확하게 분리하는건 어렵다. (실제로도 별로 그럴 이유가 없다) 하지만 이해를 위해서 코드조각을 분리할 필요가 있는데 아래의 3개 코드조각은 이러한 분해를 최대한 비슷하게 수동구현한 것이다. \n",
    "\n",
    "`1`. 배치화: `trainer_input` $\\to$ `batched_data`\n",
    "\n",
    "```Python\n",
    "batch_maker = transformers.Trainer(\n",
    "    model = model,\n",
    "    data_collator = lambda x: x\n",
    ")\n",
    "_batched_data = batch_maker.get_test_dataloader(trainer_input)\n",
    "batched_data = list(_batched_data)\n",
    "```\n",
    "\n",
    "`2`. 데이터콜렉팅: `single_batch` $\\to$ `collated_data`\n",
    "\n",
    "```Python\n",
    "#for single_batch in batched_data:\n",
    "    collated_data = data_collator(single_batch)\n",
    "```\n",
    "\n",
    "`3`. 추론: `collated_data` $\\to$ `model_out`\n",
    "\n",
    "```Python\n",
    "#for single_batch in batched_data:\n",
    "    #collated_data = data_collator(single_batch)\n",
    "    model_out = model(**collated_data)\n",
    "```\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. FOOD101 -- 복습 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref: <https://huggingface.co/docs/transformers/tasks/image_classification>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*1. 데이터준비: `\"guebin/food101-tiny\"` $\\to$ `trainer_input`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food = datasets.load_dataset(\"guebin/food101-tiny\")\n",
    "image_processor = transformers.AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "normalize = torchvision.transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomResizedCrop(size), \n",
    "    torchvision.transforms.ToTensor(), \n",
    "    normalize\n",
    "])\n",
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples\n",
    "trainer_input = food['train'].with_transform(transforms)\n",
    "trainer_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2. 모델준비: `\"google/vit-base-patch16-224-in21k\"` $\\to$`model`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "labels = food[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "model = transformers.AutoModelForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*3. 데이터콜렉터: `DefaultDataCollator()` $\\to$ `data_collator`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DefaultDataCollator(return_tensors='pt')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = transformers.DefaultDataCollator()\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터콜렉터가 올바로 설정되었는지 체크하고, 적당한 `trainer`를 만들어 \n",
    "\n",
    "```Python\n",
    "trainer.predict(trainer_input)\n",
    "```\n",
    "\n",
    "이 정상동작하는지 확인하라. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(풀이1)` -- 실패"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'image'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m batch_maker \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m      2\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m      3\u001b[0m     data_collator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x\n\u001b[1;32m      4\u001b[0m ) \u001b[38;5;66;03m# 이 과정에서 model이 cuda로 감 \u001b[39;00m\n\u001b[1;32m      5\u001b[0m _batched_data \u001b[38;5;241m=\u001b[39m batch_maker\u001b[38;5;241m.\u001b[39mget_test_dataloader(trainer_input) \u001b[38;5;66;03m# 이 과정에서 trainer_input이 cuda로 감\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m batched_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_batched_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m single_batch \u001b[38;5;241m=\u001b[39m batched_data[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# 경우에 따라 생략해야할수도있음\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/accelerate/data_loader.py:550\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/datasets/arrow_dataset.py:2865\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2863\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: List) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[1;32m   2864\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2865\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2866\u001b[0m     n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[1;32m   2867\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/datasets/arrow_dataset.py:2861\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2859\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2860\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2861\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/datasets/arrow_dataset.py:2846\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2844\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2845\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[0;32m-> 2846\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[1;32m   2848\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2849\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/datasets/formatting/formatting.py:633\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    631\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/datasets/formatting/formatting.py:401\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/datasets/formatting/formatting.py:516\u001b[0m, in \u001b[0;36mCustomFormatter.format_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    514\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_batch(pa_table)\n\u001b[1;32m    515\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_batch(batch)\n\u001b[0;32m--> 516\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 15\u001b[0m, in \u001b[0;36mtransforms\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransforms\u001b[39m(examples):\n\u001b[0;32m---> 15\u001b[0m     examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [_transforms(img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m \u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m]\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m examples\n",
      "\u001b[0;31mKeyError\u001b[0m: 'image'"
     ]
    }
   ],
   "source": [
    "batch_maker = transformers.Trainer(\n",
    "    model = model,\n",
    "    data_collator = lambda x: x\n",
    ") # 이 과정에서 model이 cuda로 감 \n",
    "_batched_data = batch_maker.get_test_dataloader(trainer_input) # 이 과정에서 trainer_input이 cuda로 감\n",
    "batched_data = list(_batched_data)\n",
    "single_batch = batched_data[0]\n",
    "model.to(\"cpu\") # 경우에 따라 생략해야할수도있음\n",
    "model(**data_collator(single_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 왜 실패했지?? (예전에는 분명히 되었던 것 같은뎅..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "**<에러메시지의 해석>**\n",
    "\n",
    "`-` 아래가 동작하지 않음. \n",
    "\n",
    "```Python\n",
    "batched_data = list(_batched_data)\n",
    "```\n",
    "\n",
    "`-` 그 이유는 아래가 동작하지 않기 때문임. \n",
    "\n",
    "```Python \n",
    "next(dataloader_iter)\n",
    "```\n",
    "\n",
    "`-` ...(생략)...\n",
    "\n",
    "`-` 최종적으로는 아래가 동작하지 않기 때문에 생긴 문제였음. (그런데 이건 `.with_transform()`에 있는 코드인데?)\n",
    "\n",
    "```Python\n",
    "examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "```\n",
    "\n",
    "`-` 결국 \n",
    "\n",
    "```Python\n",
    "[_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "```\n",
    "\n",
    "를 실행하는 시점에서 `examples[\"image\"]`가 없었다는 의미.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 눈치: `with_transform`이 지금 실행되는거였어?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 왜 이런일이 생기지? \n",
    "\n",
    "`-` 배치화를 하는 코드 \n",
    "\n",
    "```Python\n",
    "_batched_data = batch_maker.get_test_dataloader(trainer_input)\n",
    "```\n",
    "\n",
    "에서 아래의 column_names: \n",
    "\n",
    "- `pixel_values`\n",
    "- `head_mask`\n",
    "- `labels`\n",
    "- `output_attentions`\n",
    "- `output_hidden_states`\n",
    "- `interpolate_pos_encoding`\n",
    "- `return_dict`\n",
    "\n",
    "를 제외하고는 모두 트레이너(`batch_maker = trainer`)가 강제로 제거하는 로직이 있음.^[왜 이런 로직이 있을까? 이런 로직이 없다면 model의 args를 강제로 외우고 있어야 하니까..]\n",
    "\n",
    "`-` `image`라는 column_name은 위에 해당되지 않으므로 제거됨. \n",
    "\n",
    "`-` 그리고 `image` 칼럼이 제거된 이후에 `with_transform` 이 나중에 실행되면서 (지연실행) 문제가 발생. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 이걸 어떻게 알았냐고요? 코드뜯어봤습니다.. $\\to$ 숙제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "### 중간정리\n",
    "\n",
    "`trainer.predict()` 은 (1) 배치화 (2) 데이터콜렉팅 (3) 추론의 과정을 거친다. 그리고 배치화와 데이터콜렉팅 사이에 \"싱글배치\"를 만드는 과정이 있다. \n",
    "\n",
    "- 세부사항1: 그런데 \"**배치화**\"단계에서 `model.forward()`의 입력으로 사용되지 않는 columns는 지워지는 내부로직이 존재한다. \n",
    "- 세부사항2: `trainer_input`에 걸려있는 `.with_transform()`은 \"**배치화**\"이후 싱글배치가 만들어지는 과정에서 실행된다. \n",
    "\n",
    "\n",
    "따라서 `.with_transform()` 에서 특정컬럼의 변화시키는 동작이 약속된 경우, 그 컬럼이 **배치화**의 단계에서 자동제거되어 코드가 돌아가지 않을 수 있는 위험성이 존재한다. \n",
    "\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(풀이2)` -- `image`를 `return_dict` 로 위장.. // 완전 테크니컬한 풀이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 현재상황: `food['train']`에 `.with_transform(transforms)`을 걸어두고(?) `trainer_input`을 만든상황 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 문제: `trainer.predict()` 내부동작에서 `.with_transform(transform)` 이 실현될때 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "\u001b[0;32mdef\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pixel_values\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdel\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      /tmp/ipykernel_706133/1515420127.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "transforms??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 내용이 실행되어야하는데, `image`는 model의 입력으로 유하하지 않은 키라서 트레이너가 이미 제거한 상태임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 전략: 제거가 안되게 막아보자.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.forward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['return_dict', 'label'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_input2 = trainer_input.rename_columns({'image':'return_dict'})\n",
    "trainer_input2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 6,\n",
       " 'pixel_values': tensor([[[-0.7882, -0.7882, -0.7882,  ..., -0.7725, -0.7647, -0.7647],\n",
       "          [-0.7882, -0.7961, -0.8039,  ..., -0.7569, -0.7569, -0.7569],\n",
       "          [-0.8039, -0.7804, -0.7804,  ..., -0.7490, -0.7490, -0.7569],\n",
       "          ...,\n",
       "          [-0.2784, -0.2627, -0.2471,  ..., -0.0667, -0.1608, -0.2000],\n",
       "          [-0.2627, -0.2235, -0.2157,  ..., -0.1843, -0.1451, -0.0980],\n",
       "          [-0.3020, -0.2549, -0.2706,  ..., -0.1608, -0.1686, -0.1216]],\n",
       " \n",
       "         [[-0.7804, -0.7804, -0.7725,  ..., -0.8275, -0.8275, -0.8275],\n",
       "          [-0.7804, -0.7882, -0.7882,  ..., -0.8118, -0.8196, -0.8196],\n",
       "          [-0.7961, -0.7647, -0.7647,  ..., -0.8039, -0.8118, -0.8196],\n",
       "          ...,\n",
       "          [-0.3098, -0.3176, -0.3176,  ..., -0.1608, -0.2549, -0.2941],\n",
       "          [-0.2941, -0.2706, -0.2784,  ..., -0.2706, -0.2235, -0.1843],\n",
       "          [-0.3333, -0.3020, -0.3255,  ..., -0.2235, -0.2314, -0.1843]],\n",
       " \n",
       "         [[-0.8353, -0.8353, -0.8353,  ..., -0.9059, -0.9059, -0.9137],\n",
       "          [-0.8275, -0.8353, -0.8353,  ..., -0.8980, -0.9059, -0.9059],\n",
       "          [-0.8353, -0.8039, -0.7961,  ..., -0.8980, -0.8980, -0.9059],\n",
       "          ...,\n",
       "          [-0.5216, -0.5216, -0.5137,  ..., -0.3333, -0.4275, -0.4667],\n",
       "          [-0.5059, -0.4745, -0.4824,  ..., -0.4431, -0.3961, -0.3569],\n",
       "          [-0.5451, -0.5059, -0.5294,  ..., -0.4039, -0.4118, -0.3647]]])}"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_input2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['return_dict', 'label'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transforms2(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"return_dict\"]]\n",
    "    del examples[\"return_dict\"]\n",
    "    return examples\n",
    "trainer_input3 = trainer_input2.with_transform(transforms2)\n",
    "trainer_input3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=tensor(4.7354, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.0689,  0.1476, -0.1162, -0.1043, -0.0416,  0.0670, -0.1192, -0.1510,\n",
       "          0.0512, -0.0218,  0.0342, -0.0052, -0.0202, -0.1095, -0.2432, -0.1588,\n",
       "         -0.1816, -0.0084,  0.0445,  0.0051, -0.0051, -0.3004,  0.2271,  0.0927,\n",
       "         -0.0244,  0.0604,  0.0328, -0.2132, -0.0526, -0.0161,  0.1167, -0.0621,\n",
       "         -0.0960,  0.2350,  0.0693, -0.1266,  0.2053,  0.0429,  0.0229,  0.2440,\n",
       "          0.0103,  0.2298,  0.0807,  0.0563, -0.0894, -0.0288,  0.0675, -0.2696,\n",
       "          0.1174,  0.0797, -0.0105,  0.1302, -0.0687, -0.1368, -0.0403, -0.0390,\n",
       "          0.1230, -0.0256,  0.1477, -0.1459, -0.1190, -0.1551, -0.0197,  0.0003,\n",
       "          0.1087,  0.1157, -0.0924, -0.1210,  0.0742,  0.1664, -0.0729,  0.0445,\n",
       "          0.0857,  0.1474,  0.1720, -0.0251,  0.1180, -0.0707, -0.0207, -0.0817,\n",
       "         -0.0022,  0.0356,  0.0499,  0.1388,  0.0806,  0.0158,  0.0330,  0.0186,\n",
       "          0.0979,  0.1848,  0.1790,  0.0604, -0.1124, -0.0258,  0.0059, -0.0266,\n",
       "         -0.0023, -0.1017, -0.0090, -0.1801,  0.1208],\n",
       "        [-0.2108,  0.1510, -0.0838, -0.0891, -0.1120, -0.0534, -0.1076, -0.0662,\n",
       "          0.0740, -0.0068,  0.0638,  0.0392, -0.0991, -0.0040, -0.1481, -0.1171,\n",
       "         -0.0179,  0.0073, -0.0582,  0.0634, -0.1418, -0.1840, -0.0586,  0.0883,\n",
       "         -0.0368,  0.1339,  0.0532,  0.0793, -0.0788, -0.0104, -0.0218,  0.0096,\n",
       "         -0.1185,  0.1508,  0.1224, -0.1911,  0.2175,  0.0687, -0.0875,  0.0534,\n",
       "         -0.0413,  0.1520,  0.0398,  0.0617, -0.0058,  0.0921,  0.1326, -0.1214,\n",
       "          0.0273, -0.0643, -0.0886,  0.0894,  0.0073,  0.0416, -0.1266, -0.0339,\n",
       "          0.0035,  0.0214, -0.0013,  0.0665, -0.1330, -0.0427,  0.1205,  0.1008,\n",
       "         -0.0200,  0.1898, -0.0074, -0.1241, -0.0709,  0.0757,  0.1014,  0.0628,\n",
       "          0.0634,  0.0679,  0.1524, -0.1116, -0.0402, -0.0940,  0.0463, -0.0655,\n",
       "         -0.1454,  0.0073,  0.0336,  0.1563, -0.1066,  0.0307,  0.0414, -0.1558,\n",
       "          0.1143,  0.1420,  0.0836,  0.0133,  0.0067, -0.0532, -0.0095,  0.0595,\n",
       "          0.0689, -0.1216, -0.0184, -0.1371,  0.0361]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_maker = transformers.Trainer(\n",
    "    model = model,\n",
    "    data_collator = lambda x: x\n",
    ")\n",
    "_batched_data = batch_maker.get_test_dataloader(trainer_input3)\n",
    "batched_data = list(_batched_data)\n",
    "single_batch = batched_data[-1]\n",
    "#model.to(\"cpu\")\n",
    "model(**data_collator(single_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 성공.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 0.08239509,  0.04826868, -0.02438424, ..., -0.0937563 ,\n",
       "        -0.06260583,  0.00630521],\n",
       "       [-0.08984111,  0.07344969,  0.00403693, ..., -0.02388449,\n",
       "        -0.13815996,  0.16475692],\n",
       "       [ 0.07642026, -0.02146504,  0.05515821, ..., -0.12969677,\n",
       "        -0.05842251,  0.01547094],\n",
       "       ...,\n",
       "       [-0.11216477,  0.03565915,  0.00984988, ..., -0.02980242,\n",
       "        -0.05086675,  0.17964877],\n",
       "       [-0.06892063,  0.14759967, -0.11619416, ..., -0.00896731,\n",
       "        -0.18010172,  0.12083632],\n",
       "       [-0.2107594 ,  0.15095882, -0.08375154, ..., -0.01836788,\n",
       "        -0.13705489,  0.0361177 ]], dtype=float32), label_ids=array([6, 6, 6, 6, 6, 6, 6, 6, 6, 6]), metrics={'test_loss': 4.647078514099121, 'test_model_preparation_time': 0.0013, 'test_runtime': 0.0691, 'test_samples_per_second': 144.64, 'test_steps_per_second': 28.928})"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.predict(trainer_input3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(풀이3)` -- trainer_input 에 예약된 `with_transform`을 지연실행하지 않고 즉시 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 6,\n",
       "  'pixel_values': tensor([[[-0.7882, -0.7882, -0.7882,  ..., -0.7725, -0.7647, -0.7647],\n",
       "           [-0.7882, -0.7961, -0.8039,  ..., -0.7569, -0.7569, -0.7569],\n",
       "           [-0.8039, -0.7804, -0.7804,  ..., -0.7490, -0.7490, -0.7569],\n",
       "           ...,\n",
       "           [-0.2784, -0.2627, -0.2471,  ..., -0.0667, -0.1608, -0.2000],\n",
       "           [-0.2627, -0.2235, -0.2157,  ..., -0.1843, -0.1451, -0.0980],\n",
       "           [-0.3020, -0.2549, -0.2706,  ..., -0.1608, -0.1686, -0.1216]],\n",
       "  \n",
       "          [[-0.7804, -0.7804, -0.7725,  ..., -0.8275, -0.8275, -0.8275],\n",
       "           [-0.7804, -0.7882, -0.7882,  ..., -0.8118, -0.8196, -0.8196],\n",
       "           [-0.7961, -0.7647, -0.7647,  ..., -0.8039, -0.8118, -0.8196],\n",
       "           ...,\n",
       "           [-0.3098, -0.3176, -0.3176,  ..., -0.1608, -0.2549, -0.2941],\n",
       "           [-0.2941, -0.2706, -0.2784,  ..., -0.2706, -0.2235, -0.1843],\n",
       "           [-0.3333, -0.3020, -0.3255,  ..., -0.2235, -0.2314, -0.1843]],\n",
       "  \n",
       "          [[-0.8353, -0.8353, -0.8353,  ..., -0.9059, -0.9059, -0.9137],\n",
       "           [-0.8275, -0.8353, -0.8353,  ..., -0.8980, -0.9059, -0.9059],\n",
       "           [-0.8353, -0.8039, -0.7961,  ..., -0.8980, -0.8980, -0.9059],\n",
       "           ...,\n",
       "           [-0.5216, -0.5216, -0.5137,  ..., -0.3333, -0.4275, -0.4667],\n",
       "           [-0.5059, -0.4745, -0.4824,  ..., -0.4431, -0.3961, -0.3569],\n",
       "           [-0.5451, -0.5059, -0.5294,  ..., -0.4039, -0.4118, -0.3647]]])},\n",
       " {'label': 6,\n",
       "  'pixel_values': tensor([[[ 0.7804,  0.7882,  0.7804,  ...,  0.5294,  0.4980,  0.5843],\n",
       "           [ 0.7490,  0.7490,  0.7333,  ...,  0.4588,  0.5373,  0.6549],\n",
       "           [ 0.6314,  0.6235,  0.6314,  ...,  0.5294,  0.5765,  0.6627],\n",
       "           ...,\n",
       "           [ 0.2941,  0.2706,  0.2392,  ...,  0.1451,  0.1451,  0.1216],\n",
       "           [ 0.2941,  0.2863,  0.2706,  ...,  0.1451,  0.1451,  0.1529],\n",
       "           [ 0.3020,  0.3020,  0.2863,  ...,  0.1529,  0.1451,  0.1529]],\n",
       "  \n",
       "          [[ 0.7020,  0.7098,  0.6941,  ...,  0.3490,  0.3412,  0.4510],\n",
       "           [ 0.6157,  0.6078,  0.5843,  ...,  0.2863,  0.4118,  0.5529],\n",
       "           [ 0.4980,  0.4824,  0.4902,  ...,  0.3882,  0.4745,  0.5765],\n",
       "           ...,\n",
       "           [-0.1373, -0.1608, -0.1843,  ..., -0.2627, -0.2627, -0.2863],\n",
       "           [-0.1059, -0.1294, -0.1608,  ..., -0.2627, -0.2627, -0.2549],\n",
       "           [-0.0667, -0.1059, -0.1373,  ..., -0.2549, -0.2627, -0.2549]],\n",
       "  \n",
       "          [[ 0.8118,  0.8353,  0.8353,  ...,  0.3961,  0.4039,  0.5216],\n",
       "           [ 0.7020,  0.7176,  0.7255,  ...,  0.3490,  0.4824,  0.6392],\n",
       "           [ 0.5843,  0.5843,  0.6235,  ...,  0.4667,  0.5686,  0.6863],\n",
       "           ...,\n",
       "           [-0.2784, -0.3176, -0.3490,  ..., -0.4353, -0.4353, -0.4510],\n",
       "           [-0.2392, -0.2863, -0.3333,  ..., -0.4353, -0.4275, -0.4196],\n",
       "           [-0.1765, -0.2471, -0.3020,  ..., -0.4196, -0.4275, -0.4196]]])},\n",
       " {'label': 6,\n",
       "  'pixel_values': tensor([[[ 0.0353,  0.0353,  0.0431,  ..., -0.8667, -0.8745, -0.8824],\n",
       "           [ 0.0196,  0.0039, -0.0118,  ..., -0.8745, -0.8745, -0.8824],\n",
       "           [-0.0353, -0.0353, -0.0353,  ..., -0.8824, -0.8745, -0.8824],\n",
       "           ...,\n",
       "           [ 0.2549,  0.3490,  0.3569,  ...,  0.1529,  0.1529,  0.1765],\n",
       "           [ 0.0902,  0.2863,  0.3569,  ...,  0.2627,  0.3020,  0.3020],\n",
       "           [ 0.2471,  0.3333,  0.3569,  ...,  0.3333,  0.3333,  0.3412]],\n",
       "  \n",
       "          [[-0.0667, -0.0588, -0.0431,  ..., -0.9137, -0.9216, -0.9216],\n",
       "           [-0.1059, -0.1137, -0.1137,  ..., -0.9137, -0.9137, -0.9216],\n",
       "           [-0.1922, -0.1765, -0.1765,  ..., -0.9137, -0.9137, -0.9216],\n",
       "           ...,\n",
       "           [ 0.2471,  0.3412,  0.3569,  ...,  0.1686,  0.1765,  0.2000],\n",
       "           [ 0.0902,  0.2863,  0.3569,  ...,  0.2941,  0.3255,  0.3333],\n",
       "           [ 0.2471,  0.3333,  0.3569,  ...,  0.3725,  0.3725,  0.3804]],\n",
       "  \n",
       "          [[-0.0745, -0.0667, -0.0431,  ..., -0.9529, -0.9529, -0.9529],\n",
       "           [-0.1216, -0.1216, -0.1137,  ..., -0.9451, -0.9451, -0.9529],\n",
       "           [-0.1843, -0.1765, -0.1608,  ..., -0.9373, -0.9451, -0.9529],\n",
       "           ...,\n",
       "           [ 0.2157,  0.3098,  0.3176,  ...,  0.1294,  0.1294,  0.1529],\n",
       "           [ 0.0667,  0.2627,  0.3412,  ...,  0.2627,  0.2863,  0.2941],\n",
       "           [ 0.2314,  0.3176,  0.3412,  ...,  0.3412,  0.3255,  0.3333]]])},\n",
       " {'label': 6,\n",
       "  'pixel_values': tensor([[[ 0.5059,  0.5137,  0.5137,  ...,  0.6392,  0.6314,  0.6235],\n",
       "           [ 0.5059,  0.5059,  0.5059,  ...,  0.6392,  0.6392,  0.6314],\n",
       "           [ 0.5137,  0.4980,  0.4902,  ...,  0.6471,  0.6471,  0.6549],\n",
       "           ...,\n",
       "           [ 0.6706,  0.6784,  0.6863,  ...,  0.6784,  0.6784,  0.6627],\n",
       "           [ 0.6863,  0.6941,  0.6941,  ...,  0.6627,  0.6549,  0.6471],\n",
       "           [ 0.6941,  0.7020,  0.7098,  ...,  0.6706,  0.6706,  0.6549]],\n",
       "  \n",
       "          [[ 0.3176,  0.3255,  0.3333,  ...,  0.4510,  0.4510,  0.4431],\n",
       "           [ 0.3176,  0.3176,  0.3255,  ...,  0.4431,  0.4431,  0.4353],\n",
       "           [ 0.3255,  0.3098,  0.3176,  ...,  0.4431,  0.4510,  0.4431],\n",
       "           ...,\n",
       "           [ 0.6000,  0.6078,  0.6157,  ...,  0.4980,  0.4980,  0.4824],\n",
       "           [ 0.6157,  0.6235,  0.6157,  ...,  0.4824,  0.4745,  0.4667],\n",
       "           [ 0.6235,  0.6314,  0.6314,  ...,  0.4902,  0.4902,  0.4745]],\n",
       "  \n",
       "          [[-0.0431, -0.0353, -0.0275,  ..., -0.1451, -0.1529, -0.1608],\n",
       "           [-0.0275, -0.0431, -0.0353,  ..., -0.2157, -0.2235, -0.2314],\n",
       "           [-0.0196, -0.0431, -0.0510,  ..., -0.2471, -0.2549, -0.2706],\n",
       "           ...,\n",
       "           [ 0.6078,  0.6157,  0.6235,  ...,  0.2549,  0.2549,  0.2392],\n",
       "           [ 0.6235,  0.6314,  0.6235,  ...,  0.2392,  0.2314,  0.2235],\n",
       "           [ 0.6471,  0.6392,  0.6392,  ...,  0.2471,  0.2471,  0.2314]]])},\n",
       " {'label': 6,\n",
       "  'pixel_values': tensor([[[ 0.6784,  0.7020,  0.7412,  ...,  0.9216,  0.9294,  0.9294],\n",
       "           [ 0.6941,  0.7098,  0.7255,  ...,  0.9137,  0.9216,  0.9294],\n",
       "           [ 0.7176,  0.7176,  0.7255,  ...,  0.9059,  0.9216,  0.9216],\n",
       "           ...,\n",
       "           [ 0.0275,  0.0275,  0.0275,  ...,  0.6627,  0.6941,  0.7098],\n",
       "           [ 0.0196,  0.0196,  0.0196,  ...,  0.7176,  0.7333,  0.7412],\n",
       "           [ 0.0275,  0.0275,  0.0275,  ...,  0.8118,  0.8118,  0.8118]],\n",
       "  \n",
       "          [[ 0.5843,  0.6078,  0.6549,  ...,  0.8902,  0.8980,  0.8980],\n",
       "           [ 0.6157,  0.6235,  0.6471,  ...,  0.8824,  0.8902,  0.8980],\n",
       "           [ 0.6471,  0.6471,  0.6549,  ...,  0.8824,  0.8902,  0.8980],\n",
       "           ...,\n",
       "           [-0.1137, -0.1137, -0.1137,  ...,  0.5137,  0.5451,  0.5686],\n",
       "           [-0.1137, -0.1137, -0.1137,  ...,  0.5451,  0.5686,  0.5765],\n",
       "           [-0.1059, -0.1059, -0.1059,  ...,  0.6235,  0.6235,  0.6235]],\n",
       "  \n",
       "          [[ 0.4588,  0.4824,  0.5137,  ...,  0.7961,  0.8039,  0.8039],\n",
       "           [ 0.4980,  0.4980,  0.5059,  ...,  0.7882,  0.7961,  0.8039],\n",
       "           [ 0.5294,  0.5216,  0.5216,  ...,  0.7804,  0.7882,  0.7961],\n",
       "           ...,\n",
       "           [-0.1922, -0.1922, -0.1922,  ...,  0.3255,  0.3725,  0.3961],\n",
       "           [-0.1922, -0.1922, -0.1922,  ...,  0.3490,  0.3804,  0.3961],\n",
       "           [-0.1843, -0.1843, -0.1843,  ...,  0.4039,  0.4118,  0.4196]]])},\n",
       " {'label': 6,\n",
       "  'pixel_values': tensor([[[ 0.2000,  0.2235,  0.2549,  ...,  0.3725,  0.3647,  0.3412],\n",
       "           [ 0.1922,  0.2157,  0.2627,  ...,  0.3490,  0.3412,  0.3255],\n",
       "           [ 0.1843,  0.2157,  0.2549,  ...,  0.3098,  0.3020,  0.2863],\n",
       "           ...,\n",
       "           [ 0.1843,  0.1922,  0.1843,  ...,  0.2235,  0.1922,  0.1686],\n",
       "           [ 0.1922,  0.2000,  0.1922,  ...,  0.2000,  0.1765,  0.1529],\n",
       "           [ 0.1843,  0.1922,  0.2000,  ...,  0.1765,  0.1529,  0.1373]],\n",
       "  \n",
       "          [[ 0.0275,  0.0510,  0.0824,  ...,  0.1529,  0.1451,  0.1216],\n",
       "           [ 0.0275,  0.0510,  0.0902,  ...,  0.1216,  0.1137,  0.0980],\n",
       "           [ 0.0196,  0.0510,  0.0902,  ...,  0.0902,  0.0824,  0.0667],\n",
       "           ...,\n",
       "           [-0.0196, -0.0118, -0.0196,  ...,  0.0353,  0.0196, -0.0039],\n",
       "           [-0.0196, -0.0118, -0.0196,  ...,  0.0196, -0.0039, -0.0275],\n",
       "           [-0.0275, -0.0196, -0.0118,  ...,  0.0039, -0.0275, -0.0431]],\n",
       "  \n",
       "          [[-0.1373, -0.1059, -0.0745,  ..., -0.0196, -0.0431, -0.0745],\n",
       "           [-0.1294, -0.0980, -0.0667,  ..., -0.0510, -0.0667, -0.0902],\n",
       "           [-0.1294, -0.0980, -0.0667,  ..., -0.0824, -0.0902, -0.1137],\n",
       "           ...,\n",
       "           [-0.2000, -0.1922, -0.2000,  ..., -0.1373, -0.1529, -0.1686],\n",
       "           [-0.1843, -0.1843, -0.1922,  ..., -0.1451, -0.1608, -0.1686],\n",
       "           [-0.1922, -0.1843, -0.1843,  ..., -0.1608, -0.1686, -0.1765]]])},\n",
       " {'label': 6,\n",
       "  'pixel_values': tensor([[[-0.2235, -0.2235, -0.2000,  ..., -0.5373, -0.5294, -0.5373],\n",
       "           [-0.2314, -0.2471, -0.2078,  ..., -0.5137, -0.4980, -0.5059],\n",
       "           [-0.2627, -0.2627, -0.2627,  ..., -0.5137, -0.4980, -0.5216],\n",
       "           ...,\n",
       "           [-0.1765, -0.1294, -0.0510,  ..., -0.1765, -0.1922, -0.1922],\n",
       "           [-0.1529, -0.1373, -0.0510,  ..., -0.1765, -0.1843, -0.1686],\n",
       "           [-0.1373, -0.1137, -0.0196,  ..., -0.1608, -0.1686, -0.1686]],\n",
       "  \n",
       "          [[-0.4588, -0.4588, -0.4353,  ..., -0.7333, -0.7255, -0.7333],\n",
       "           [-0.4824, -0.4902, -0.4510,  ..., -0.7255, -0.7098, -0.7020],\n",
       "           [-0.5137, -0.5059, -0.5059,  ..., -0.7333, -0.7098, -0.7255],\n",
       "           ...,\n",
       "           [-0.5216, -0.4745, -0.3961,  ..., -0.5216, -0.5373, -0.5294],\n",
       "           [-0.4980, -0.4824, -0.3882,  ..., -0.5216, -0.5294, -0.5059],\n",
       "           [-0.4824, -0.4588, -0.3569,  ..., -0.5059, -0.5137, -0.4980]],\n",
       "  \n",
       "          [[-0.4431, -0.4353, -0.4196,  ..., -0.7647, -0.7569, -0.7647],\n",
       "           [-0.4667, -0.4902, -0.4667,  ..., -0.7569, -0.7333, -0.7333],\n",
       "           [-0.5059, -0.5137, -0.5216,  ..., -0.7569, -0.7412, -0.7569],\n",
       "           ...,\n",
       "           [-0.7333, -0.7020, -0.6314,  ..., -0.7490, -0.7647, -0.7569],\n",
       "           [-0.7176, -0.7176, -0.6392,  ..., -0.7412, -0.7569, -0.7412],\n",
       "           [-0.7098, -0.6941, -0.6078,  ..., -0.7255, -0.7412, -0.7333]]])},\n",
       " {'label': 6,\n",
       "  'pixel_values': tensor([[[ 0.1765,  0.0980,  0.2549,  ...,  0.8667,  0.8824,  0.8824],\n",
       "           [ 0.1843,  0.0980,  0.2471,  ...,  0.8824,  0.8902,  0.8902],\n",
       "           [ 0.1843,  0.0902,  0.2157,  ...,  0.8745,  0.8824,  0.8824],\n",
       "           ...,\n",
       "           [ 0.0039,  0.0588,  0.0588,  ...,  0.8745,  0.8980,  0.8980],\n",
       "           [ 0.0275,  0.0667,  0.0353,  ...,  0.8824,  0.8902,  0.8902],\n",
       "           [ 0.0510,  0.0902,  0.0431,  ...,  0.8902,  0.8824,  0.8902]],\n",
       "  \n",
       "          [[-0.1216, -0.2000, -0.0431,  ...,  0.8196,  0.8353,  0.8353],\n",
       "           [-0.1137, -0.2000, -0.0510,  ...,  0.8353,  0.8431,  0.8431],\n",
       "           [-0.1137, -0.2078, -0.0824,  ...,  0.8275,  0.8353,  0.8353],\n",
       "           ...,\n",
       "           [-0.5843, -0.5451, -0.5608,  ...,  0.8353,  0.8588,  0.8588],\n",
       "           [-0.5765, -0.5373, -0.5843,  ...,  0.8431,  0.8510,  0.8510],\n",
       "           [-0.5529, -0.5216, -0.5686,  ...,  0.8510,  0.8431,  0.8510]],\n",
       "  \n",
       "          [[-0.1451, -0.2235, -0.0667,  ...,  0.8353,  0.8510,  0.8510],\n",
       "           [-0.1373, -0.2235, -0.0745,  ...,  0.8510,  0.8588,  0.8588],\n",
       "           [-0.1373, -0.2314, -0.1059,  ...,  0.8431,  0.8510,  0.8510],\n",
       "           ...,\n",
       "           [-0.8588, -0.8118, -0.8196,  ...,  0.8118,  0.8353,  0.8353],\n",
       "           [-0.8510, -0.8196, -0.8510,  ...,  0.8196,  0.8275,  0.8275],\n",
       "           [-0.8353, -0.8039, -0.8510,  ...,  0.8275,  0.8196,  0.8275]]])},\n",
       " {'label': 6,\n",
       "  'pixel_values': tensor([[[-0.1843, -0.1843, -0.2235,  ...,  0.8431,  0.8353,  0.8510],\n",
       "           [-0.1059, -0.1451, -0.2078,  ...,  0.8431,  0.8588,  0.8667],\n",
       "           [ 0.0196, -0.0510, -0.1451,  ...,  0.8275,  0.8431,  0.8431],\n",
       "           ...,\n",
       "           [ 0.7961,  0.7804,  0.7804,  ...,  0.2941,  0.3647,  0.5529],\n",
       "           [ 0.7882,  0.8039,  0.7961,  ...,  0.3333,  0.3412,  0.5059],\n",
       "           [ 0.7098,  0.7725,  0.7804,  ...,  0.4824,  0.4275,  0.4510]],\n",
       "  \n",
       "          [[-0.7020, -0.6941, -0.7255,  ...,  0.6078,  0.6078,  0.6235],\n",
       "           [-0.6627, -0.6627, -0.6863,  ...,  0.6000,  0.6157,  0.6235],\n",
       "           [-0.5686, -0.5922, -0.6235,  ...,  0.5843,  0.6000,  0.6000],\n",
       "           ...,\n",
       "           [ 0.4510,  0.4353,  0.4275,  ..., -0.1686, -0.1137,  0.0745],\n",
       "           [ 0.4510,  0.4588,  0.4431,  ..., -0.1451, -0.1529,  0.0118],\n",
       "           [ 0.4353,  0.4667,  0.4431,  ..., -0.0118, -0.0745, -0.0353]],\n",
       "  \n",
       "          [[-0.9608, -0.9373, -0.9608,  ...,  0.0667,  0.0902,  0.1216],\n",
       "           [-0.9529, -0.9451, -0.9608,  ...,  0.0902,  0.1059,  0.1059],\n",
       "           [-0.8824, -0.8980, -0.9294,  ...,  0.0745,  0.0902,  0.0902],\n",
       "           ...,\n",
       "           [-0.3882, -0.3961, -0.3961,  ..., -0.8431, -0.7569, -0.5529],\n",
       "           [-0.3647, -0.3490, -0.3647,  ..., -0.8275, -0.8118, -0.6078],\n",
       "           [-0.4118, -0.3412, -0.3961,  ..., -0.7647, -0.8196, -0.7490]]])},\n",
       " {'label': 6,\n",
       "  'pixel_values': tensor([[[ 0.0824,  0.1137,  0.1373,  ...,  0.5059,  0.5059,  0.5137],\n",
       "           [ 0.1294,  0.1529,  0.1765,  ...,  0.4980,  0.4980,  0.4902],\n",
       "           [ 0.1373,  0.1451,  0.1529,  ...,  0.4431,  0.4353,  0.4353],\n",
       "           ...,\n",
       "           [-1.0000, -1.0000, -1.0000,  ..., -0.3255, -0.3098, -0.2706],\n",
       "           [-1.0000, -1.0000, -1.0000,  ..., -0.7490, -0.7255, -0.6941],\n",
       "           [-0.9922, -0.9922, -0.9922,  ..., -0.9294, -0.9216, -0.9216]],\n",
       "  \n",
       "          [[ 0.0039,  0.0353,  0.0588,  ...,  0.3176,  0.3176,  0.3255],\n",
       "           [ 0.0510,  0.0745,  0.0980,  ...,  0.3098,  0.3098,  0.2941],\n",
       "           [ 0.0588,  0.0667,  0.0745,  ...,  0.2471,  0.2392,  0.2392],\n",
       "           ...,\n",
       "           [-0.9843, -0.9843, -0.9843,  ..., -0.4824, -0.4667, -0.4275],\n",
       "           [-0.9843, -0.9843, -0.9843,  ..., -0.8902, -0.8824, -0.8510],\n",
       "           [-0.9922, -0.9922, -0.9922,  ..., -0.9843, -0.9843, -0.9922]],\n",
       "  \n",
       "          [[-0.0902, -0.0510, -0.0275,  ...,  0.3020,  0.3020,  0.3098],\n",
       "           [-0.0353, -0.0039,  0.0196,  ...,  0.2863,  0.2863,  0.2863],\n",
       "           [-0.0196, -0.0039,  0.0039,  ...,  0.2157,  0.2078,  0.2157],\n",
       "           ...,\n",
       "           [-1.0000, -1.0000, -1.0000,  ..., -0.5529, -0.5373, -0.4980],\n",
       "           [-1.0000, -1.0000, -1.0000,  ..., -0.9216, -0.9059, -0.8902],\n",
       "           [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]])}]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_input2 = [l for l in trainer_input]\n",
    "trainer_input2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "batch_maker = transformers.Trainer(\n",
    "    model = model,\n",
    "    data_collator = lambda x: x\n",
    ")\n",
    "_batched_data = batch_maker.get_test_dataloader(trainer_input2)\n",
    "batched_data = list(_batched_data)\n",
    "single_batch = batched_data[0]\n",
    "#model.to(\"cpu\")\n",
    "model(**data_collator(single_batch));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.predict(trainer_input2); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(풀이4)` -- 트레이너가 가진 \"사용하지 않는 column을 제거하는 기능\"을 `False` 시킴.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_maker = transformers.Trainer(\n",
    "    model = model,\n",
    "    data_collator = lambda x: x,\n",
    "    args = transformers.TrainingArguments(\n",
    "        output_dir= \"asdf\", # 아무거나 써야함. \n",
    "        remove_unused_columns= False # 이 부분이 포인트!!\n",
    "    )        \n",
    ")\n",
    "_batched_data = batch_maker.get_test_dataloader(trainer_input)\n",
    "batched_data = list(_batched_data)\n",
    "single_batch = batched_data[0]\n",
    "#model.to(\"cpu\")\n",
    "model(**data_collator(single_batch));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir= 'asdf', # 아무거나 써야함\n",
    "        remove_unused_columns= False # 이 부분이 포인트!!\n",
    "    )        \n",
    ")\n",
    "trainer.predict(trainer_input);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(풀이5)` -- 트레이너가 가진 \"사용하지 않는 column을 제거하는 기능\"을 `False` 시킬꺼면, `batch_maker`를 고려할 필요도 없이 아래와 같이 바로 `single_batch`를 얻을 수 있음. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*풀이4: 실제로 trainer가 싱글배치를 얻는 과정과 유사하게 얻는 방법*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_maker = transformers.Trainer(\n",
    "    model = model,\n",
    "    data_collator = lambda x: x,\n",
    "    args = transformers.TrainingArguments(\n",
    "        output_dir= \"asdf\", # 아무거나 써야함. \n",
    "        remove_unused_columns= False # 이 부분이 포인트!!\n",
    "    )        \n",
    ")\n",
    "_batched_data = batch_maker.get_test_dataloader(trainer_input)\n",
    "batched_data = list(_batched_data)\n",
    "single_batch = batched_data[-1]\n",
    "#single_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 형식관찰: `single_batch`는 `[Dict, Dict, Dict, .... Dict]` 꼴임을 주목하라."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*풀이5: 형식관찰에 힌트를 얻어 무식하게 얻은 싱글배치*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_batch = [\n",
    "    trainer_input[0],\n",
    "    trainer_input[1],\n",
    "    trainer_input[2],\n",
    "    trainer_input[3],\n",
    "    trainer_input[4],\n",
    "    trainer_input[5],\n",
    "    trainer_input[6],\n",
    "    trainer_input[7],\n",
    "]\n",
    "#single_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*아무튼 풀이5 스타일로 싱글배치를 얻었다면? 이후의 코드는 동일*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-3.11896242e-02, -2.25847423e-01,  1.63939549e-03, ...,\n",
       "        -2.29127333e-03,  3.87525409e-02,  4.29060608e-02],\n",
       "       [ 4.27692719e-02, -1.01897612e-01, -9.16560292e-02, ...,\n",
       "         4.52993475e-02, -8.15727636e-02, -5.56547865e-02],\n",
       "       [-5.40109351e-04, -1.02955595e-01, -1.03463437e-02, ...,\n",
       "        -3.05373464e-02,  9.11735594e-02,  8.82753134e-02],\n",
       "       ...,\n",
       "       [ 8.04588795e-02,  2.72037834e-02, -1.16188087e-01, ...,\n",
       "         6.56688288e-02,  1.24141075e-01,  3.59139368e-02],\n",
       "       [ 5.94812930e-02, -9.47779790e-02, -2.50497796e-02, ...,\n",
       "         5.41531555e-02,  6.13874942e-03, -5.53418696e-03],\n",
       "       [ 7.43777454e-02, -1.62318684e-02,  1.30667584e-04, ...,\n",
       "        -3.19715589e-04, -9.09084827e-02, -1.61014676e-01]], dtype=float32), label_ids=array([6, 6, 6, 6, 6, 6, 6, 6, 6, 6]), metrics={'test_loss': 4.532778739929199, 'test_model_preparation_time': 0.0013, 'test_runtime': 0.0551, 'test_samples_per_second': 181.424, 'test_steps_per_second': 36.285})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 풀이4 \n",
    "### STEP1: 싱글배치로 체크하기 \n",
    "batch_maker = transformers.Trainer(\n",
    "    model = model,\n",
    "    data_collator = lambda x: x,\n",
    "    args = transformers.TrainingArguments(\n",
    "        output_dir= \"asdf\",\n",
    "        remove_unused_columns= False\n",
    "    )        \n",
    ")\n",
    "_batched_data = batch_maker.get_test_dataloader(trainer_input)\n",
    "batched_data = list(_batched_data)\n",
    "single_batch = batched_data[0]\n",
    "#model.to(\"cpu\")\n",
    "model(**data_collator(single_batch))\n",
    "### STEP2: 트레이너 설계하고 predict하기\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir= 'asdf',\n",
    "        remove_unused_columns= False\n",
    "    )        \n",
    ")\n",
    "trainer.predict(trainer_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-3.11896242e-02, -2.25847423e-01,  1.63939549e-03, ...,\n",
       "        -2.29127333e-03,  3.87525409e-02,  4.29060608e-02],\n",
       "       [ 4.27692719e-02, -1.01897612e-01, -9.16560292e-02, ...,\n",
       "         4.52993475e-02, -8.15727636e-02, -5.56547865e-02],\n",
       "       [-5.40109351e-04, -1.02955595e-01, -1.03463437e-02, ...,\n",
       "        -3.05373464e-02,  9.11735594e-02,  8.82753134e-02],\n",
       "       ...,\n",
       "       [ 8.04588795e-02,  2.72037834e-02, -1.16188087e-01, ...,\n",
       "         6.56688288e-02,  1.24141075e-01,  3.59139368e-02],\n",
       "       [ 5.94812930e-02, -9.47779790e-02, -2.50497796e-02, ...,\n",
       "         5.41531555e-02,  6.13874942e-03, -5.53418696e-03],\n",
       "       [ 7.43777454e-02, -1.62318684e-02,  1.30667584e-04, ...,\n",
       "        -3.19715589e-04, -9.09084827e-02, -1.61014676e-01]], dtype=float32), label_ids=array([6, 6, 6, 6, 6, 6, 6, 6, 6, 6]), metrics={'test_loss': 4.532778739929199, 'test_model_preparation_time': 0.0015, 'test_runtime': 0.1057, 'test_samples_per_second': 94.587, 'test_steps_per_second': 18.917})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 풀이5\n",
    "### STEP1: 싱글배치로 체크하기 \n",
    "single_batch =[\n",
    "    trainer_input[0],\n",
    "    trainer_input[1],\n",
    "    trainer_input[2],\n",
    "    trainer_input[3],\n",
    "    trainer_input[4],\n",
    "    trainer_input[5],\n",
    "    trainer_input[6],\n",
    "    trainer_input[7],\n",
    "]\n",
    "model.to(\"cpu\")\n",
    "model(**data_collator(single_batch));\n",
    "### STEP2: 트레이너 설계하고 predict하기\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir= 'asdf', # 아무거나 써야함\n",
    "        remove_unused_columns= False # 이 부분이 포인트!!\n",
    "    )        \n",
    ")\n",
    "trainer.predict(trainer_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*참고1: 아래의 방식으로는 싱글배치를 얻을 수 없음*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer_input[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이유:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_input[:2] == [trainer_input[0], trainer_input[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*참고2: 아래의 방식으로도 싱글배치를 얻을 수 없음 -- 이유? 지연실행때문에..*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer_input.to_list()[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. FOOD101 -- DefaultDataCollator 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*1. 데이터준비: `\"guebin/food101-tiny\"` $\\to$ `trainer_input`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food = datasets.load_dataset(\"guebin/food101-tiny\")\n",
    "image_processor = transformers.AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "normalize = torchvision.transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomResizedCrop(size), \n",
    "    torchvision.transforms.ToTensor(), \n",
    "    normalize\n",
    "])\n",
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples\n",
    "trainer_input = food['train'].with_transform(transforms)\n",
    "trainer_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2. 모델준비: `\"google/vit-base-patch16-224-in21k\"` $\\to$`model`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "labels = food[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "model = transformers.AutoModelForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*3. 데이터콜렉터: `collate_fn` 직접설계*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_collator = transformers.DefaultDataCollator()\n",
    "# data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(single_batch):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DefaultDataCollator()` 와 동일한 역할을 하는 `collate_fn`을 설계하라. 이를 이용하여 적당한 `trainer`를 만들어 \n",
    "\n",
    "```Python\n",
    "trainer.predict(trainer_input)\n",
    "```\n",
    "\n",
    "이 정상동작하는지 확인하라. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(풀이)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 6,\n",
       "  'pixel_values': tensor([[[ 0.2000,  0.1529,  0.0745,  ...,  0.1686,  0.1137,  0.0824],\n",
       "           [ 0.1608,  0.1451,  0.1294,  ...,  0.1686,  0.1451,  0.1373],\n",
       "           [ 0.1137,  0.1294,  0.1529,  ...,  0.1608,  0.1686,  0.1765],\n",
       "           ...,\n",
       "           [-0.0275, -0.0275, -0.0431,  ..., -0.1294, -0.1451, -0.1608],\n",
       "           [-0.0980, -0.0745, -0.0431,  ..., -0.1608, -0.1608, -0.1686],\n",
       "           [-0.1373, -0.1137, -0.0667,  ..., -0.1843, -0.1922, -0.2000]],\n",
       "  \n",
       "          [[ 0.1529,  0.1137,  0.0353,  ...,  0.1294,  0.0745,  0.0431],\n",
       "           [ 0.1216,  0.1137,  0.0902,  ...,  0.1294,  0.1059,  0.0980],\n",
       "           [ 0.0824,  0.0980,  0.1216,  ...,  0.1216,  0.1294,  0.1373],\n",
       "           ...,\n",
       "           [-0.0902, -0.0902, -0.1059,  ..., -0.1373, -0.1529, -0.1686],\n",
       "           [-0.1608, -0.1373, -0.1059,  ..., -0.1686, -0.1686, -0.1765],\n",
       "           [-0.2000, -0.1765, -0.1294,  ..., -0.1922, -0.2000, -0.2078]],\n",
       "  \n",
       "          [[-0.0510, -0.0980, -0.1765,  ..., -0.0431, -0.0980, -0.1294],\n",
       "           [-0.0902, -0.0980, -0.1216,  ..., -0.0431, -0.0667, -0.0745],\n",
       "           [-0.1294, -0.1137, -0.0902,  ..., -0.0510, -0.0431, -0.0353],\n",
       "           ...,\n",
       "           [-0.1922, -0.1922, -0.2078,  ..., -0.3020, -0.3176, -0.3255],\n",
       "           [-0.2627, -0.2392, -0.2078,  ..., -0.3333, -0.3333, -0.3333],\n",
       "           [-0.3020, -0.2784, -0.2314,  ..., -0.3569, -0.3647, -0.3647]]])},\n",
       " {'label': 6,\n",
       "  'pixel_values': tensor([[[ 0.4824,  0.4824,  0.4980,  ..., -0.4667, -0.4431, -0.3725],\n",
       "           [ 0.5765,  0.6000,  0.5451,  ..., -0.4667, -0.4510, -0.3961],\n",
       "           [ 0.6706,  0.6549,  0.5922,  ..., -0.4588, -0.4510, -0.4275],\n",
       "           ...,\n",
       "           [ 0.4667,  0.4118,  0.4039,  ...,  0.1216,  0.0824,  0.0431],\n",
       "           [ 0.4824,  0.4510,  0.4353,  ...,  0.1373,  0.1373,  0.1137],\n",
       "           [ 0.4824,  0.4667,  0.4353,  ...,  0.1451,  0.1451,  0.1216]],\n",
       "  \n",
       "          [[ 0.2314,  0.1922,  0.1843,  ..., -0.8118, -0.8118, -0.7569],\n",
       "           [ 0.3255,  0.3176,  0.2549,  ..., -0.8196, -0.8118, -0.7725],\n",
       "           [ 0.4118,  0.3804,  0.3098,  ..., -0.8039, -0.8118, -0.8039],\n",
       "           ...,\n",
       "           [ 0.2706,  0.1843,  0.1608,  ..., -0.2863, -0.3176, -0.3490],\n",
       "           [ 0.3176,  0.2627,  0.2157,  ..., -0.2706, -0.2706, -0.2863],\n",
       "           [ 0.3569,  0.3098,  0.2549,  ..., -0.2471, -0.2471, -0.2627]],\n",
       "  \n",
       "          [[ 0.3176,  0.3412,  0.3725,  ..., -0.9059, -0.9294, -0.8902],\n",
       "           [ 0.4118,  0.4745,  0.4275,  ..., -0.9137, -0.9294, -0.9059],\n",
       "           [ 0.4980,  0.5294,  0.4824,  ..., -0.9059, -0.9216, -0.9216],\n",
       "           ...,\n",
       "           [ 0.3412,  0.2784,  0.2627,  ..., -0.3961, -0.4275, -0.4588],\n",
       "           [ 0.3647,  0.3333,  0.2863,  ..., -0.3804, -0.3804, -0.3961],\n",
       "           [ 0.3882,  0.3725,  0.3176,  ..., -0.3569, -0.3569, -0.3804]]])}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_maker = transformers.Trainer(\n",
    "#     model = model,\n",
    "#     data_collator = lambda x: x,\n",
    "#     args = transformers.TrainingArguments(\n",
    "#         output_dir= \"asdf\", # 아무거나 써야함. \n",
    "#         remove_unused_columns= False, # 이 부분이 포인트!!\n",
    "#     )        \n",
    "# )\n",
    "# _batched_data = batch_maker.get_test_dataloader(trainer_input)\n",
    "# batched_data = list(_batched_data)\n",
    "# single_batch = batched_data[-1]\n",
    "# single_batch\n",
    "#---#\n",
    "single_batch = [trainer_input[0],trainer_input[1]]\n",
    "single_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이대로 `model(**sigle_batch)`를 실행하면 에러가 나겠죠? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data_collator_input):\n",
    "    out = dict()\n",
    "    out['pixel_values'] = torch.stack([l['pixel_values'] for l in data_collator_input])\n",
    "    out['labels'] = torch.tensor([l['label'] for l in data_collator_input])\n",
    "    return out                                  \n",
    "collate_fn(single_batch);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[ 0.2000,  0.1529,  0.0745,  ...,  0.1686,  0.1137,  0.0824],\n",
       "           [ 0.1608,  0.1451,  0.1294,  ...,  0.1686,  0.1451,  0.1373],\n",
       "           [ 0.1137,  0.1294,  0.1529,  ...,  0.1608,  0.1686,  0.1765],\n",
       "           ...,\n",
       "           [-0.0275, -0.0275, -0.0431,  ..., -0.1294, -0.1451, -0.1608],\n",
       "           [-0.0980, -0.0745, -0.0431,  ..., -0.1608, -0.1608, -0.1686],\n",
       "           [-0.1373, -0.1137, -0.0667,  ..., -0.1843, -0.1922, -0.2000]],\n",
       " \n",
       "          [[ 0.1529,  0.1137,  0.0353,  ...,  0.1294,  0.0745,  0.0431],\n",
       "           [ 0.1216,  0.1137,  0.0902,  ...,  0.1294,  0.1059,  0.0980],\n",
       "           [ 0.0824,  0.0980,  0.1216,  ...,  0.1216,  0.1294,  0.1373],\n",
       "           ...,\n",
       "           [-0.0902, -0.0902, -0.1059,  ..., -0.1373, -0.1529, -0.1686],\n",
       "           [-0.1608, -0.1373, -0.1059,  ..., -0.1686, -0.1686, -0.1765],\n",
       "           [-0.2000, -0.1765, -0.1294,  ..., -0.1922, -0.2000, -0.2078]],\n",
       " \n",
       "          [[-0.0510, -0.0980, -0.1765,  ..., -0.0431, -0.0980, -0.1294],\n",
       "           [-0.0902, -0.0980, -0.1216,  ..., -0.0431, -0.0667, -0.0745],\n",
       "           [-0.1294, -0.1137, -0.0902,  ..., -0.0510, -0.0431, -0.0353],\n",
       "           ...,\n",
       "           [-0.1922, -0.1922, -0.2078,  ..., -0.3020, -0.3176, -0.3255],\n",
       "           [-0.2627, -0.2392, -0.2078,  ..., -0.3333, -0.3333, -0.3333],\n",
       "           [-0.3020, -0.2784, -0.2314,  ..., -0.3569, -0.3647, -0.3647]]],\n",
       " \n",
       " \n",
       "         [[[ 0.4824,  0.4824,  0.4980,  ..., -0.4667, -0.4431, -0.3725],\n",
       "           [ 0.5765,  0.6000,  0.5451,  ..., -0.4667, -0.4510, -0.3961],\n",
       "           [ 0.6706,  0.6549,  0.5922,  ..., -0.4588, -0.4510, -0.4275],\n",
       "           ...,\n",
       "           [ 0.4667,  0.4118,  0.4039,  ...,  0.1216,  0.0824,  0.0431],\n",
       "           [ 0.4824,  0.4510,  0.4353,  ...,  0.1373,  0.1373,  0.1137],\n",
       "           [ 0.4824,  0.4667,  0.4353,  ...,  0.1451,  0.1451,  0.1216]],\n",
       " \n",
       "          [[ 0.2314,  0.1922,  0.1843,  ..., -0.8118, -0.8118, -0.7569],\n",
       "           [ 0.3255,  0.3176,  0.2549,  ..., -0.8196, -0.8118, -0.7725],\n",
       "           [ 0.4118,  0.3804,  0.3098,  ..., -0.8039, -0.8118, -0.8039],\n",
       "           ...,\n",
       "           [ 0.2706,  0.1843,  0.1608,  ..., -0.2863, -0.3176, -0.3490],\n",
       "           [ 0.3176,  0.2627,  0.2157,  ..., -0.2706, -0.2706, -0.2863],\n",
       "           [ 0.3569,  0.3098,  0.2549,  ..., -0.2471, -0.2471, -0.2627]],\n",
       " \n",
       "          [[ 0.3176,  0.3412,  0.3725,  ..., -0.9059, -0.9294, -0.8902],\n",
       "           [ 0.4118,  0.4745,  0.4275,  ..., -0.9137, -0.9294, -0.9059],\n",
       "           [ 0.4980,  0.5294,  0.4824,  ..., -0.9059, -0.9216, -0.9216],\n",
       "           ...,\n",
       "           [ 0.3412,  0.2784,  0.2627,  ..., -0.3961, -0.4275, -0.4588],\n",
       "           [ 0.3647,  0.3333,  0.2863,  ..., -0.3804, -0.3804, -0.3961],\n",
       "           [ 0.3882,  0.3725,  0.3176,  ..., -0.3569, -0.3569, -0.3804]]]]),\n",
       " 'labels': tensor([6, 6])}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate_fn(single_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=tensor(4.6280, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1365,  0.1620, -0.0223, -0.1143, -0.0091,  0.0963,  0.0333,  0.0601,\n",
       "          0.0912,  0.0556,  0.0271,  0.1042, -0.0219,  0.0306, -0.0393, -0.0080,\n",
       "         -0.0542, -0.0108, -0.0006,  0.1087, -0.0748,  0.1145, -0.0873,  0.0175,\n",
       "          0.0028,  0.0402, -0.0207, -0.1050,  0.0393, -0.1119,  0.0858,  0.0700,\n",
       "          0.2079,  0.1616,  0.0466,  0.0322, -0.0162,  0.0378, -0.0287, -0.0579,\n",
       "          0.0594, -0.0013, -0.0045,  0.0189,  0.0158, -0.0060,  0.1940, -0.1585,\n",
       "          0.0083, -0.1220, -0.1615,  0.0422, -0.0152, -0.0166, -0.1047, -0.0262,\n",
       "          0.0144,  0.1229, -0.0456,  0.0743,  0.0304, -0.2039,  0.0185,  0.1134,\n",
       "          0.0305,  0.1549, -0.1589,  0.0945,  0.1259,  0.0856, -0.0828, -0.0904,\n",
       "         -0.0898, -0.0174,  0.0313,  0.0354,  0.0230, -0.1014,  0.0152,  0.0344,\n",
       "         -0.1159, -0.1117,  0.0662,  0.1179, -0.0779,  0.1350, -0.0319,  0.0505,\n",
       "         -0.1316, -0.0474, -0.0665, -0.0066, -0.0134,  0.0536,  0.0452, -0.1686,\n",
       "          0.0357, -0.0118,  0.1457, -0.0710,  0.1032],\n",
       "        [ 0.0366,  0.0556, -0.0137, -0.1656, -0.0090,  0.0739, -0.0446,  0.1007,\n",
       "         -0.0920,  0.0444, -0.0102, -0.0698, -0.1062,  0.0905, -0.0900,  0.1204,\n",
       "         -0.1509,  0.0827, -0.1261,  0.1187,  0.0131,  0.1127,  0.1462,  0.0453,\n",
       "         -0.1192, -0.0616, -0.0737, -0.0078,  0.0745, -0.0538,  0.0300, -0.2822,\n",
       "          0.0087,  0.3359,  0.0989, -0.0393, -0.1764, -0.0209,  0.0560, -0.0405,\n",
       "          0.0323, -0.0294, -0.1065,  0.0538,  0.0117,  0.1993, -0.0987, -0.0698,\n",
       "          0.0792,  0.0495,  0.0622, -0.1118, -0.0335,  0.1133, -0.1798,  0.2170,\n",
       "          0.0649,  0.0884, -0.1315,  0.0037, -0.0107, -0.1050, -0.0190,  0.0721,\n",
       "         -0.0200, -0.1063, -0.0141,  0.0931,  0.0818,  0.0013, -0.0913, -0.0590,\n",
       "          0.0650,  0.0915, -0.1172, -0.0345,  0.0156, -0.0926, -0.0562,  0.0948,\n",
       "          0.0360,  0.0274,  0.1030,  0.0143, -0.0145, -0.0407,  0.0120,  0.0319,\n",
       "          0.1479,  0.0513,  0.0342,  0.0604, -0.2672, -0.2135,  0.0936, -0.0181,\n",
       "         -0.0566,  0.0828,  0.0910,  0.0048, -0.0291]],\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model(**collate_fn(single_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 0.03769832,  0.03013101,  0.020927  , ...,  0.12225489,\n",
       "        -0.06108126, -0.09649463],\n",
       "       [ 0.01888916,  0.04456399, -0.03133049, ...,  0.09108149,\n",
       "        -0.03305664, -0.00110111],\n",
       "       [ 0.08471692,  0.04713244, -0.07810733, ..., -0.00054852,\n",
       "        -0.09846766,  0.08669026],\n",
       "       ...,\n",
       "       [ 0.03736099, -0.06543357,  0.05543617, ..., -0.06087255,\n",
       "         0.08601949,  0.05294777],\n",
       "       [-0.02205896,  0.03141039,  0.00066247, ...,  0.17388818,\n",
       "        -0.01842239,  0.02367421],\n",
       "       [-0.00704368,  0.0352019 , -0.14236571, ...,  0.03917494,\n",
       "        -0.04025203,  0.07081404]], dtype=float32), label_ids=array([6, 6, 6, 6, 6, 6, 6, 6, 6, 6]), metrics={'test_loss': 4.671304225921631, 'test_model_preparation_time': 0.0023, 'test_runtime': 0.0835, 'test_samples_per_second': 119.83, 'test_steps_per_second': 59.915})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model = model,\n",
    "    data_collator = collate_fn,\n",
    "    args = transformers.TrainingArguments(\n",
    "        output_dir= \"asdf\", # 아무거나 써야함. \n",
    "        remove_unused_columns= False, # 이 부분이 포인트!!\n",
    "        per_device_eval_batch_size= 2\n",
    "    )        \n",
    ")\n",
    "trainer.predict(trainer_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. IMDB -- DataCollatorWithPadding 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref: <https://huggingface.co/docs/transformers/tasks/sequence_classification>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*1. 데이터준비: `\"guebin/imdb-tiny\"` $\\to$ `trainer_input`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = datasets.load_dataset(\"guebin/imdb-tiny\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\") \n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "tokenized_imdb = imdb.map(preprocess_function,batched=True)\n",
    "trainer_input = tokenized_imdb['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2. 모델준비: `\"distilbert/distilbert-base-uncased\"` $\\to$`model`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*3. 데이터콜렉터: `collate_fn` 직접설계* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(single_batch):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DefaultDataCollator()` 와 동일한 역할을 하는 `collate_fn`을 설계하라. 이를 이용하여 적당한 `trainer`를 만들어 \n",
    "\n",
    "```Python\n",
    "trainer.predict(trainer_input)\n",
    "```\n",
    "\n",
    "이 정상동작하는지 확인하라. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(풀이)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List Overview:\n",
      "Total items: 2\n",
      "\n",
      "1. list[0]\n",
      "   - Type: dict\n",
      "   - Length: 3\n",
      "   - Values: {'label': 0, 'input_ids': [101, 2040, 2024, 2122, 1000, 2027, 1000, 1011, 1996, 5889, 1029, 1996, 16587, 1029, 5121, 2481, 1005, 1056, 2022, 1996, 4378, 1011, 2023, 2003, 2426, 1996, 2087, 2250, 1011, 23893, 2098, 5453, 1999, 4598, 1012, 2009, 1005, 1055, 1996, 2785, 1997, 3185, 2008, 3504, 2066, 2009, 2001, 1037, 2843, 1997, 4569, 2000, 5607, 2205, 2172, 4569, 1010, 6343, 2003, 2893, 2151, 5025, 2147, 2589, 1010, 1998, 2008, 2471, 2467, 3084, 2005, 1037, 3185, 2008, 1005, 1055, 2053, 4569, 2000, 3422, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 23168, 2123, 2015, 7877, 2061, 2004, 2000, 8691, ... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "2. list[1]\n",
      "   - Type: dict\n",
      "   - Length: 3\n",
      "   - Values: {'label': 0, 'input_ids': [101, 2023, 2003, 2056, 2000, 2022, 1037, 3167, 2143, 2005, 2848, 22132, 5280, 18891, 10649, 1012, 2002, 2241, 2009, 2006, 2010, 2166, 2021, 2904, 2477, 2105, 2000, 4906, 1996, 3494, 1010, 2040, 2024, 18145, 1012, 2122, 18145, 3058, 3376, 4275, 1998, 2031, 2053, 3291, 2893, 2068, 1012, 4165, 2062, 2066, 1037, 19965, 18286, 12127, 2084, 1037, 6317, 1010, 2987, 1005, 1056, 2009, 1029, 2023, 2972, 3185, 2001, 2517, 2011, 2848, 1010, 1998, 2009, 3065, 2129, 2041, 1997, 3543, 2007, 2613, 2111, 2002, 2001, 1012, 2017, 1005, 2128, 4011, 2000, 4339, 2054, 2017, 2113, 1010, 1998, 2002, 2106, ... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "batch_maker = transformers.Trainer(\n",
    "    model = model,\n",
    "    data_collator = lambda x: x \n",
    ")\n",
    "_batched_data = batch_maker.get_test_dataloader(trainer_input)\n",
    "batched_data = list(_batched_data)\n",
    "single_batch = batched_data[-1]\n",
    "show(single_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = dict()\n",
    "model_input['labels'] = torch.tensor([l['label'] for l in single_batch])\n",
    "model_input['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2040,  2024,  ..., 22132,  7847,   102],\n",
       "        [  101,  2023,  2003,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_list = [torch.tensor(l['input_ids']) for l in single_batch]\n",
    "model_input['input_ids'] = torch.nn.utils.rnn.pad_sequence(input_ids_list).t()\n",
    "model_input['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask_list = [torch.tensor(l['attention_mask']) for l in single_batch]\n",
    "model_input['attention_mask'] = torch.nn.utils.rnn.pad_sequence(attention_mask_list).t()\n",
    "model_input['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(single_batch):\n",
    "    model_input = dict()\n",
    "    model_input['labels'] = torch.tensor([l['label'] for l in single_batch])\n",
    "    input_ids_list = [torch.tensor(l['input_ids']) for l in single_batch]\n",
    "    model_input['input_ids'] = torch.nn.utils.rnn.pad_sequence(input_ids_list).t()\n",
    "    attention_mask_list = [torch.tensor(l['attention_mask']) for l in single_batch]\n",
    "    model_input['attention_mask'] = torch.nn.utils.rnn.pad_sequence(attention_mask_list).t()\n",
    "    return model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.6734, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0025, -0.0352],\n",
       "        [ 0.0139, -0.0282]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model(**collate_fn(single_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model = model,\n",
    "    data_collator = collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 0.08966769, -0.06166375],\n",
       "       [ 0.01500158, -0.06875467],\n",
       "       [ 0.03916247, -0.0061186 ],\n",
       "       [ 0.0405627 , -0.05440582],\n",
       "       [-0.0063449 , -0.03747641],\n",
       "       [ 0.03792842, -0.04466516],\n",
       "       [ 0.03690895, -0.08592646],\n",
       "       [ 0.00534204, -0.05428012],\n",
       "       [ 0.00248959, -0.03522498],\n",
       "       [ 0.01390314, -0.02818924]], dtype=float32), label_ids=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), metrics={'test_loss': 0.6564630270004272, 'test_model_preparation_time': 0.0012, 'test_runtime': 0.0366, 'test_samples_per_second': 273.189, 'test_steps_per_second': 54.638})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(trainer_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 연습 -- `sms_spam`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sms', 'label'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=2\n",
    ")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "spam = datasets.load_dataset('guebin/spam-tiny')\n",
    "spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. 방법1: 고정패딩, `collate_fn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sms', 'label'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_transform_batch(example_batch):\n",
    "    # example_batch = {'sms':[xxx,xxxx,...], 'label':[yyy,yyyy,...]}\n",
    "    result = tokenizer(example_batch['sms'],padding=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2175,  2127, 18414, 17583,  2391,  1010,  4689,  1012,  1012,\n",
       "          2800,  2069,  1999, 11829,  2483,  1050,  2307,  2088,  2474,  1041,\n",
       "         28305,  1012,  1012,  1012, 25022,  2638,  2045,  2288, 26297, 28194,\n",
       "          1012,  1012,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [  101,  7929,  2474,  2099,  1012,  1012,  1012, 16644, 15536,  2546,\n",
       "          1057,  2006,  2072,  1012,  1012,  1012,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [  101,  2489,  4443,  1999,  1016,  1037,  1059,  2243,  2135,  4012,\n",
       "          2361,  2000,  2663,  6904,  2452,  2345,  1056, 25509,  2015,  7398,\n",
       "          2089,  2384,  1012,  3793,  6904,  2000,  6584, 12521,  2487,  2000,\n",
       "          4374,  4443,  3160,  1006,  2358,  2094, 19067,  2102,  3446,  1007,\n",
       "          1056,  1004,  1039,  1005,  1055,  6611,  5511, 19961, 22407, 18613,\n",
       "         23352,  7840, 15136,  1005,  1055,   102],\n",
       "        [  101,  1057, 24654,  2360,  2061,  2220,  7570,  2099,  1012,  1012,\n",
       "          1012,  1057,  1039,  2525,  2059,  2360,  1012,  1012,  1012,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [  101, 20976,  1045,  2123,  1005,  1056,  2228,  2002,  3632,  2000,\n",
       "          2149,  2546,  1010,  2002,  3268,  2105,  2182,  2295,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [  101,  2489,  5244,  2290,  4931,  2045,  9548,  2009,  1005,  1055,\n",
       "          2042,  1017,  2733,  1005,  1055,  2085,  1998,  2053,  2773,  2067,\n",
       "           999,  1045,  1005,  1040,  2066,  2070,  4569,  2017,  2039,  2005,\n",
       "          2009,  2145,  1029, 26419,  7929,   999, 22038,  2595,  2358,  2094,\n",
       "         10381,  5620,  2000,  4604,  1010, 14534,  1012,  2753,  2000, 22110,\n",
       "          2615,   102,     0,     0,     0,     0],\n",
       "        [  101,  2130,  2026,  2567,  2003,  2025,  2066,  2000,  3713,  2007,\n",
       "          2033,  1012,  2027,  7438,  2033,  2066,  8387,  7353,  1012,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [  101,  2004,  2566,  2115,  5227,  1005, 11463,  2571, 11463,  2571,\n",
       "          1006,  2030,  2226,  8117, 28987, 11231,  3070, 18447,  2063, 27617,\n",
       "          5575,  2226, 29525, 15464,  1007,  1005,  2038,  2042,  2275,  2004,\n",
       "          2115, 20587,  8525,  2638,  2005,  2035, 20587,  2015,  1012,  2811,\n",
       "          1008,  1023,  2000,  6100,  2115,  2814, 20587,  8525,  2638,   102,\n",
       "             0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam2 = spam.map(m_transform_batch,batched=True,batch_size=8)\n",
    "spam2.set_format(\"pt\")\n",
    "spam2['train'][:8]['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이때 `input_ids`, `attention_mask`는 매트릭스 O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([  101,  2175,  2127, 18414, 17583,  2391,  1010,  4689,  1012,  1012,\n",
       "          2800,  2069,  1999, 11829,  2483,  1050,  2307,  2088,  2474,  1041,\n",
       "         28305,  1012,  1012,  1012, 25022,  2638,  2045,  2288, 26297, 28194,\n",
       "          1012,  1012,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]),\n",
       " tensor([  101,  7929,  2474,  2099,  1012,  1012,  1012, 16644, 15536,  2546,\n",
       "          1057,  2006,  2072,  1012,  1012,  1012,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]),\n",
       " tensor([  101,  2489,  4443,  1999,  1016,  1037,  1059,  2243,  2135,  4012,\n",
       "          2361,  2000,  2663,  6904,  2452,  2345,  1056, 25509,  2015,  7398,\n",
       "          2089,  2384,  1012,  3793,  6904,  2000,  6584, 12521,  2487,  2000,\n",
       "          4374,  4443,  3160,  1006,  2358,  2094, 19067,  2102,  3446,  1007,\n",
       "          1056,  1004,  1039,  1005,  1055,  6611,  5511, 19961, 22407, 18613,\n",
       "         23352,  7840, 15136,  1005,  1055,   102]),\n",
       " tensor([  101,  1057, 24654,  2360,  2061,  2220,  7570,  2099,  1012,  1012,\n",
       "          1012,  1057,  1039,  2525,  2059,  2360,  1012,  1012,  1012,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]),\n",
       " tensor([  101, 20976,  1045,  2123,  1005,  1056,  2228,  2002,  3632,  2000,\n",
       "          2149,  2546,  1010,  2002,  3268,  2105,  2182,  2295,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]),\n",
       " tensor([  101,  2489,  5244,  2290,  4931,  2045,  9548,  2009,  1005,  1055,\n",
       "          2042,  1017,  2733,  1005,  1055,  2085,  1998,  2053,  2773,  2067,\n",
       "           999,  1045,  1005,  1040,  2066,  2070,  4569,  2017,  2039,  2005,\n",
       "          2009,  2145,  1029, 26419,  7929,   999, 22038,  2595,  2358,  2094,\n",
       "         10381,  5620,  2000,  4604,  1010, 14534,  1012,  2753,  2000, 22110,\n",
       "          2615,   102,     0,     0,     0,     0]),\n",
       " tensor([ 101, 2130, 2026, 2567, 2003, 2025, 2066, 2000, 3713, 2007, 2033, 1012,\n",
       "         2027, 7438, 2033, 2066, 8387, 7353, 1012,  102,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]),\n",
       " tensor([  101,  2004,  2566,  2115,  5227,  1005, 11463,  2571, 11463,  2571,\n",
       "          1006,  2030,  2226,  8117, 28987, 11231,  3070, 18447,  2063, 27617,\n",
       "          5575,  2226, 29525, 15464,  1007,  1005,  2038,  2042,  2275,  2004,\n",
       "          2115, 20587,  8525,  2638,  2005,  2035, 20587,  2015,  1012,  2811,\n",
       "          1008,  1023,  2000,  6100,  2115,  2814, 20587,  8525,  2638,   102,\n",
       "             0,     0,     0,     0,     0,     0]),\n",
       " tensor([  101,  3453,   999,   999,  2004,  1037, 11126,  2897,  8013,  2017,\n",
       "          2031,  2042,  3479,  2000,  4374,  2050,  1069, 21057,  2692,  3396,\n",
       "         10377,   999,  2000,  4366,  2655,  5641,  2692,  2575, 16576, 24096,\n",
       "         21472,  2487,  1012,  4366,  3642,  1047,  2140, 22022,  2487,  1012,\n",
       "          9398,  2260,  2847,  2069,  1012,   102])]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam2['train'][:9]['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이때 `input_ids`, `attention_mask`는 매트릭스 X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  3453,   999,   999,  2004,  1037, 11126,  2897,  8013,  2017,\n",
       "          2031,  2042,  3479,  2000,  4374,  2050,  1069, 21057,  2692,  3396,\n",
       "         10377,   999,  2000,  4366,  2655,  5641,  2692,  2575, 16576, 24096,\n",
       "         21472,  2487,  1012,  4366,  3642,  1047,  2140, 22022,  2487,  1012,\n",
       "          9398,  2260,  2847,  2069,  1012,   102],\n",
       "        [  101,  2018,  2115,  4684,  2340,  2706,  2030,  2062,  1029,  1057,\n",
       "          1054,  4709,  2000, 10651,  2000,  1996,  6745,  6120,  4684,  2015,\n",
       "          2007,  4950,  2005,  2489,   999,  2655,  1996,  4684, 10651,  2522,\n",
       "          2489,  2006,  5511,  8889, 24594, 20842,  2692, 14142,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam2['train'][8:]['input_ids'] # 텐서로 묶임.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이때 `input_ids`, `attention_mask`는 매트릭스 O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_input = spam2['train'].remove_columns('sms').rename_columns({'label':'labels'})\n",
    "trainer_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'labels': tensor(1, device='cuda:0'),\n",
       "  'input_ids': tensor([  101,  3453,   999,   999,  2004,  1037, 11126,  2897,  8013,  2017,\n",
       "           2031,  2042,  3479,  2000,  4374,  2050,  1069, 21057,  2692,  3396,\n",
       "          10377,   999,  2000,  4366,  2655,  5641,  2692,  2575, 16576, 24096,\n",
       "          21472,  2487,  1012,  4366,  3642,  1047,  2140, 22022,  2487,  1012,\n",
       "           9398,  2260,  2847,  2069,  1012,   102], device='cuda:0'),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         device='cuda:0')},\n",
       " {'labels': tensor(1, device='cuda:0'),\n",
       "  'input_ids': tensor([  101,  2018,  2115,  4684,  2340,  2706,  2030,  2062,  1029,  1057,\n",
       "           1054,  4709,  2000, 10651,  2000,  1996,  6745,  6120,  4684,  2015,\n",
       "           2007,  4950,  2005,  2489,   999,  2655,  1996,  4684, 10651,  2522,\n",
       "           2489,  2006,  5511,  8889, 24594, 20842,  2692, 14142,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0], device='cuda:0'),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         device='cuda:0')}]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_maker = transformers.Trainer(\n",
    "    model=model,\n",
    "    data_collator=lambda x: x,\n",
    ")\n",
    "batched_data = list(batch_maker.get_test_dataloader(trainer_input))\n",
    "single_batch = batched_data[-1]\n",
    "single_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(single_batch):\n",
    "    out = dict()\n",
    "    out['labels'] = torch.tensor([dct['labels'] for dct in single_batch])\n",
    "    out['input_ids'] = torch.stack([dct['input_ids'] for dct in single_batch])\n",
    "    out['attention_mask'] = torch.stack([dct['attention_mask'] for dct in single_batch])\n",
    "    return out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([1, 1]),\n",
       " 'input_ids': tensor([[  101,  3453,   999,   999,  2004,  1037, 11126,  2897,  8013,  2017,\n",
       "           2031,  2042,  3479,  2000,  4374,  2050,  1069, 21057,  2692,  3396,\n",
       "          10377,   999,  2000,  4366,  2655,  5641,  2692,  2575, 16576, 24096,\n",
       "          21472,  2487,  1012,  4366,  3642,  1047,  2140, 22022,  2487,  1012,\n",
       "           9398,  2260,  2847,  2069,  1012,   102],\n",
       "         [  101,  2018,  2115,  4684,  2340,  2706,  2030,  2062,  1029,  1057,\n",
       "           1054,  4709,  2000, 10651,  2000,  1996,  6745,  6120,  4684,  2015,\n",
       "           2007,  4950,  2005,  2489,   999,  2655,  1996,  4684, 10651,  2522,\n",
       "           2489,  2006,  5511,  8889, 24594, 20842,  2692, 14142,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0]], device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]],\n",
       "        device='cuda:0')}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate_fn(single_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model = model,\n",
    "    data_collator = collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-0.04641762, -0.03605646],\n",
       "       [-0.02506144, -0.01335288],\n",
       "       [ 0.02076511, -0.02322948],\n",
       "       [-0.00962518,  0.03754075],\n",
       "       [-0.03287388, -0.03730439],\n",
       "       [-0.02798662, -0.02491809],\n",
       "       [-0.09155686, -0.04009426],\n",
       "       [-0.0420014 , -0.02552293],\n",
       "       [-0.0209422 , -0.01544815],\n",
       "       [-0.01533531,  0.01857282]], dtype=float32), label_ids=array([0, 0, 1, 0, 0, 1, 0, 0, 1, 1]), metrics={'test_loss': 0.6999672651290894, 'test_model_preparation_time': 0.0022, 'test_runtime': 0.0163, 'test_samples_per_second': 613.543, 'test_steps_per_second': 122.709})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(trainer_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. 방법2: 고정패딩, DefaultDataCollator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 방법1과 거의 동일 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sms', 'label'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-0.04641762, -0.03605646],\n",
       "       [-0.02506144, -0.01335288],\n",
       "       [ 0.02076511, -0.02322948],\n",
       "       [-0.00962518,  0.03754075],\n",
       "       [-0.03287388, -0.03730439],\n",
       "       [-0.02798662, -0.02491809],\n",
       "       [-0.09155686, -0.04009426],\n",
       "       [-0.0420014 , -0.02552293],\n",
       "       [-0.0209422 , -0.01544815],\n",
       "       [-0.01533531,  0.01857282]], dtype=float32), label_ids=array([0, 0, 1, 0, 0, 1, 0, 0, 1, 1]), metrics={'test_loss': 0.6999672651290894, 'test_model_preparation_time': 0.0011, 'test_runtime': 0.0125, 'test_samples_per_second': 800.333, 'test_steps_per_second': 160.067})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def m_transform_batch(example_batch):\n",
    "    # example_batch = {'sms':[xxx,xxxx,...], 'label':[yyy,yyyy,...]}\n",
    "    result = tokenizer(example_batch['sms'],padding=True)\n",
    "    return result\n",
    "spam2 = spam.map(m_transform_batch,batched=True,batch_size=8)\n",
    "spam2.set_format(\"pt\")\n",
    "trainer_input = spam2['train'].remove_columns('sms').rename_columns({'label':'labels'})\n",
    "\n",
    "# def collate_fn(single_batch):\n",
    "#     out = dict()\n",
    "#     out['labels'] = torch.tensor([dct['labels'] for dct in single_batch])\n",
    "#     out['input_ids'] = torch.stack([dct['input_ids'] for dct in single_batch])\n",
    "#     out['attention_mask'] = torch.stack([dct['attention_mask'] for dct in single_batch])\n",
    "#     return out \n",
    "\n",
    "data_collator = transformers.DefaultDataCollator()\n",
    "trainer = transformers.Trainer(\n",
    "    model = model,\n",
    "    data_collator = data_collator\n",
    ")\n",
    "trainer.predict(trainer_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. 방법3: 동적패딩, `DataCollatorWithPadding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sms', 'label'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_transform(examples):\n",
    "    #examples = {'input_ids':[xxx,xxxx,....], 'label':[yyy,yyyy]}\n",
    "    out = tokenizer(examples['sms'],truncation=True)\n",
    "    out['labels'] = torch.tensor(examples['label'])\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sms', 'label'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_input = spam.with_transform(w_transform)['train']\n",
    "trainer_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_maker = transformers.Trainer(\n",
    "    model = model,\n",
    "    data_collator= lambda x: x,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir = \"asdf\",\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    ")\n",
    "_batched_data = batch_maker.get_eval_dataloader(trainer_input)\n",
    "batched_data = list(_batched_data)\n",
    "single_batch = batched_data[-1]\n",
    "#single_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  3453,   999,   999,  2004,  1037, 11126,  2897,  8013,  2017,\n",
       "          2031,  2042,  3479,  2000,  4374,  2050,  1069, 21057,  2692,  3396,\n",
       "         10377,   999,  2000,  4366,  2655,  5641,  2692,  2575, 16576, 24096,\n",
       "         21472,  2487,  1012,  4366,  3642,  1047,  2140, 22022,  2487,  1012,\n",
       "          9398,  2260,  2847,  2069,  1012,   102],\n",
       "        [  101,  2018,  2115,  4684,  2340,  2706,  2030,  2062,  1029,  1057,\n",
       "          1054,  4709,  2000, 10651,  2000,  1996,  6745,  6120,  4684,  2015,\n",
       "          2007,  4950,  2005,  2489,   999,  2655,  1996,  4684, 10651,  2522,\n",
       "          2489,  2006,  5511,  8889, 24594, 20842,  2692, 14142,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([1, 1])}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = transformers.DataCollatorWithPadding(tokenizer)\n",
    "data_collator(single_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.6834, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0209, -0.0154],\n",
       "        [-0.0153,  0.0186]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model(**data_collator(single_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-0.04641762, -0.03605646],\n",
       "       [-0.02506144, -0.01335288],\n",
       "       [ 0.02076511, -0.02322948],\n",
       "       [-0.00962518,  0.03754075],\n",
       "       [-0.03287388, -0.03730439],\n",
       "       [-0.02798662, -0.02491809],\n",
       "       [-0.09155686, -0.04009426],\n",
       "       [-0.0420014 , -0.02552293],\n",
       "       [-0.0209422 , -0.01544815],\n",
       "       [-0.01533531,  0.01857282]], dtype=float32), label_ids=array([0, 0, 1, 0, 0, 1, 0, 0, 1, 1]), metrics={'test_loss': 0.6999672651290894, 'test_model_preparation_time': 0.0007, 'test_runtime': 0.0093, 'test_samples_per_second': 1072.877, 'test_steps_per_second': 214.575})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model = model,\n",
    "    data_collator = data_collator,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=\"asdf\",\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    ")\n",
    "trainer.predict(trainer_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. 방법4: 동적패딩, 전처리X $(\\star)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sms', 'label'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_input = spam['train']\n",
    "trainer_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(single_batch):\n",
    "    # single_batch = [Dict, Dict, Dict,...]\n",
    "    out = tokenizer([dct['sms'] for dct in single_batch],padding=True,return_tensors=\"pt\",truncation=True)\n",
    "    out['labels'] = torch.tensor([dct['label'] for dct in single_batch])\n",
    "    return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-0.04641762, -0.03605646],\n",
       "       [-0.02506144, -0.01335288],\n",
       "       [ 0.02076511, -0.02322948],\n",
       "       [-0.00962518,  0.03754075],\n",
       "       [-0.03287388, -0.03730439],\n",
       "       [-0.02798662, -0.02491809],\n",
       "       [-0.09155686, -0.04009426],\n",
       "       [-0.0420014 , -0.02552293],\n",
       "       [-0.0209422 , -0.01544815],\n",
       "       [-0.01533531,  0.01857282]], dtype=float32), label_ids=array([0, 0, 1, 0, 0, 1, 0, 0, 1, 1]), metrics={'test_loss': 0.6999672651290894, 'test_model_preparation_time': 0.0011, 'test_runtime': 0.012, 'test_samples_per_second': 830.013, 'test_steps_per_second': 166.003})"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    data_collator=collate_fn,\n",
    "    args= transformers.TrainingArguments(\n",
    "        output_dir= 'asdf',\n",
    "        remove_unused_columns= False\n",
    "    )\n",
    ")\n",
    "trainer.predict(trainer_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
