{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"(강의) Model\"\n",
    "author: \"신록예찬\"\n",
    "date: \"11/08/2024\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. 강의영상 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgb3/anaconda3/envs/hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import huggingface_hub\n",
    "import torch\n",
    "import torchvision\n",
    "import pytorchvideo.data\n",
    "import PIL\n",
    "import tarfile\n",
    "import mp2024pkg as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model 입력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 아래중 하나의 방법순으로.. \n",
    "\n",
    "- 방법1: `model.forward?` 에서 시그니처를 확인\n",
    "- 방법2: `model.forward?` 에서 사용예제를 확인\n",
    "- 방법3: 인터넷을 활용한 외부 자료 확인 (공식문서, 공식튜토리얼, 신뢰할만한 블로그, ChatGPT등)\n",
    "- 방법4: `model.forward??` 를 보고 모든 코드를 뜯어봄 <--- 하지마세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 모델의 입력이 어떤형태로 정리되어야 하는지 알아내는 확실한 방법은 없음 \n",
    "\n",
    "- 방법1,2,3 은 다른사람의 호의에 기대해야함.\n",
    "- 방법4는 사실상 불가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# 예제1` -- 텍스트분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model1 = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*모델의 기본정보(config)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"distilbert/distilbert-base-uncased\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.46.2\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최대단어수 = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*모델의 입력파악*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mattention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mhead_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minputs_embeds\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequenceClassifierOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "The [`DistilBertForSequenceClassification`] forward method, overrides the `__call__` special method.\n",
      "\n",
      "<Tip>\n",
      "\n",
      "Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
      "instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
      "the latter silently ignores them.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "Args:\n",
      "    input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "        Indices of input sequence tokens in the vocabulary.\n",
      "\n",
      "        Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      "        [`PreTrainedTokenizer.__call__`] for details.\n",
      "\n",
      "        [What are input IDs?](../glossary#input-ids)\n",
      "    attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "        Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
      "\n",
      "        - 1 for tokens that are **not masked**,\n",
      "        - 0 for tokens that are **masked**.\n",
      "\n",
      "        [What are attention masks?](../glossary#attention-mask)\n",
      "    head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
      "        Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
      "\n",
      "        - 1 indicates the head is **not masked**,\n",
      "        - 0 indicates the head is **masked**.\n",
      "\n",
      "    inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
      "        Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
      "        is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
      "        model's internal embedding lookup matrix.\n",
      "    output_attentions (`bool`, *optional*):\n",
      "        Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
      "        tensors for more detail.\n",
      "    output_hidden_states (`bool`, *optional*):\n",
      "        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
      "        more detail.\n",
      "    return_dict (`bool`, *optional*):\n",
      "        Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      "\n",
      "    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
      "        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
      "        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
      "        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
      "    \n",
      "Returns:\n",
      "    [`transformers.modeling_outputs.SequenceClassifierOutput`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.SequenceClassifierOutput`] or a tuple of\n",
      "    `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
      "    elements depending on the configuration ([`DistilBertConfig`]) and inputs.\n",
      "\n",
      "    - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) -- Classification (or regression if config.num_labels==1) loss.\n",
      "    - **logits** (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) -- Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
      "    - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
      "      one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
      "\n",
      "      Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
      "    - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      "      sequence_length)`.\n",
      "\n",
      "      Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
      "      heads.\n",
      "\n",
      "Example of single-label classification:\n",
      "\n",
      "```python\n",
      ">>> import torch\n",
      ">>> from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
      "\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
      ">>> model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
      "\n",
      ">>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
      "\n",
      ">>> with torch.no_grad():\n",
      "...     logits = model(**inputs).logits\n",
      "\n",
      ">>> predicted_class_id = logits.argmax().item()\n",
      "\n",
      ">>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
      ">>> num_labels = len(model.config.id2label)\n",
      ">>> model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)\n",
      "\n",
      ">>> labels = torch.tensor([1])\n",
      ">>> loss = model(**inputs, labels=labels).loss\n",
      "```\n",
      "\n",
      "Example of multi-label classification:\n",
      "\n",
      "```python\n",
      ">>> import torch\n",
      ">>> from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
      "\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
      ">>> model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", problem_type=\"multi_label_classification\")\n",
      "\n",
      ">>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
      "\n",
      ">>> with torch.no_grad():\n",
      "...     logits = model(**inputs).logits\n",
      "\n",
      ">>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n",
      "\n",
      ">>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
      ">>> num_labels = len(model.config.id2label)\n",
      ">>> model = DistilBertForSequenceClassification.from_pretrained(\n",
      "...     \"distilbert-base-uncased\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n",
      "... )\n",
      "\n",
      ">>> labels = torch.sum(\n",
      "...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n",
      "... ).to(torch.float)\n",
      ">>> loss = model(**inputs, labels=labels).loss\n",
      "```\n",
      "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "model1.forward?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시1 -- 입력나열, loss O*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.6945, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0012,  0.0777],\n",
       "        [-0.0101,  0.0711]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.forward(\n",
    "    input_ids = torch.tensor([[1,2,3],[2,3,4]]),\n",
    "    labels = torch.tensor([1,0])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시2 -- `**딕셔너리`, loss O*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.6945, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0012,  0.0777],\n",
       "        [-0.0101,  0.0711]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_input = dict(\n",
    "    input_ids = torch.tensor([[1,2,3],[2,3,4]]),\n",
    "    labels = torch.tensor([1,0])\n",
    ")\n",
    "model1.forward(**model1_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시3 -- 입력나열, loss X*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0012,  0.0777],\n",
       "        [-0.0101,  0.0711]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1(\n",
    "    input_ids = torch.tensor([[1,2,3],[2,3,4]])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시4 -- `**딕셔너리`, loss X*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0012,  0.0777],\n",
       "        [-0.0101,  0.0711]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_input = dict(\n",
    "    input_ids = torch.tensor([[1,2,3],[2,3,4]]),\n",
    ")\n",
    "model1(**model1_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시5 -- 초간단, loss X*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0012,  0.0777],\n",
       "        [-0.0101,  0.0711]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1(\n",
    "    torch.tensor([[1,2,3],[2,3,4]])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 사용예시1~5에서 `model1()` 대신에 `model1.forward()`를 사용해도 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# 예제2` -- 이미지분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model2 = transformers.AutoModelForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=3 # 그냥 대충 3이라고 했음.. 별 이유는 없음\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*모델의 기본정보(config)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"google/vit-base-patch16-224-in21k\",\n",
       "  \"architectures\": [\n",
       "    \"ViTModel\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"encoder_stride\": 16,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"image_size\": 224,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"vit\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"patch_size\": 16,\n",
       "  \"qkv_bias\": true,\n",
       "  \"transformers_version\": \"4.46.2\"\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이미지크기: 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*모델의 입력파악*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpixel_values\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mhead_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minterpolate_pos_encoding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageClassifierOutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "The [`ViTForImageClassification`] forward method, overrides the `__call__` special method.\n",
      "\n",
      "<Tip>\n",
      "\n",
      "Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
      "instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
      "the latter silently ignores them.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "Args:\n",
      "    pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n",
      "        Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See [`ViTImageProcessor.__call__`]\n",
      "        for details.\n",
      "\n",
      "    head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
      "        Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
      "\n",
      "        - 1 indicates the head is **not masked**,\n",
      "        - 0 indicates the head is **masked**.\n",
      "\n",
      "    output_attentions (`bool`, *optional*):\n",
      "        Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
      "        tensors for more detail.\n",
      "    output_hidden_states (`bool`, *optional*):\n",
      "        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
      "        more detail.\n",
      "    interpolate_pos_encoding (`bool`, *optional*):\n",
      "        Whether to interpolate the pre-trained position encodings.\n",
      "    return_dict (`bool`, *optional*):\n",
      "        Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      "\n",
      "    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
      "        Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n",
      "        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
      "        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
      "    \n",
      "Returns:\n",
      "    [`transformers.modeling_outputs.ImageClassifierOutput`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.ImageClassifierOutput`] or a tuple of\n",
      "    `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
      "    elements depending on the configuration ([`ViTConfig`]) and inputs.\n",
      "\n",
      "    - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) -- Classification (or regression if config.num_labels==1) loss.\n",
      "    - **logits** (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) -- Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
      "    - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
      "      one for the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`. Hidden-states\n",
      "      (also called feature maps) of the model at the output of each stage.\n",
      "    - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, patch_size,\n",
      "      sequence_length)`.\n",
      "\n",
      "      Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
      "      heads.\n",
      "\n",
      "Example:\n",
      "\n",
      "```python\n",
      ">>> from transformers import AutoImageProcessor, ViTForImageClassification\n",
      ">>> import torch\n",
      ">>> from datasets import load_dataset\n",
      "\n",
      ">>> dataset = load_dataset(\"huggingface/cats-image\", trust_remote_code=True)\n",
      ">>> image = dataset[\"test\"][\"image\"][0]\n",
      "\n",
      ">>> image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
      ">>> model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
      "\n",
      ">>> inputs = image_processor(image, return_tensors=\"pt\")\n",
      "\n",
      ">>> with torch.no_grad():\n",
      "...     logits = model(**inputs).logits\n",
      "\n",
      ">>> # model predicts one of the 1000 ImageNet classes\n",
      ">>> predicted_label = logits.argmax(-1).item()\n",
      ">>> print(model.config.id2label[predicted_label])\n",
      "Egyptian cat\n",
      "```\n",
      "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/models/vit/modeling_vit.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "model2.forward?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시1 -- 입력나열, loss O*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=tensor(1.1209, grad_fn=<NllLossBackward0>), logits=tensor([[0.0642, 0.1145, 0.0695],\n",
       "        [0.0579, 0.1165, 0.0758]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2(\n",
    "    pixel_values = torch.randn(2,3,224,224),\n",
    "    labels = torch.tensor([0,0])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시2 -- `**딕셔너리`, loss O*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=tensor(1.1137, grad_fn=<NllLossBackward0>), logits=tensor([[0.0601, 0.1022, 0.0659],\n",
       "        [0.0666, 0.1041, 0.0707]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2_input = dict(\n",
    "    pixel_values = torch.randn(2,3,224,224),\n",
    "    labels = torch.tensor([0,0])\n",
    ")\n",
    "model2(**model2_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시3 -- 입력나열, loss X*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=None, logits=tensor([[0.0973, 0.1043, 0.0674],\n",
       "        [0.0890, 0.1302, 0.0539]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2(\n",
    "    pixel_values = torch.randn(2,3,224,224),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시4 -- `**딕셔너리`, loss X*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=None, logits=tensor([[0.0769, 0.1314, 0.0494],\n",
       "        [0.0713, 0.0917, 0.0259]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2_input = dict(\n",
    "    pixel_values = torch.randn(2,3,224,224),\n",
    ")\n",
    "model2(**model2_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시5 -- 초간단, loss X*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=None, logits=tensor([[0.0644, 0.1262, 0.0855],\n",
       "        [0.0257, 0.1343, 0.0867]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2(\n",
    "    torch.randn(2,3,224,224),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# 예제3` -- 동영상분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model3 = transformers.VideoMAEForVideoClassification.from_pretrained(\n",
    "    \"MCG-NJU/videomae-base\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*모델의 기본정보(config)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoMAEConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"MCG-NJU/videomae-base\",\n",
       "  \"architectures\": [\n",
       "    \"VideoMAEForPreTraining\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"decoder_hidden_size\": 384,\n",
       "  \"decoder_intermediate_size\": 1536,\n",
       "  \"decoder_num_attention_heads\": 6,\n",
       "  \"decoder_num_hidden_layers\": 4,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"image_size\": 224,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"videomae\",\n",
       "  \"norm_pix_loss\": true,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_frames\": 16,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"patch_size\": 16,\n",
       "  \"qkv_bias\": true,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.46.2\",\n",
       "  \"tubelet_size\": 2,\n",
       "  \"use_mean_pooling\": false\n",
       "}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이미지크기: 224\n",
    "- 프레임: 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*모델의 입력파악*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpixel_values\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mhead_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageClassifierOutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "The [`VideoMAEForVideoClassification`] forward method, overrides the `__call__` special method.\n",
      "\n",
      "<Tip>\n",
      "\n",
      "Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
      "instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
      "the latter silently ignores them.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "Args:\n",
      "    pixel_values (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels, height, width)`):\n",
      "        Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See\n",
      "        [`VideoMAEImageProcessor.__call__`] for details.\n",
      "\n",
      "    head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
      "        Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
      "\n",
      "        - 1 indicates the head is **not masked**,\n",
      "        - 0 indicates the head is **masked**.\n",
      "\n",
      "    output_attentions (`bool`, *optional*):\n",
      "        Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
      "        tensors for more detail.\n",
      "    output_hidden_states (`bool`, *optional*):\n",
      "        Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
      "        more detail.\n",
      "    return_dict (`bool`, *optional*):\n",
      "        Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      "\n",
      "    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
      "        Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n",
      "        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
      "        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
      "\n",
      "\n",
      "    Returns:\n",
      "        [`transformers.modeling_outputs.ImageClassifierOutput`] or `tuple(torch.FloatTensor)`: A [`transformers.modeling_outputs.ImageClassifierOutput`] or a tuple of\n",
      "        `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
      "        elements depending on the configuration ([`VideoMAEConfig`]) and inputs.\n",
      "\n",
      "        - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) -- Classification (or regression if config.num_labels==1) loss.\n",
      "        - **logits** (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) -- Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
      "        - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
      "          one for the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`. Hidden-states\n",
      "          (also called feature maps) of the model at the output of each stage.\n",
      "        - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, patch_size,\n",
      "          sequence_length)`.\n",
      "\n",
      "          Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
      "          heads.\n",
      "  \n",
      "\n",
      "    Examples:\n",
      "\n",
      "    ```python\n",
      "    >>> import av\n",
      "    >>> import torch\n",
      "    >>> import numpy as np\n",
      "\n",
      "    >>> from transformers import AutoImageProcessor, VideoMAEForVideoClassification\n",
      "    >>> from huggingface_hub import hf_hub_download\n",
      "\n",
      "    >>> np.random.seed(0)\n",
      "\n",
      "\n",
      "    >>> def read_video_pyav(container, indices):\n",
      "    ...     '''\n",
      "    ...     Decode the video with PyAV decoder.\n",
      "    ...     Args:\n",
      "    ...         container (`av.container.input.InputContainer`): PyAV container.\n",
      "    ...         indices (`List[int]`): List of frame indices to decode.\n",
      "    ...     Returns:\n",
      "    ...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
      "    ...     '''\n",
      "    ...     frames = []\n",
      "    ...     container.seek(0)\n",
      "    ...     start_index = indices[0]\n",
      "    ...     end_index = indices[-1]\n",
      "    ...     for i, frame in enumerate(container.decode(video=0)):\n",
      "    ...         if i > end_index:\n",
      "    ...             break\n",
      "    ...         if i >= start_index and i in indices:\n",
      "    ...             frames.append(frame)\n",
      "    ...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
      "\n",
      "\n",
      "    >>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
      "    ...     '''\n",
      "    ...     Sample a given number of frame indices from the video.\n",
      "    ...     Args:\n",
      "    ...         clip_len (`int`): Total number of frames to sample.\n",
      "    ...         frame_sample_rate (`int`): Sample every n-th frame.\n",
      "    ...         seg_len (`int`): Maximum allowed index of sample's last frame.\n",
      "    ...     Returns:\n",
      "    ...         indices (`List[int]`): List of sampled frame indices\n",
      "    ...     '''\n",
      "    ...     converted_len = int(clip_len * frame_sample_rate)\n",
      "    ...     end_idx = np.random.randint(converted_len, seg_len)\n",
      "    ...     start_idx = end_idx - converted_len\n",
      "    ...     indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
      "    ...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
      "    ...     return indices\n",
      "\n",
      "\n",
      "    >>> # video clip consists of 300 frames (10 seconds at 30 FPS)\n",
      "    >>> file_path = hf_hub_download(\n",
      "    ...     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n",
      "    ... )\n",
      "    >>> container = av.open(file_path)\n",
      "\n",
      "    >>> # sample 16 frames\n",
      "    >>> indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
      "    >>> video = read_video_pyav(container, indices)\n",
      "\n",
      "    >>> image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n",
      "    >>> model = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n",
      "\n",
      "    >>> inputs = image_processor(list(video), return_tensors=\"pt\")\n",
      "\n",
      "    >>> with torch.no_grad():\n",
      "    ...     outputs = model(**inputs)\n",
      "    ...     logits = outputs.logits\n",
      "\n",
      "    >>> # model predicts one of the 400 Kinetics-400 classes\n",
      "    >>> predicted_label = logits.argmax(-1).item()\n",
      "    >>> print(model.config.id2label[predicted_label])\n",
      "    eating spaghetti\n",
      "    ```\n",
      "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/models/videomae/modeling_videomae.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "model3.forward?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시1 -- 입력나열, loss O*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=tensor(0.8626, grad_fn=<NllLossBackward0>), logits=tensor([[-0.1358,  0.3941],\n",
       "        [-0.2414,  0.6883]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3(\n",
    "    pixel_values = torch.randn(2,16,3,224,224),\n",
    "    labels = torch.tensor([1,0])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시2 -- `**딕셔너리`, loss O*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=tensor(0.6844, grad_fn=<NllLossBackward0>), logits=tensor([[-0.2486,  0.5740],\n",
       "        [-0.1603,  0.3884]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3_input = dict(\n",
    "    pixel_values = torch.randn(2,16,3,224,224),\n",
    "    labels = torch.tensor([1,0])\n",
    ")\n",
    "model3(**model3_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시3 -- 입력나열, loss X*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=None, logits=tensor([[-0.0368,  0.5111],\n",
       "        [-0.0464,  0.4651]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3(\n",
    "    pixel_values = torch.randn(2,16,3,224,224),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시4 -- `**딕셔너리`, loss X*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=None, logits=tensor([[-0.2656,  0.5812],\n",
       "        [-0.1089,  0.5978]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3_input = dict(\n",
    "    pixel_values = torch.randn(2,16,3,224,224),\n",
    ")\n",
    "model3(**model3_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사용예시5 -- 초간단, loss X*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=None, logits=tensor([[-0.2789,  0.5706],\n",
       "        [-0.2000,  0.6558]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3(\n",
    "    torch.randn(2,16,3,224,224),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model 사용 연습 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. 텍스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model1 = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=2\n",
    ")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# 예제1` -- imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb = datasets.load_dataset('imdb')\n",
    "d = imdb['train'].select(range(3))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(풀이1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*실패*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 363 at dim 1 (got 304)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model1(\n\u001b[0;32m----> 2\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      3\u001b[0m     labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 363 at dim 1 (got 304)"
     ]
    }
   ],
   "source": [
    "model1(\n",
    "    input_ids = torch.tensor(tokenizer(d['text'])['input_ids']),\n",
    "    labels = torch.tensor([0,0,1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*원인분석*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1 - Type: list, Length: 3, Content: [[101, 1045, 12524, 1045, ... // ... , 7987, 1013, 1028, 102]]\n",
      "     Level 2 - Type: list, Length: 363, Content: [101, 1045, 12524, 1045,  ... // ... 7, 1037, 5436, 1012, 102]\n",
      "     Level 2 - Type: list, Length: 304, Content: [101, 1000, 1045, 2572, 8 ... // ... 5, 1055, 4230, 1012, 102]\n",
      "     Level 2 - Type: list, Length: 133, Content: [101, 2065, 2069, 2000, 4 ... // ... 6, 7987, 1013, 1028, 102]\n"
     ]
    }
   ],
   "source": [
    "mp.show_list(tokenizer(d['text'])['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1 - Type: list, Length: 3, Content: [[101, 1045, 12524, 1045, ... // ...  0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "     Level 2 - Type: list, Length: 363, Content: [101, 1045, 12524, 1045,  ... // ... 7, 1037, 5436, 1012, 102]\n",
      "     Level 2 - Type: list, Length: 363, Content: [101, 1000, 1045, 2572, 8 ... // ... , 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "     Level 2 - Type: list, Length: 363, Content: [101, 2065, 2069, 2000, 4 ... // ... , 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "mp.show_list(\n",
    "    tokenizer(d['text'],padding=True)['input_ids']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*성공*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.7086, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0432,  0.0316],\n",
       "        [-0.0439,  0.0662],\n",
       "        [-0.0688,  0.0301]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1(\n",
    "    input_ids = torch.tensor(tokenizer(d['text'],padding=True)['input_ids']),\n",
    "    labels = torch.tensor([0,0,1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(풀이2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1045, 12524,  ...,  5436,  1012,   102],\n",
       "        [  101,  1000,  1045,  ...,     0,     0,     0],\n",
       "        [  101,  2065,  2069,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(d['text'],padding=True,return_tensors='pt')['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.7086, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0432,  0.0316],\n",
       "        [-0.0439,  0.0662],\n",
       "        [-0.0688,  0.0301]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1(\n",
    "    input_ids = tokenizer(d['text'],padding=True,return_tensors='pt')['input_ids'],\n",
    "    labels = torch.tensor([0,0,1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.7086, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0432,  0.0316],\n",
       "        [-0.0439,  0.0662],\n",
       "        [-0.0688,  0.0301]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1(\n",
    "    tokenizer(d['text'],padding=True,return_tensors='pt')['input_ids'],\n",
    "    labels = torch.tensor([0,0,1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# 예제2` -- emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion = datasets.load_dataset('emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = emotion['train'].select(range(3))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(풀이)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0105,  0.0449],\n",
       "        [ 0.0015,  0.0298],\n",
       "        [-0.0091,  0.0532]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1(\n",
    "    torch.tensor(tokenizer(d['text'],padding=True)['input_ids'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0105,  0.0449],\n",
       "        [ 0.0015,  0.0298],\n",
       "        [-0.0091,  0.0532]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1(\n",
    "    tokenizer(d['text'],padding=True,return_tensors=\"pt\")['input_ids']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# 예제3` -- MBTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['type', 'posts'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = datasets.Dataset.from_csv(\"mbti_1.csv\").select(range(3))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(풀이1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*실패*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2102) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[228], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mposts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:883\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    881\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 883\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    892\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[1;32m    893\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:695\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    693\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 695\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_flash_attention_2:\n\u001b[1;32m    698\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask \u001b[38;5;28;01mif\u001b[39;00m (attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m attention_mask) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:125\u001b[0m, in \u001b[0;36mEmbeddings.forward\u001b[0;34m(self, input_ids, input_embeds)\u001b[0m\n\u001b[1;32m    121\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand_as(input_ids)  \u001b[38;5;66;03m# (bs, max_seq_length)\u001b[39;00m\n\u001b[1;32m    123\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43minput_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m    126\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(embeddings)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m    127\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embeddings)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2102) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "model1(\n",
    "    tokenizer(d['posts'],padding=True,return_tensors=\"pt\")['input_ids']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*원인분석*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2102 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1 - Type: Tensor, Length: 3, Content: tensor([[ 101, 1005, 8299 ... // ...  ...,    0,    0,    0]])\n",
      "     Level 2 - Type: Tensor, Length: 2102, Content: tensor([ 101, 1005, 8299,  ...,    0,    0,    0])\n",
      "     Level 2 - Type: Tensor, Length: 2102, Content: tensor([ 101, 1005, 1045,  ..., 1012, 1005,  102])\n",
      "     Level 2 - Type: Tensor, Length: 2102, Content: tensor([ 101, 1005, 2204,  ...,    0,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "mp.show_list(tokenizer(d['posts'],padding=True,return_tensors=\"pt\")['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1 - Type: Tensor, Length: 3, Content: tensor([[ 101, 1005, 8299 ... // ...  ..., 1012, 2077,  102]])\n",
      "     Level 2 - Type: Tensor, Length: 512, Content: tensor([  101,  1005,  82 ... // ... 7,\n",
      "         2215,   102])\n",
      "     Level 2 - Type: Tensor, Length: 512, Content: tensor([  101,  1005,  10 ... // ... 9,\n",
      "         2028,   102])\n",
      "     Level 2 - Type: Tensor, Length: 512, Content: tensor([  101,  1005,  22 ... // ... 2,\n",
      "         2077,   102])\n"
     ]
    }
   ],
   "source": [
    "mp.show_list(tokenizer(d['posts'],truncation=True,return_tensors=\"pt\")['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*성공*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0469, -0.0175],\n",
       "        [-0.0534, -0.0048],\n",
       "        [-0.0759, -0.0220]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1(\n",
    "    tokenizer(d['posts'],truncation=True,return_tensors=\"pt\")['input_ids']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(풀이2)` *--모델설정변경 (퀴즈5, 모델의 프레임수를 4로 바꾸는 예제에서 사용한 테크닉)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*distilbert/distilbert-base-uncased 설정값 부르기*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"distilbert/distilbert-base-uncased\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.46.2\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = transformers.AutoConfig.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*설정값체크 -- `config.` + tab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e393d_row0_col0, #T_e393d_row1_col0, #T_e393d_row2_col0, #T_e393d_row3_col0, #T_e393d_row4_col0, #T_e393d_row5_col0, #T_e393d_row6_col0, #T_e393d_row7_col0, #T_e393d_row8_col0, #T_e393d_row9_col0, #T_e393d_row10_col0, #T_e393d_row11_col0, #T_e393d_row12_col0, #T_e393d_row13_col0, #T_e393d_row14_col0, #T_e393d_row15_col0, #T_e393d_row16_col0, #T_e393d_row17_col0, #T_e393d_row18_col0, #T_e393d_row19_col0, #T_e393d_row20_col0, #T_e393d_row21_col0, #T_e393d_row22_col0, #T_e393d_row23_col0, #T_e393d_row24_col0, #T_e393d_row25_col0, #T_e393d_row26_col0, #T_e393d_row27_col0, #T_e393d_row28_col0, #T_e393d_row29_col0, #T_e393d_row30_col0, #T_e393d_row31_col0, #T_e393d_row32_col0, #T_e393d_row33_col0, #T_e393d_row34_col0, #T_e393d_row35_col0, #T_e393d_row36_col0, #T_e393d_row37_col0, #T_e393d_row38_col0, #T_e393d_row39_col0, #T_e393d_row40_col0, #T_e393d_row41_col0, #T_e393d_row42_col0, #T_e393d_row43_col0, #T_e393d_row44_col0, #T_e393d_row45_col0, #T_e393d_row46_col0, #T_e393d_row47_col0, #T_e393d_row48_col0, #T_e393d_row49_col0, #T_e393d_row50_col0, #T_e393d_row51_col0, #T_e393d_row52_col0, #T_e393d_row53_col0, #T_e393d_row54_col0, #T_e393d_row55_col0, #T_e393d_row56_col0, #T_e393d_row57_col0, #T_e393d_row58_col0, #T_e393d_row59_col0, #T_e393d_row60_col0, #T_e393d_row61_col0, #T_e393d_row62_col0, #T_e393d_row63_col0, #T_e393d_row64_col0, #T_e393d_row65_col0, #T_e393d_row66_col0, #T_e393d_row67_col0, #T_e393d_row68_col0, #T_e393d_row69_col0, #T_e393d_row70_col0, #T_e393d_row71_col0, #T_e393d_row72_col0, #T_e393d_row73_col0, #T_e393d_row74_col0, #T_e393d_row75_col0, #T_e393d_row76_col0, #T_e393d_row77_col0, #T_e393d_row78_col0, #T_e393d_row79_col0, #T_e393d_row80_col0, #T_e393d_row81_col0, #T_e393d_row82_col0, #T_e393d_row83_col0, #T_e393d_row84_col0, #T_e393d_row85_col0, #T_e393d_row86_col0, #T_e393d_row87_col0, #T_e393d_row88_col0 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e393d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e393d_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Type</th>\n",
       "      <th class=\"index_name level1\" >Name</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"17\">NoneType</th>\n",
       "      <th id=\"T_e393d_level1_row0\" class=\"row_heading level1 row0\" >bad_words_ids</th>\n",
       "      <td id=\"T_e393d_row0_col0\" class=\"data row0 col0\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row1\" class=\"row_heading level1 row1\" >begin_suppress_tokens</th>\n",
       "      <td id=\"T_e393d_row1_col0\" class=\"data row1 col0\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row2\" class=\"row_heading level1 row2\" >bos_token_id</th>\n",
       "      <td id=\"T_e393d_row2_col0\" class=\"data row2 col0\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row3\" class=\"row_heading level1 row3\" >cross_attention_hidden_size</th>\n",
       "      <td id=\"T_e393d_row3_col0\" class=\"data row3 col0\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row4\" class=\"row_heading level1 row4\" >decoder_start_token_id</th>\n",
       "      <td id=\"T_e393d_row4_col0\" class=\"data row4 col0\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row5\" class=\"row_heading level1 row5\" >eos_token_id</th>\n",
       "      <td id=\"T_e393d_row5_col0\" class=\"data row5 col0\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row6\" class=\"row_heading level1 row6\" >exponential_decay_length_penalty</th>\n",
       "      <td id=\"T_e393d_row6_col0\" class=\"data row6 col0\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row7\" class=\"row_heading level1 row7\" >finetuning_task</th>\n",
       "      <td id=\"T_e393d_row7_col0\" class=\"data row7 col0\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row8\" class=\"row_heading level1 row8\" >forced_bos_token_id</th>\n",
       "      <td id=\"T_e393d_row8_col0\" class=\"data row8 col0\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row9\" class=\"row_heading level1 row9\" >forced_eos_token_id</th>\n",
       "      <td id=\"T_e393d_row9_col0\" class=\"data row9 col0\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row10\" class=\"row_heading level1 row10\" >prefix</th>\n",
       "      <td id=\"T_e393d_row10_col0\" class=\"data row10 col0\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row11\" class=\"row_heading level1 row11\" >problem_type</th>\n",
       "      <td id=\"T_e393d_row11_col0\" class=\"data row11 col0\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row12\" class=\"row_heading level1 row12\" >sep_token_id</th>\n",
       "      <td id=\"T_e393d_row12_col0\" class=\"data row12 col0\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row13\" class=\"row_heading level1 row13\" >suppress_tokens</th>\n",
       "      <td id=\"T_e393d_row13_col0\" class=\"data row13 col0\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row14\" class=\"row_heading level1 row14\" >task_specific_params</th>\n",
       "      <td id=\"T_e393d_row14_col0\" class=\"data row14 col0\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row15\" class=\"row_heading level1 row15\" >tokenizer_class</th>\n",
       "      <td id=\"T_e393d_row15_col0\" class=\"data row15 col0\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row16\" class=\"row_heading level1 row16\" >torch_dtype</th>\n",
       "      <td id=\"T_e393d_row16_col0\" class=\"data row16 col0\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level0_row17\" class=\"row_heading level0 row17\" rowspan=\"20\">bool</th>\n",
       "      <th id=\"T_e393d_level1_row17\" class=\"row_heading level1 row17\" >add_cross_attention</th>\n",
       "      <td id=\"T_e393d_row17_col0\" class=\"data row17 col0\" >bool(x) -> bool\n",
       "\n",
       "Returns True when the argument x is true, False otherwise.\n",
       "The builtins True and False are the only two instances of the class bool.\n",
       "The class bool is a subclass of the class int, and cannot be subclassed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row18\" class=\"row_heading level1 row18\" >do_sample</th>\n",
       "      <td id=\"T_e393d_row18_col0\" class=\"data row18 col0\" >bool(x) -> bool\n",
       "\n",
       "Returns True when the argument x is true, False otherwise.\n",
       "The builtins True and False are the only two instances of the class bool.\n",
       "The class bool is a subclass of the class int, and cannot be subclassed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row19\" class=\"row_heading level1 row19\" >early_stopping</th>\n",
       "      <td id=\"T_e393d_row19_col0\" class=\"data row19 col0\" >bool(x) -> bool\n",
       "\n",
       "Returns True when the argument x is true, False otherwise.\n",
       "The builtins True and False are the only two instances of the class bool.\n",
       "The class bool is a subclass of the class int, and cannot be subclassed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row20\" class=\"row_heading level1 row20\" >is_composition</th>\n",
       "      <td id=\"T_e393d_row20_col0\" class=\"data row20 col0\" >bool(x) -> bool\n",
       "\n",
       "Returns True when the argument x is true, False otherwise.\n",
       "The builtins True and False are the only two instances of the class bool.\n",
       "The class bool is a subclass of the class int, and cannot be subclassed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row21\" class=\"row_heading level1 row21\" >is_decoder</th>\n",
       "      <td id=\"T_e393d_row21_col0\" class=\"data row21 col0\" >bool(x) -> bool\n",
       "\n",
       "Returns True when the argument x is true, False otherwise.\n",
       "The builtins True and False are the only two instances of the class bool.\n",
       "The class bool is a subclass of the class int, and cannot be subclassed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row22\" class=\"row_heading level1 row22\" >is_encoder_decoder</th>\n",
       "      <td id=\"T_e393d_row22_col0\" class=\"data row22 col0\" >bool(x) -> bool\n",
       "\n",
       "Returns True when the argument x is true, False otherwise.\n",
       "The builtins True and False are the only two instances of the class bool.\n",
       "The class bool is a subclass of the class int, and cannot be subclassed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row23\" class=\"row_heading level1 row23\" >output_attentions</th>\n",
       "      <td id=\"T_e393d_row23_col0\" class=\"data row23 col0\" >bool(x) -> bool\n",
       "\n",
       "Returns True when the argument x is true, False otherwise.\n",
       "The builtins True and False are the only two instances of the class bool.\n",
       "The class bool is a subclass of the class int, and cannot be subclassed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row24\" class=\"row_heading level1 row24\" >output_hidden_states</th>\n",
       "      <td id=\"T_e393d_row24_col0\" class=\"data row24 col0\" >bool(x) -> bool\n",
       "\n",
       "Returns True when the argument x is true, False otherwise.\n",
       "The builtins True and False are the only two instances of the class bool.\n",
       "The class bool is a subclass of the class int, and cannot be subclassed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row25\" class=\"row_heading level1 row25\" >output_scores</th>\n",
       "      <td id=\"T_e393d_row25_col0\" class=\"data row25 col0\" >bool(x) -> bool\n",
       "\n",
       "Returns True when the argument x is true, False otherwise.\n",
       "The builtins True and False are the only two instances of the class bool.\n",
       "The class bool is a subclass of the class int, and cannot be subclassed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row26\" class=\"row_heading level1 row26\" >remove_invalid_values</th>\n",
       "      <td id=\"T_e393d_row26_col0\" class=\"data row26 col0\" >bool(x) -> bool\n",
       "\n",
       "Returns True when the argument x is true, False otherwise.\n",
       "The builtins True and False are the only two instances of the class bool.\n",
       "The class bool is a subclass of the class int, and cannot be subclassed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row27\" class=\"row_heading level1 row27\" >return_dict</th>\n",
       "      <td id=\"T_e393d_row27_col0\" class=\"data row27 col0\" >bool(x) -> bool\n",
       "\n",
       "Returns True when the argument x is true, False otherwise.\n",
       "The builtins True and False are the only two instances of the class bool.\n",
       "The class bool is a subclass of the class int, and cannot be subclassed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row28\" class=\"row_heading level1 row28\" >return_dict_in_generate</th>\n",
       "      <td id=\"T_e393d_row28_col0\" class=\"data row28 col0\" >bool(x) -> bool\n",
       "\n",
       "Returns True when the argument x is true, False otherwise.\n",
       "The builtins True and False are the only two instances of the class bool.\n",
       "The class bool is a subclass of the class int, and cannot be subclassed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row29\" class=\"row_heading level1 row29\" >sinusoidal_pos_embds</th>\n",
       "      <td id=\"T_e393d_row29_col0\" class=\"data row29 col0\" >bool(x) -> bool\n",
       "\n",
       "Returns True when the argument x is true, False otherwise.\n",
       "The builtins True and False are the only two instances of the class bool.\n",
       "The class bool is a subclass of the class int, and cannot be subclassed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row30\" class=\"row_heading level1 row30\" >tf_legacy_loss</th>\n",
       "      <td id=\"T_e393d_row30_col0\" class=\"data row30 col0\" >bool(x) -> bool\n",
       "\n",
       "Returns True when the argument x is true, False otherwise.\n",
       "The builtins True and False are the only two instances of the class bool.\n",
       "The class bool is a subclass of the class int, and cannot be subclassed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row31\" class=\"row_heading level1 row31\" >tie_encoder_decoder</th>\n",
       "      <td id=\"T_e393d_row31_col0\" class=\"data row31 col0\" >bool(x) -> bool\n",
       "\n",
       "Returns True when the argument x is true, False otherwise.\n",
       "The builtins True and False are the only two instances of the class bool.\n",
       "The class bool is a subclass of the class int, and cannot be subclassed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row32\" class=\"row_heading level1 row32\" >tie_weights_</th>\n",
       "      <td id=\"T_e393d_row32_col0\" class=\"data row32 col0\" >bool(x) -> bool\n",
       "\n",
       "Returns True when the argument x is true, False otherwise.\n",
       "The builtins True and False are the only two instances of the class bool.\n",
       "The class bool is a subclass of the class int, and cannot be subclassed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row33\" class=\"row_heading level1 row33\" >tie_word_embeddings</th>\n",
       "      <td id=\"T_e393d_row33_col0\" class=\"data row33 col0\" >bool(x) -> bool\n",
       "\n",
       "Returns True when the argument x is true, False otherwise.\n",
       "The builtins True and False are the only two instances of the class bool.\n",
       "The class bool is a subclass of the class int, and cannot be subclassed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row34\" class=\"row_heading level1 row34\" >torchscript</th>\n",
       "      <td id=\"T_e393d_row34_col0\" class=\"data row34 col0\" >bool(x) -> bool\n",
       "\n",
       "Returns True when the argument x is true, False otherwise.\n",
       "The builtins True and False are the only two instances of the class bool.\n",
       "The class bool is a subclass of the class int, and cannot be subclassed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row35\" class=\"row_heading level1 row35\" >use_bfloat16</th>\n",
       "      <td id=\"T_e393d_row35_col0\" class=\"data row35 col0\" >bool(x) -> bool\n",
       "\n",
       "Returns True when the argument x is true, False otherwise.\n",
       "The builtins True and False are the only two instances of the class bool.\n",
       "The class bool is a subclass of the class int, and cannot be subclassed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row36\" class=\"row_heading level1 row36\" >use_return_dict</th>\n",
       "      <td id=\"T_e393d_row36_col0\" class=\"data row36 col0\" >bool(x) -> bool\n",
       "\n",
       "Returns True when the argument x is true, False otherwise.\n",
       "The builtins True and False are the only two instances of the class bool.\n",
       "The class bool is a subclass of the class int, and cannot be subclassed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level0_row37\" class=\"row_heading level0 row37\" rowspan=\"4\">dict</th>\n",
       "      <th id=\"T_e393d_level1_row37\" class=\"row_heading level1 row37\" >attribute_map</th>\n",
       "      <td id=\"T_e393d_row37_col0\" class=\"data row37 col0\" >dict() -> new empty dictionary\n",
       "dict(mapping) -> new dictionary initialized from a mapping object's\n",
       "    (key, value) pairs\n",
       "dict(iterable) -> new dictionary initialized as if via:\n",
       "    d = {}\n",
       "    for k, v in iterable:\n",
       "        d[k] = v\n",
       "dict(**kwargs) -> new dictionary initialized with the name=value pairs\n",
       "    in the keyword argument list.  For example:  dict(one=1, two=2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row38\" class=\"row_heading level1 row38\" >id2label</th>\n",
       "      <td id=\"T_e393d_row38_col0\" class=\"data row38 col0\" >dict() -> new empty dictionary\n",
       "dict(mapping) -> new dictionary initialized from a mapping object's\n",
       "    (key, value) pairs\n",
       "dict(iterable) -> new dictionary initialized as if via:\n",
       "    d = {}\n",
       "    for k, v in iterable:\n",
       "        d[k] = v\n",
       "dict(**kwargs) -> new dictionary initialized with the name=value pairs\n",
       "    in the keyword argument list.  For example:  dict(one=1, two=2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row39\" class=\"row_heading level1 row39\" >label2id</th>\n",
       "      <td id=\"T_e393d_row39_col0\" class=\"data row39 col0\" >dict() -> new empty dictionary\n",
       "dict(mapping) -> new dictionary initialized from a mapping object's\n",
       "    (key, value) pairs\n",
       "dict(iterable) -> new dictionary initialized as if via:\n",
       "    d = {}\n",
       "    for k, v in iterable:\n",
       "        d[k] = v\n",
       "dict(**kwargs) -> new dictionary initialized with the name=value pairs\n",
       "    in the keyword argument list.  For example:  dict(one=1, two=2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row40\" class=\"row_heading level1 row40\" >pruned_heads</th>\n",
       "      <td id=\"T_e393d_row40_col0\" class=\"data row40 col0\" >dict() -> new empty dictionary\n",
       "dict(mapping) -> new dictionary initialized from a mapping object's\n",
       "    (key, value) pairs\n",
       "dict(iterable) -> new dictionary initialized as if via:\n",
       "    d = {}\n",
       "    for k, v in iterable:\n",
       "        d[k] = v\n",
       "dict(**kwargs) -> new dictionary initialized with the name=value pairs\n",
       "    in the keyword argument list.  For example:  dict(one=1, two=2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level0_row41\" class=\"row_heading level0 row41\" rowspan=\"11\">float</th>\n",
       "      <th id=\"T_e393d_level1_row41\" class=\"row_heading level1 row41\" >attention_dropout</th>\n",
       "      <td id=\"T_e393d_row41_col0\" class=\"data row41 col0\" >Convert a string or number to a floating point number, if possible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row42\" class=\"row_heading level1 row42\" >diversity_penalty</th>\n",
       "      <td id=\"T_e393d_row42_col0\" class=\"data row42 col0\" >Convert a string or number to a floating point number, if possible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row43\" class=\"row_heading level1 row43\" >dropout</th>\n",
       "      <td id=\"T_e393d_row43_col0\" class=\"data row43 col0\" >Convert a string or number to a floating point number, if possible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row44\" class=\"row_heading level1 row44\" >initializer_range</th>\n",
       "      <td id=\"T_e393d_row44_col0\" class=\"data row44 col0\" >Convert a string or number to a floating point number, if possible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row45\" class=\"row_heading level1 row45\" >length_penalty</th>\n",
       "      <td id=\"T_e393d_row45_col0\" class=\"data row45 col0\" >Convert a string or number to a floating point number, if possible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row46\" class=\"row_heading level1 row46\" >qa_dropout</th>\n",
       "      <td id=\"T_e393d_row46_col0\" class=\"data row46 col0\" >Convert a string or number to a floating point number, if possible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row47\" class=\"row_heading level1 row47\" >repetition_penalty</th>\n",
       "      <td id=\"T_e393d_row47_col0\" class=\"data row47 col0\" >Convert a string or number to a floating point number, if possible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row48\" class=\"row_heading level1 row48\" >seq_classif_dropout</th>\n",
       "      <td id=\"T_e393d_row48_col0\" class=\"data row48 col0\" >Convert a string or number to a floating point number, if possible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row49\" class=\"row_heading level1 row49\" >temperature</th>\n",
       "      <td id=\"T_e393d_row49_col0\" class=\"data row49 col0\" >Convert a string or number to a floating point number, if possible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row50\" class=\"row_heading level1 row50\" >top_p</th>\n",
       "      <td id=\"T_e393d_row50_col0\" class=\"data row50 col0\" >Convert a string or number to a floating point number, if possible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row51\" class=\"row_heading level1 row51\" >typical_p</th>\n",
       "      <td id=\"T_e393d_row51_col0\" class=\"data row51 col0\" >Convert a string or number to a floating point number, if possible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level0_row52\" class=\"row_heading level0 row52\" rowspan=\"17\">int</th>\n",
       "      <th id=\"T_e393d_level1_row52\" class=\"row_heading level1 row52\" >chunk_size_feed_forward</th>\n",
       "      <td id=\"T_e393d_row52_col0\" class=\"data row52 col0\" >int([x]) -> integer\n",
       "int(x, base=10) -> integer\n",
       "\n",
       "Convert a number or string to an integer, or return 0 if no arguments\n",
       "are given.  If x is a number, return x.__int__().  For floating point\n",
       "numbers, this truncates towards zero.\n",
       "\n",
       "If x is not a number or if base is given, then x must be a string,\n",
       "bytes, or bytearray instance representing an integer literal in the\n",
       "given base.  The literal can be preceded by '+' or '-' and be surrounded\n",
       "by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\n",
       "Base 0 means to interpret the base from the string as an integer literal.\n",
       ">>> int('0b100', base=0)\n",
       "4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row53\" class=\"row_heading level1 row53\" >dim</th>\n",
       "      <td id=\"T_e393d_row53_col0\" class=\"data row53 col0\" >int([x]) -> integer\n",
       "int(x, base=10) -> integer\n",
       "\n",
       "Convert a number or string to an integer, or return 0 if no arguments\n",
       "are given.  If x is a number, return x.__int__().  For floating point\n",
       "numbers, this truncates towards zero.\n",
       "\n",
       "If x is not a number or if base is given, then x must be a string,\n",
       "bytes, or bytearray instance representing an integer literal in the\n",
       "given base.  The literal can be preceded by '+' or '-' and be surrounded\n",
       "by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\n",
       "Base 0 means to interpret the base from the string as an integer literal.\n",
       ">>> int('0b100', base=0)\n",
       "4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row54\" class=\"row_heading level1 row54\" >encoder_no_repeat_ngram_size</th>\n",
       "      <td id=\"T_e393d_row54_col0\" class=\"data row54 col0\" >int([x]) -> integer\n",
       "int(x, base=10) -> integer\n",
       "\n",
       "Convert a number or string to an integer, or return 0 if no arguments\n",
       "are given.  If x is a number, return x.__int__().  For floating point\n",
       "numbers, this truncates towards zero.\n",
       "\n",
       "If x is not a number or if base is given, then x must be a string,\n",
       "bytes, or bytearray instance representing an integer literal in the\n",
       "given base.  The literal can be preceded by '+' or '-' and be surrounded\n",
       "by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\n",
       "Base 0 means to interpret the base from the string as an integer literal.\n",
       ">>> int('0b100', base=0)\n",
       "4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row55\" class=\"row_heading level1 row55\" >hidden_dim</th>\n",
       "      <td id=\"T_e393d_row55_col0\" class=\"data row55 col0\" >int([x]) -> integer\n",
       "int(x, base=10) -> integer\n",
       "\n",
       "Convert a number or string to an integer, or return 0 if no arguments\n",
       "are given.  If x is a number, return x.__int__().  For floating point\n",
       "numbers, this truncates towards zero.\n",
       "\n",
       "If x is not a number or if base is given, then x must be a string,\n",
       "bytes, or bytearray instance representing an integer literal in the\n",
       "given base.  The literal can be preceded by '+' or '-' and be surrounded\n",
       "by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\n",
       "Base 0 means to interpret the base from the string as an integer literal.\n",
       ">>> int('0b100', base=0)\n",
       "4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row56\" class=\"row_heading level1 row56\" >max_length</th>\n",
       "      <td id=\"T_e393d_row56_col0\" class=\"data row56 col0\" >int([x]) -> integer\n",
       "int(x, base=10) -> integer\n",
       "\n",
       "Convert a number or string to an integer, or return 0 if no arguments\n",
       "are given.  If x is a number, return x.__int__().  For floating point\n",
       "numbers, this truncates towards zero.\n",
       "\n",
       "If x is not a number or if base is given, then x must be a string,\n",
       "bytes, or bytearray instance representing an integer literal in the\n",
       "given base.  The literal can be preceded by '+' or '-' and be surrounded\n",
       "by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\n",
       "Base 0 means to interpret the base from the string as an integer literal.\n",
       ">>> int('0b100', base=0)\n",
       "4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row57\" class=\"row_heading level1 row57\" >max_position_embeddings</th>\n",
       "      <td id=\"T_e393d_row57_col0\" class=\"data row57 col0\" >int([x]) -> integer\n",
       "int(x, base=10) -> integer\n",
       "\n",
       "Convert a number or string to an integer, or return 0 if no arguments\n",
       "are given.  If x is a number, return x.__int__().  For floating point\n",
       "numbers, this truncates towards zero.\n",
       "\n",
       "If x is not a number or if base is given, then x must be a string,\n",
       "bytes, or bytearray instance representing an integer literal in the\n",
       "given base.  The literal can be preceded by '+' or '-' and be surrounded\n",
       "by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\n",
       "Base 0 means to interpret the base from the string as an integer literal.\n",
       ">>> int('0b100', base=0)\n",
       "4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row58\" class=\"row_heading level1 row58\" >min_length</th>\n",
       "      <td id=\"T_e393d_row58_col0\" class=\"data row58 col0\" >int([x]) -> integer\n",
       "int(x, base=10) -> integer\n",
       "\n",
       "Convert a number or string to an integer, or return 0 if no arguments\n",
       "are given.  If x is a number, return x.__int__().  For floating point\n",
       "numbers, this truncates towards zero.\n",
       "\n",
       "If x is not a number or if base is given, then x must be a string,\n",
       "bytes, or bytearray instance representing an integer literal in the\n",
       "given base.  The literal can be preceded by '+' or '-' and be surrounded\n",
       "by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\n",
       "Base 0 means to interpret the base from the string as an integer literal.\n",
       ">>> int('0b100', base=0)\n",
       "4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row59\" class=\"row_heading level1 row59\" >n_heads</th>\n",
       "      <td id=\"T_e393d_row59_col0\" class=\"data row59 col0\" >int([x]) -> integer\n",
       "int(x, base=10) -> integer\n",
       "\n",
       "Convert a number or string to an integer, or return 0 if no arguments\n",
       "are given.  If x is a number, return x.__int__().  For floating point\n",
       "numbers, this truncates towards zero.\n",
       "\n",
       "If x is not a number or if base is given, then x must be a string,\n",
       "bytes, or bytearray instance representing an integer literal in the\n",
       "given base.  The literal can be preceded by '+' or '-' and be surrounded\n",
       "by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\n",
       "Base 0 means to interpret the base from the string as an integer literal.\n",
       ">>> int('0b100', base=0)\n",
       "4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row60\" class=\"row_heading level1 row60\" >n_layers</th>\n",
       "      <td id=\"T_e393d_row60_col0\" class=\"data row60 col0\" >int([x]) -> integer\n",
       "int(x, base=10) -> integer\n",
       "\n",
       "Convert a number or string to an integer, or return 0 if no arguments\n",
       "are given.  If x is a number, return x.__int__().  For floating point\n",
       "numbers, this truncates towards zero.\n",
       "\n",
       "If x is not a number or if base is given, then x must be a string,\n",
       "bytes, or bytearray instance representing an integer literal in the\n",
       "given base.  The literal can be preceded by '+' or '-' and be surrounded\n",
       "by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\n",
       "Base 0 means to interpret the base from the string as an integer literal.\n",
       ">>> int('0b100', base=0)\n",
       "4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row61\" class=\"row_heading level1 row61\" >no_repeat_ngram_size</th>\n",
       "      <td id=\"T_e393d_row61_col0\" class=\"data row61 col0\" >int([x]) -> integer\n",
       "int(x, base=10) -> integer\n",
       "\n",
       "Convert a number or string to an integer, or return 0 if no arguments\n",
       "are given.  If x is a number, return x.__int__().  For floating point\n",
       "numbers, this truncates towards zero.\n",
       "\n",
       "If x is not a number or if base is given, then x must be a string,\n",
       "bytes, or bytearray instance representing an integer literal in the\n",
       "given base.  The literal can be preceded by '+' or '-' and be surrounded\n",
       "by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\n",
       "Base 0 means to interpret the base from the string as an integer literal.\n",
       ">>> int('0b100', base=0)\n",
       "4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row62\" class=\"row_heading level1 row62\" >num_beam_groups</th>\n",
       "      <td id=\"T_e393d_row62_col0\" class=\"data row62 col0\" >int([x]) -> integer\n",
       "int(x, base=10) -> integer\n",
       "\n",
       "Convert a number or string to an integer, or return 0 if no arguments\n",
       "are given.  If x is a number, return x.__int__().  For floating point\n",
       "numbers, this truncates towards zero.\n",
       "\n",
       "If x is not a number or if base is given, then x must be a string,\n",
       "bytes, or bytearray instance representing an integer literal in the\n",
       "given base.  The literal can be preceded by '+' or '-' and be surrounded\n",
       "by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\n",
       "Base 0 means to interpret the base from the string as an integer literal.\n",
       ">>> int('0b100', base=0)\n",
       "4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row63\" class=\"row_heading level1 row63\" >num_beams</th>\n",
       "      <td id=\"T_e393d_row63_col0\" class=\"data row63 col0\" >int([x]) -> integer\n",
       "int(x, base=10) -> integer\n",
       "\n",
       "Convert a number or string to an integer, or return 0 if no arguments\n",
       "are given.  If x is a number, return x.__int__().  For floating point\n",
       "numbers, this truncates towards zero.\n",
       "\n",
       "If x is not a number or if base is given, then x must be a string,\n",
       "bytes, or bytearray instance representing an integer literal in the\n",
       "given base.  The literal can be preceded by '+' or '-' and be surrounded\n",
       "by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\n",
       "Base 0 means to interpret the base from the string as an integer literal.\n",
       ">>> int('0b100', base=0)\n",
       "4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row64\" class=\"row_heading level1 row64\" >num_labels</th>\n",
       "      <td id=\"T_e393d_row64_col0\" class=\"data row64 col0\" >int([x]) -> integer\n",
       "int(x, base=10) -> integer\n",
       "\n",
       "Convert a number or string to an integer, or return 0 if no arguments\n",
       "are given.  If x is a number, return x.__int__().  For floating point\n",
       "numbers, this truncates towards zero.\n",
       "\n",
       "If x is not a number or if base is given, then x must be a string,\n",
       "bytes, or bytearray instance representing an integer literal in the\n",
       "given base.  The literal can be preceded by '+' or '-' and be surrounded\n",
       "by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\n",
       "Base 0 means to interpret the base from the string as an integer literal.\n",
       ">>> int('0b100', base=0)\n",
       "4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row65\" class=\"row_heading level1 row65\" >num_return_sequences</th>\n",
       "      <td id=\"T_e393d_row65_col0\" class=\"data row65 col0\" >int([x]) -> integer\n",
       "int(x, base=10) -> integer\n",
       "\n",
       "Convert a number or string to an integer, or return 0 if no arguments\n",
       "are given.  If x is a number, return x.__int__().  For floating point\n",
       "numbers, this truncates towards zero.\n",
       "\n",
       "If x is not a number or if base is given, then x must be a string,\n",
       "bytes, or bytearray instance representing an integer literal in the\n",
       "given base.  The literal can be preceded by '+' or '-' and be surrounded\n",
       "by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\n",
       "Base 0 means to interpret the base from the string as an integer literal.\n",
       ">>> int('0b100', base=0)\n",
       "4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row66\" class=\"row_heading level1 row66\" >pad_token_id</th>\n",
       "      <td id=\"T_e393d_row66_col0\" class=\"data row66 col0\" >int([x]) -> integer\n",
       "int(x, base=10) -> integer\n",
       "\n",
       "Convert a number or string to an integer, or return 0 if no arguments\n",
       "are given.  If x is a number, return x.__int__().  For floating point\n",
       "numbers, this truncates towards zero.\n",
       "\n",
       "If x is not a number or if base is given, then x must be a string,\n",
       "bytes, or bytearray instance representing an integer literal in the\n",
       "given base.  The literal can be preceded by '+' or '-' and be surrounded\n",
       "by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\n",
       "Base 0 means to interpret the base from the string as an integer literal.\n",
       ">>> int('0b100', base=0)\n",
       "4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row67\" class=\"row_heading level1 row67\" >top_k</th>\n",
       "      <td id=\"T_e393d_row67_col0\" class=\"data row67 col0\" >int([x]) -> integer\n",
       "int(x, base=10) -> integer\n",
       "\n",
       "Convert a number or string to an integer, or return 0 if no arguments\n",
       "are given.  If x is a number, return x.__int__().  For floating point\n",
       "numbers, this truncates towards zero.\n",
       "\n",
       "If x is not a number or if base is given, then x must be a string,\n",
       "bytes, or bytearray instance representing an integer literal in the\n",
       "given base.  The literal can be preceded by '+' or '-' and be surrounded\n",
       "by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\n",
       "Base 0 means to interpret the base from the string as an integer literal.\n",
       ">>> int('0b100', base=0)\n",
       "4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row68\" class=\"row_heading level1 row68\" >vocab_size</th>\n",
       "      <td id=\"T_e393d_row68_col0\" class=\"data row68 col0\" >int([x]) -> integer\n",
       "int(x, base=10) -> integer\n",
       "\n",
       "Convert a number or string to an integer, or return 0 if no arguments\n",
       "are given.  If x is a number, return x.__int__().  For floating point\n",
       "numbers, this truncates towards zero.\n",
       "\n",
       "If x is not a number or if base is given, then x must be a string,\n",
       "bytes, or bytearray instance representing an integer literal in the\n",
       "given base.  The literal can be preceded by '+' or '-' and be surrounded\n",
       "by whitespace.  The base defaults to 10.  Valid bases are 0 and 2-36.\n",
       "Base 0 means to interpret the base from the string as an integer literal.\n",
       ">>> int('0b100', base=0)\n",
       "4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level0_row69\" class=\"row_heading level0 row69\" >list</th>\n",
       "      <th id=\"T_e393d_level1_row69\" class=\"row_heading level1 row69\" >architectures</th>\n",
       "      <td id=\"T_e393d_row69_col0\" class=\"data row69 col0\" >Built-in mutable sequence.\n",
       "\n",
       "If no argument is given, the constructor creates a new empty list.\n",
       "The argument must be an iterable if specified.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level0_row70\" class=\"row_heading level0 row70\" rowspan=\"15\">method</th>\n",
       "      <th id=\"T_e393d_level1_row70\" class=\"row_heading level1 row70\" >dict_torch_dtype_to_str</th>\n",
       "      <td id=\"T_e393d_row70_col0\" class=\"data row70 col0\" >Checks whether the passed dictionary and its nested dicts have a *torch_dtype* key and if it's not None,\n",
       "converts torch.dtype to a string of just the type. For example, `torch.float32` get converted into *\"float32\"*\n",
       "string, which can then be stored in the json format.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row71\" class=\"row_heading level1 row71\" >from_dict</th>\n",
       "      <td id=\"T_e393d_row71_col0\" class=\"data row71 col0\" >Instantiates a [`PretrainedConfig`] from a Python dictionary of parameters.\n",
       "\n",
       "Args:\n",
       "    config_dict (`Dict[str, Any]`):\n",
       "        Dictionary that will be used to instantiate the configuration object. Such a dictionary can be\n",
       "        retrieved from a pretrained checkpoint by leveraging the [`~PretrainedConfig.get_config_dict`] method.\n",
       "    kwargs (`Dict[str, Any]`):\n",
       "        Additional parameters from which to initialize the configuration object.\n",
       "\n",
       "Returns:\n",
       "    [`PretrainedConfig`]: The configuration object instantiated from those parameters.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row72\" class=\"row_heading level1 row72\" >from_json_file</th>\n",
       "      <td id=\"T_e393d_row72_col0\" class=\"data row72 col0\" >Instantiates a [`PretrainedConfig`] from the path to a JSON file of parameters.\n",
       "\n",
       "Args:\n",
       "    json_file (`str` or `os.PathLike`):\n",
       "        Path to the JSON file containing the parameters.\n",
       "\n",
       "Returns:\n",
       "    [`PretrainedConfig`]: The configuration object instantiated from that JSON file.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row73\" class=\"row_heading level1 row73\" >from_pretrained</th>\n",
       "      <td id=\"T_e393d_row73_col0\" class=\"data row73 col0\" >Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\n",
       "\n",
       "Args:\n",
       "    pretrained_model_name_or_path (`str` or `os.PathLike`):\n",
       "        This can be either:\n",
       "\n",
       "        - a string, the *model id* of a pretrained model configuration hosted inside a model repo on\n",
       "          huggingface.co.\n",
       "        - a path to a *directory* containing a configuration file saved using the\n",
       "          [`~PretrainedConfig.save_pretrained`] method, e.g., `./my_model_directory/`.\n",
       "        - a path or url to a saved configuration JSON *file*, e.g., `./my_model_directory/configuration.json`.\n",
       "    cache_dir (`str` or `os.PathLike`, *optional*):\n",
       "        Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
       "        standard cache should not be used.\n",
       "    force_download (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to force to (re-)download the configuration files and override the cached versions if\n",
       "        they exist.\n",
       "    resume_download:\n",
       "        Deprecated and ignored. All downloads are now resumed by default when possible.\n",
       "        Will be removed in v5 of Transformers.\n",
       "    proxies (`Dict[str, str]`, *optional*):\n",
       "        A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
       "        'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n",
       "    token (`str` or `bool`, *optional*):\n",
       "        The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use\n",
       "        the token generated when running `huggingface-cli login` (stored in `~/.huggingface`).\n",
       "    revision (`str`, *optional*, defaults to `\"main\"`):\n",
       "        The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
       "        git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
       "        identifier allowed by git.\n",
       "\n",
       "        <Tip>\n",
       "\n",
       "        To test a pull request you made on the Hub, you can pass `revision=\"refs/pr/<pr_number>\"`.\n",
       "\n",
       "        </Tip>\n",
       "\n",
       "    return_unused_kwargs (`bool`, *optional*, defaults to `False`):\n",
       "        If `False`, then this function returns just the final configuration object.\n",
       "\n",
       "        If `True`, then this functions returns a `Tuple(config, unused_kwargs)` where *unused_kwargs* is a\n",
       "        dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the\n",
       "        part of `kwargs` which has not been used to update `config` and is otherwise ignored.\n",
       "    subfolder (`str`, *optional*, defaults to `\"\"`):\n",
       "        In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\n",
       "        specify the folder name here.\n",
       "    kwargs (`Dict[str, Any]`, *optional*):\n",
       "        The values in kwargs of any keys which are configuration attributes will be used to override the loaded\n",
       "        values. Behavior concerning key/value pairs whose keys are *not* configuration attributes is controlled\n",
       "        by the `return_unused_kwargs` keyword parameter.\n",
       "\n",
       "Returns:\n",
       "    [`PretrainedConfig`]: The configuration object instantiated from this pretrained model.\n",
       "\n",
       "Examples:\n",
       "\n",
       "```python\n",
       "# We can't instantiate directly the base class *PretrainedConfig* so let's show the examples on a\n",
       "# derived class: BertConfig\n",
       "config = BertConfig.from_pretrained(\n",
       "    \"google-bert/bert-base-uncased\"\n",
       ")  # Download configuration from huggingface.co and cache.\n",
       "config = BertConfig.from_pretrained(\n",
       "    \"./test/saved_model/\"\n",
       ")  # E.g. config (or model) was saved using *save_pretrained('./test/saved_model/')*\n",
       "config = BertConfig.from_pretrained(\"./test/saved_model/my_configuration.json\")\n",
       "config = BertConfig.from_pretrained(\"google-bert/bert-base-uncased\", output_attentions=True, foo=False)\n",
       "assert config.output_attentions == True\n",
       "config, unused_kwargs = BertConfig.from_pretrained(\n",
       "    \"google-bert/bert-base-uncased\", output_attentions=True, foo=False, return_unused_kwargs=True\n",
       ")\n",
       "assert config.output_attentions == True\n",
       "assert unused_kwargs == {\"foo\": False}\n",
       "```</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row74\" class=\"row_heading level1 row74\" >get_config_dict</th>\n",
       "      <td id=\"T_e393d_row74_col0\" class=\"data row74 col0\" >From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to be used for instantiating a\n",
       "[`PretrainedConfig`] using `from_dict`.\n",
       "\n",
       "Parameters:\n",
       "    pretrained_model_name_or_path (`str` or `os.PathLike`):\n",
       "        The identifier of the pre-trained checkpoint from which we want the dictionary of parameters.\n",
       "\n",
       "Returns:\n",
       "    `Tuple[Dict, Dict]`: The dictionary(ies) that will be used to instantiate the configuration object.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row75\" class=\"row_heading level1 row75\" >get_text_config</th>\n",
       "      <td id=\"T_e393d_row75_col0\" class=\"data row75 col0\" >Returns the config that is meant to be used with text IO. On most models, it is the original config instance\n",
       "itself. On specific composite models, it is under a set of valid names.\n",
       "\n",
       "If `decoder` is set to `True`, then only search for decoder config names.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row76\" class=\"row_heading level1 row76\" >push_to_hub</th>\n",
       "      <td id=\"T_e393d_row76_col0\" class=\"data row76 col0\" >Upload the configuration file to the 🤗 Model Hub.\n",
       "\n",
       "Parameters:\n",
       "    repo_id (`str`):\n",
       "        The name of the repository you want to push your config to. It should contain your organization name\n",
       "        when pushing to a given organization.\n",
       "    use_temp_dir (`bool`, *optional*):\n",
       "        Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n",
       "        Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n",
       "    commit_message (`str`, *optional*):\n",
       "        Message to commit while pushing. Will default to `\"Upload config\"`.\n",
       "    private (`bool`, *optional*):\n",
       "        Whether or not the repository created should be private.\n",
       "    token (`bool` or `str`, *optional*):\n",
       "        The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
       "        when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n",
       "        is not specified.\n",
       "    max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`):\n",
       "        Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n",
       "        will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n",
       "        by a unit (like `\"5MB\"`). We default it to `\"5GB\"` so that users can easily load models on free-tier\n",
       "        Google Colab instances without any CPU OOM issues.\n",
       "    create_pr (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to create a PR with the uploaded files or directly commit.\n",
       "    safe_serialization (`bool`, *optional*, defaults to `True`):\n",
       "        Whether or not to convert the model weights in safetensors format for safer serialization.\n",
       "    revision (`str`, *optional*):\n",
       "        Branch to push the uploaded files to.\n",
       "    commit_description (`str`, *optional*):\n",
       "        The description of the commit that will be created\n",
       "    tags (`List[str]`, *optional*):\n",
       "        List of tags to push on the Hub.\n",
       "\n",
       "Examples:\n",
       "\n",
       "```python\n",
       "from transformers import AutoConfig\n",
       "\n",
       "config = AutoConfig.from_pretrained(\"google-bert/bert-base-cased\")\n",
       "\n",
       "# Push the config to your namespace with the name \"my-finetuned-bert\".\n",
       "config.push_to_hub(\"my-finetuned-bert\")\n",
       "\n",
       "# Push the config to an organization with the name \"my-finetuned-bert\".\n",
       "config.push_to_hub(\"huggingface/my-finetuned-bert\")\n",
       "```</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row77\" class=\"row_heading level1 row77\" >register_for_auto_class</th>\n",
       "      <td id=\"T_e393d_row77_col0\" class=\"data row77 col0\" >Register this class with a given auto class. This should only be used for custom configurations as the ones in\n",
       "the library are already mapped with `AutoConfig`.\n",
       "\n",
       "<Tip warning={true}>\n",
       "\n",
       "This API is experimental and may have some slight breaking changes in the next releases.\n",
       "\n",
       "</Tip>\n",
       "\n",
       "Args:\n",
       "    auto_class (`str` or `type`, *optional*, defaults to `\"AutoConfig\"`):\n",
       "        The auto class to register this new configuration with.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row78\" class=\"row_heading level1 row78\" >save_pretrained</th>\n",
       "      <td id=\"T_e393d_row78_col0\" class=\"data row78 col0\" >Save a configuration object to the directory `save_directory`, so that it can be re-loaded using the\n",
       "[`~PretrainedConfig.from_pretrained`] class method.\n",
       "\n",
       "Args:\n",
       "    save_directory (`str` or `os.PathLike`):\n",
       "        Directory where the configuration JSON file will be saved (will be created if it does not exist).\n",
       "    push_to_hub (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n",
       "        repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n",
       "        namespace).\n",
       "    kwargs (`Dict[str, Any]`, *optional*):\n",
       "        Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row79\" class=\"row_heading level1 row79\" >to_dict</th>\n",
       "      <td id=\"T_e393d_row79_col0\" class=\"data row79 col0\" >Serializes this instance to a Python dictionary.\n",
       "\n",
       "Returns:\n",
       "    `Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row80\" class=\"row_heading level1 row80\" >to_diff_dict</th>\n",
       "      <td id=\"T_e393d_row80_col0\" class=\"data row80 col0\" >Removes all attributes from config which correspond to the default config attributes for better readability and\n",
       "serializes to a Python dictionary.\n",
       "\n",
       "Returns:\n",
       "    `Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row81\" class=\"row_heading level1 row81\" >to_json_file</th>\n",
       "      <td id=\"T_e393d_row81_col0\" class=\"data row81 col0\" >Save this instance to a JSON file.\n",
       "\n",
       "Args:\n",
       "    json_file_path (`str` or `os.PathLike`):\n",
       "        Path to the JSON file in which this configuration instance's parameters will be saved.\n",
       "    use_diff (`bool`, *optional*, defaults to `True`):\n",
       "        If set to `True`, only the difference between the config instance and the default `PretrainedConfig()`\n",
       "        is serialized to JSON file.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row82\" class=\"row_heading level1 row82\" >to_json_string</th>\n",
       "      <td id=\"T_e393d_row82_col0\" class=\"data row82 col0\" >Serializes this instance to a JSON string.\n",
       "\n",
       "Args:\n",
       "    use_diff (`bool`, *optional*, defaults to `True`):\n",
       "        If set to `True`, only the difference between the config instance and the default `PretrainedConfig()`\n",
       "        is serialized to JSON string.\n",
       "\n",
       "Returns:\n",
       "    `str`: String containing all the attributes that make up this configuration instance in JSON format.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row83\" class=\"row_heading level1 row83\" >update</th>\n",
       "      <td id=\"T_e393d_row83_col0\" class=\"data row83 col0\" >Updates attributes of this class with attributes from `config_dict`.\n",
       "\n",
       "Args:\n",
       "    config_dict (`Dict[str, Any]`): Dictionary of attributes that should be updated for this class.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row84\" class=\"row_heading level1 row84\" >update_from_string</th>\n",
       "      <td id=\"T_e393d_row84_col0\" class=\"data row84 col0\" >Updates attributes of this class with attributes from `update_str`.\n",
       "\n",
       "The expected format is ints, floats and strings as is, and for booleans use `true` or `false`. For example:\n",
       "\"n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index\"\n",
       "\n",
       "The keys to change have to already exist in the config object.\n",
       "\n",
       "Args:\n",
       "    update_str (`str`): String with attributes that should be updated for this class.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level0_row85\" class=\"row_heading level0 row85\" rowspan=\"4\">str</th>\n",
       "      <th id=\"T_e393d_level1_row85\" class=\"row_heading level1 row85\" >activation</th>\n",
       "      <td id=\"T_e393d_row85_col0\" class=\"data row85 col0\" >str(object='') -> str\n",
       "str(bytes_or_buffer[, encoding[, errors]]) -> str\n",
       "\n",
       "Create a new string object from the given object. If encoding or\n",
       "errors is specified, then the object must expose a data buffer\n",
       "that will be decoded using the given encoding and error handler.\n",
       "Otherwise, returns the result of object.__str__() (if defined)\n",
       "or repr(object).\n",
       "encoding defaults to sys.getdefaultencoding().\n",
       "errors defaults to 'strict'.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row86\" class=\"row_heading level1 row86\" >model_type</th>\n",
       "      <td id=\"T_e393d_row86_col0\" class=\"data row86 col0\" >str(object='') -> str\n",
       "str(bytes_or_buffer[, encoding[, errors]]) -> str\n",
       "\n",
       "Create a new string object from the given object. If encoding or\n",
       "errors is specified, then the object must expose a data buffer\n",
       "that will be decoded using the given encoding and error handler.\n",
       "Otherwise, returns the result of object.__str__() (if defined)\n",
       "or repr(object).\n",
       "encoding defaults to sys.getdefaultencoding().\n",
       "errors defaults to 'strict'.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row87\" class=\"row_heading level1 row87\" >name_or_path</th>\n",
       "      <td id=\"T_e393d_row87_col0\" class=\"data row87 col0\" >str(object='') -> str\n",
       "str(bytes_or_buffer[, encoding[, errors]]) -> str\n",
       "\n",
       "Create a new string object from the given object. If encoding or\n",
       "errors is specified, then the object must expose a data buffer\n",
       "that will be decoded using the given encoding and error handler.\n",
       "Otherwise, returns the result of object.__str__() (if defined)\n",
       "or repr(object).\n",
       "encoding defaults to sys.getdefaultencoding().\n",
       "errors defaults to 'strict'.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e393d_level1_row88\" class=\"row_heading level1 row88\" >transformers_version</th>\n",
       "      <td id=\"T_e393d_row88_col0\" class=\"data row88 col0\" >str(object='') -> str\n",
       "str(bytes_or_buffer[, encoding[, errors]]) -> str\n",
       "\n",
       "Create a new string object from the given object. If encoding or\n",
       "errors is specified, then the object must expose a data buffer\n",
       "that will be decoded using the given encoding and error handler.\n",
       "Otherwise, returns the result of object.__str__() (if defined)\n",
       "or repr(object).\n",
       "encoding defaults to sys.getdefaultencoding().\n",
       "errors defaults to 'strict'.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ba12ca3b800>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp.tab(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*설정값변경*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.max_position_embeddings = 4096\n",
    "config.num_labels = 2 # 이건 잘 설정되어있어서.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*설정값으로 모델불러오기*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_large = transformers.AutoModelForSequenceClassification.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*모델사용*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1689, -0.3778],\n",
       "        [ 0.0231, -0.3971],\n",
       "        [-0.2340, -0.2929]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_large(\n",
    "    tokenizer(d['posts'],padding=True,return_tensors=\"pt\")['input_ids']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# 예제4` -- sms_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sms', 'label'],\n",
       "        num_rows: 4459\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sms', 'label'],\n",
       "        num_rows: 1115\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_spam = datasets.load_dataset('sms_spam')['train'].train_test_split(test_size=0.2, seed=42)\n",
    "sms_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sms', 'label'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = sms_spam['train'].select(range(3))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(풀이)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0414, -0.0771],\n",
       "        [-0.0181,  0.0315],\n",
       "        [ 0.0169,  0.0101]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1(\n",
    "    tokenizer(d['sms'],padding=True,return_tensors=\"pt\")['input_ids']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. 이미지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model2 = transformers.AutoModelForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=3 # 그냥 대충 3이라고 했음.. 별 이유는 없음\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# 예제1` -- food101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = datasets.load_dataset(\"food101\", split=\"train[:3]\")\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(예비학습)` -- `torchvision.transforms` 에서 제공하는 기능들은 배치처리가 가능한가? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "totensor = torchvision.transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1216, 0.1137, 0.1098,  ..., 0.0039, 0.0039, 0.0000],\n",
       "         [0.1255, 0.1216, 0.1176,  ..., 0.0039, 0.0039, 0.0000],\n",
       "         [0.1294, 0.1255, 0.1255,  ..., 0.0039, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.2588, 0.2745, 0.2863,  ..., 0.3765, 0.3882, 0.3922],\n",
       "         [0.2353, 0.2471, 0.2667,  ..., 0.3373, 0.3373, 0.3373],\n",
       "         [0.2235, 0.2275, 0.2471,  ..., 0.3333, 0.3176, 0.3059]],\n",
       "\n",
       "        [[0.1373, 0.1294, 0.1255,  ..., 0.1020, 0.1020, 0.0980],\n",
       "         [0.1412, 0.1373, 0.1333,  ..., 0.1020, 0.1020, 0.0980],\n",
       "         [0.1451, 0.1412, 0.1412,  ..., 0.1020, 0.0980, 0.0980],\n",
       "         ...,\n",
       "         [0.2471, 0.2627, 0.2745,  ..., 0.3647, 0.3765, 0.3882],\n",
       "         [0.2235, 0.2353, 0.2549,  ..., 0.3255, 0.3333, 0.3333],\n",
       "         [0.2118, 0.2157, 0.2353,  ..., 0.3216, 0.3137, 0.3020]],\n",
       "\n",
       "        [[0.1412, 0.1333, 0.1294,  ..., 0.0902, 0.0902, 0.0863],\n",
       "         [0.1451, 0.1412, 0.1451,  ..., 0.0902, 0.0902, 0.0863],\n",
       "         [0.1490, 0.1451, 0.1529,  ..., 0.0902, 0.0863, 0.0863],\n",
       "         ...,\n",
       "         [0.1725, 0.1882, 0.2000,  ..., 0.2431, 0.2549, 0.2667],\n",
       "         [0.1490, 0.1608, 0.1804,  ..., 0.2039, 0.2118, 0.2118],\n",
       "         [0.1373, 0.1412, 0.1608,  ..., 0.2000, 0.1922, 0.1804]]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totensor(d['image'][0]) # 하나의 obs는 처리가능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pic should be PIL Image or ndarray. Got <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtotensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 배치는 처리불가능\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hf/lib/python3.12/site-packages/torchvision/transforms/functional.py:142\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    140\u001b[0m     _log_api_usage_once(to_tensor)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (F_pil\u001b[38;5;241m.\u001b[39m_is_pil_image(pic) \u001b[38;5;129;01mor\u001b[39;00m _is_numpy(pic)):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should be PIL Image or ndarray. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pic)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_numpy(pic) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_numpy_image(pic):\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should be 2/3 dimensional. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpic\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: pic should be PIL Image or ndarray. Got <class 'list'>"
     ]
    }
   ],
   "source": [
    "totensor(d['image']) # 배치는 처리불가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(풀이)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "compose = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize((224,224)) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1223, 0.1166, 0.1159,  ..., 0.0054, 0.0041, 0.0014],\n",
       "         [0.1314, 0.1290, 0.1266,  ..., 0.0037, 0.0024, 0.0033],\n",
       "         [0.1425, 0.1380, 0.1339,  ..., 0.0040, 0.0026, 0.0009],\n",
       "         ...,\n",
       "         [0.2242, 0.2150, 0.2476,  ..., 0.3578, 0.3704, 0.3554],\n",
       "         [0.2610, 0.2583, 0.2324,  ..., 0.3488, 0.3618, 0.3625],\n",
       "         [0.2408, 0.2613, 0.2533,  ..., 0.3370, 0.3427, 0.3388]],\n",
       "\n",
       "        [[0.1380, 0.1323, 0.1316,  ..., 0.1040, 0.1022, 0.0995],\n",
       "         [0.1471, 0.1447, 0.1423,  ..., 0.1018, 0.0985, 0.0935],\n",
       "         [0.1582, 0.1537, 0.1495,  ..., 0.1001, 0.0921, 0.0865],\n",
       "         ...,\n",
       "         [0.2155, 0.2087, 0.2431,  ..., 0.3306, 0.3478, 0.3367],\n",
       "         [0.2492, 0.2468, 0.2231,  ..., 0.3224, 0.3469, 0.3519],\n",
       "         [0.2290, 0.2496, 0.2419,  ..., 0.3107, 0.3303, 0.3340]],\n",
       "\n",
       "        [[0.1421, 0.1409, 0.1434,  ..., 0.0922, 0.0904, 0.0877],\n",
       "         [0.1565, 0.1559, 0.1541,  ..., 0.0900, 0.0874, 0.0844],\n",
       "         [0.1700, 0.1654, 0.1614,  ..., 0.0890, 0.0832, 0.0786],\n",
       "         ...,\n",
       "         [0.1395, 0.1314, 0.1649,  ..., 0.2197, 0.2347, 0.2217],\n",
       "         [0.1747, 0.1722, 0.1474,  ..., 0.2076, 0.2271, 0.2307],\n",
       "         [0.1545, 0.1750, 0.1672,  ..., 0.1958, 0.2097, 0.2124]]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compose(d['image'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=tensor(1.1654, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0286, -0.1753, -0.0153],\n",
       "        [ 0.0542, -0.2082,  0.0403],\n",
       "        [-0.0875, -0.1001,  0.1060]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2(\n",
    "    pixel_values = torch.stack(list(map(compose,d['image']))),\n",
    "    labels = torch.tensor([0,1,0]) # 그냥 확인없이 아무거나 넣음\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# 예제2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image_file_path', 'image', 'labels'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beans = datasets.load_dataset('beans')\n",
    "d = beans['train'].select(range(3))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(풀이)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=None, logits=tensor([[-0.2100, -0.1126, -0.1297],\n",
       "        [-0.1711, -0.0252, -0.0553],\n",
       "        [-0.1465,  0.0183, -0.0034]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2(\n",
    "    torch.stack(list(map(compose,d['image'])))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. 동영상 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model3 = transformers.VideoMAEForVideoClassification.from_pretrained(\n",
    "    \"MCG-NJU/videomae-base\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# 예제1` -- UCF101_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = huggingface_hub.hf_hub_download(\n",
    "    repo_id=\"sayakpaul/ucf101-subset\",\n",
    "    filename=\"UCF101_subset.tar.gz\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "# file_path는 다운로드한 압축파일이 존재하는 경로와 파일명이 string으로 저장되어있음.\n",
    "with tarfile.open(file_path) as t:\n",
    "     t.extractall(\"./data\") # 여기에서 \".\"은 현재폴더라는 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "├── test\n",
      "│   ├── ApplyEyeMakeup\n",
      "│   │   ├── UCF101\n",
      "│   │   ├── v_ApplyEyeMakeup_g03_c01.avi\n",
      "│   │   └── ...\n",
      "│   │   └── v_ApplyEyeMakeup_g23_c06.avi\n",
      "│   ├── ApplyLipstick\n",
      "│   │   ├── UCF101\n",
      "│   │   ├── v_ApplyLipstick_g14_c01.avi\n",
      "│   │   └── ...\n",
      "│   │   └── v_ApplyLipstick_g16_c04.avi\n",
      "│   └── ...\n",
      "│   └── BenchPress\n",
      "│       ├── UCF101\n",
      "│       ├── v_BenchPress_g05_c02.avi\n",
      "│       └── ...\n",
      "│       └── v_BenchPress_g25_c06.avi\n",
      "├── train\n",
      "│   ├── ApplyEyeMakeup\n",
      "│   │   ├── UCF101\n",
      "│   │   ├── v_ApplyEyeMakeup_g02_c03.avi\n",
      "│   │   └── ...\n",
      "│   │   └── v_ApplyEyeMakeup_g25_c07.avi\n",
      "│   ├── ApplyLipstick\n",
      "│   │   ├── UCF101\n",
      "│   │   ├── v_ApplyLipstick_g01_c02.avi\n",
      "│   │   └── ...\n",
      "│   │   └── v_ApplyLipstick_g24_c05.avi\n",
      "│   └── ...\n",
      "│   └── BenchPress\n",
      "│       ├── UCF101\n",
      "│       ├── v_BenchPress_g01_c05.avi\n",
      "│       └── ...\n",
      "│       └── v_BenchPress_g24_c05.avi\n",
      "└── val\n",
      "    ├── ApplyEyeMakeup\n",
      "    │   ├── UCF101\n",
      "    │   ├── v_ApplyEyeMakeup_g01_c01.avi\n",
      "    │   ├── v_ApplyEyeMakeup_g14_c05.avi\n",
      "    │   └── v_ApplyEyeMakeup_g20_c04.avi\n",
      "    ├── ApplyLipstick\n",
      "    │   ├── UCF101\n",
      "    │   ├── v_ApplyLipstick_g10_c04.avi\n",
      "    │   ├── v_ApplyLipstick_g20_c04.avi\n",
      "    │   └── v_ApplyLipstick_g25_c02.avi\n",
      "    └── ...\n",
      "    └── BenchPress\n",
      "        ├── UCF101\n",
      "        ├── v_BenchPress_g11_c05.avi\n",
      "        ├── v_BenchPress_g17_c02.avi\n",
      "        └── v_BenchPress_g17_c06.avi\n"
     ]
    }
   ],
   "source": [
    "mp.tree(\"./data/UCF101_subset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 67, 240, 320])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_path = \"./data/UCF101_subset/test/BenchPress/v_BenchPress_g05_c02.avi\"\n",
    "video = pytorchvideo.data.encoded_video.EncodedVideo.from_path(video_path).get_clip(0, float('inf'))['video']\n",
    "video.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(풀이)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 224, 224])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video.permute(1,0,2,3)[:16,:,:224,:224].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=None, logits=tensor([[-0.1593, -0.2056]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3(\n",
    "    #video.permute(1,0,2,3)[:16,:,:224,:224].reshape(1,16,3,224,224)\n",
    "    #video.permute(1,0,2,3)[:16,:,:224,:224].unsqueeze(0)\n",
    "    torch.stack([video.permute(1,0,2,3)[:16,:,:224,:224]])\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
