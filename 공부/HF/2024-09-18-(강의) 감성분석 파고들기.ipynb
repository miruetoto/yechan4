{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"(강의) 감성분석 파고들기\"\n",
    "author: \"신록예찬\"\n",
    "date: \"09/18/2024\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import transformers\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 이전코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` Step1~4를 위한 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Step1 \n",
    "# 데이터불러오기 = datasets.load_dataset\n",
    "# 데이터전처리하기1 = 토크나이저 = transformers.AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\") \n",
    "# def 데이터전처리하기2(examples):\n",
    "#     return 데이터전처리하기1(examples[\"text\"], truncation=True)\n",
    "# ## Step2 \n",
    "# 인공지능생성하기 = transformers.AutoModelForSequenceClassification.from_pretrained\n",
    "# ## Step3 \n",
    "# 데이터콜렉터 = transformers.DataCollatorWithPadding(tokenizer=토크나이저)\n",
    "# def 평가하기(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     predictions = np.argmax(predictions, axis=1)\n",
    "#     accuracy = evaluate.load(\"accuracy\")\n",
    "#     return accuracy.compute(predictions=predictions, references=labels)\n",
    "# 트레이너세부지침생성기 = transformers.TrainingArguments\n",
    "# 트레이너생성기 = transformers.Trainer\n",
    "# ## Step4 \n",
    "# 강인공지능생성하기 = transformers.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` Step 1~4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Step1 \n",
    "# 데이터 = 데이터불러오기('imdb')\n",
    "# 전처리된데이터 = 데이터.map(데이터전처리하기2,batched=True)\n",
    "# 전처리된훈련자료, 전처리된검증자료 = 전처리된데이터['train'], 전처리된데이터['test']\n",
    "# ## Step2 \n",
    "# 인공지능 = 인공지능생성하기(\"distilbert/distilbert-base-uncased\", num_labels=2)\n",
    "# ## Step3 \n",
    "# 트레이너세부지침 = 트레이너세부지침생성기(\n",
    "#     output_dir=\"my_awesome_model\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=2, # 전체문제세트를 2번 공부하라..\n",
    "#     weight_decay=0.01,\n",
    "#     eval_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     load_best_model_at_end=True,\n",
    "#     push_to_hub=False,\n",
    "# )\n",
    "# 트레이너 = 트레이너생성기(\n",
    "#     model=인공지능,\n",
    "#     args=트레이너세부지침,\n",
    "#     train_dataset=전처리된훈련자료,\n",
    "#     eval_dataset=전처리된검증자료,\n",
    "#     tokenizer=토크나이저,\n",
    "#     data_collator=데이터콜렉터,\n",
    "#     compute_metrics=평가하기,\n",
    "# )\n",
    "# 트레이너.train()\n",
    "# ## Step4 \n",
    "# 강인공지능 = 강인공지능생성하기(\"sentiment-analysis\", model=\"my_awesome_model/checkpoint-1563\")\n",
    "# print(강인공지능(\"This movie was a huge disappointment.\"))\n",
    "# print(강인공지능(\"This was a masterpiece.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 가장 중요한 원초적 질문: 데이터 셋을 바꿔치기 하려면? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 내가 원하는 데이터를 아래와 같은 형태로 정리가능해야함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "데이터 = datasets.load_dataset('imdb')\n",
    "데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'label'],\n",
       "         num_rows: 25000\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text', 'label'],\n",
       "         num_rows: 25000\n",
       "     })\n",
       "     unsupervised: Dataset({\n",
       "         features: ['text', 'label'],\n",
       "         num_rows: 50000\n",
       "     })\n",
       " }),\n",
       " datasets.dataset_dict.DatasetDict)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "데이터, type(데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 25000\n",
       " }),\n",
       " datasets.arrow_dataset.Dataset)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "데이터['train'], type(데이터['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` `datasets.arrow_dataset.Dataset` 의 인스턴스 만들기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래와 같은 함수가 있음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets.Dataset.from_dict?\n",
    "# 클래스메소드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 함수를 쓰는 방법?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\star$ `datasets.Dataset.from_dict` 사용법\n",
    "\n",
    "`datasets.Dataset.from_dict`는 Python의 딕셔너리(`dict`)를 `Dataset` 객체로 변환하는 함수입니다. 주로 딕셔너리 형태의 데이터를 빠르게 데이터셋으로 변환할 때 사용됩니다.\n",
    "\n",
    "**간단한 사용법**\n",
    "\n",
    "```python\n",
    "from datasets import Dataset\n",
    "\n",
    "# 딕셔너리를 Dataset으로 변환\n",
    "data_dict = {\n",
    "    'text': [\"Hello world\", \"How are you?\", \"Fine, thanks!\"],\n",
    "    'label': [0, 1, 1]\n",
    "}\n",
    "\n",
    "# Dataset 생성\n",
    "dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "# 출력\n",
    "print(dataset)\n",
    "```\n",
    "\n",
    "**주요 매개변수:**\n",
    "- `mapping`: 필수, 문자열을 키로 하고 리스트 또는 배열을 값으로 하는 딕셔너리.\n",
    "- `features`: 선택, 데이터셋의 각 필드 타입을 정의.\n",
    "- `info`: 선택, 데이터셋에 대한 추가 정보(설명, 인용 등).\n",
    "- `split`: 선택, 데이터셋의 나누기('train', 'test' 등).\n",
    "\n",
    "**반환값:**\n",
    "- `Dataset`: PyArrow 기반의 데이터셋 객체."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {\n",
    "    'text': [\n",
    "        \"I prefer making decisions based on logic and objective facts.\",\n",
    "        \"I always consider how others might feel when making a decision.\",\n",
    "        \"Data and analysis drive most of my decisions.\",\n",
    "        \"I rely on my empathy and personal values to guide my choices.\"\n",
    "    ],\n",
    "    'label': [0, 1, 0, 1]  # 0은 T(사고형), 1은 F(감정형)\n",
    "}\n",
    "\n",
    "test_dict = {\n",
    "    'text': [\n",
    "        \"I find it important to weigh all the pros and cons logically.\",\n",
    "        \"When making decisions, I prioritize harmony and people's emotions.\"\n",
    "    ],\n",
    "    'label': [0, 1]  # 0은 T(사고형), 1은 F(감정형)\n",
    "}\n",
    "train = datasets.Dataset.from_dict(train_dict)\n",
    "test = datasets.Dataset.from_dict(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` `datasets.dataset_dict.DatasetDict`의 인스턴스 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "_데이터 = datasets.DatasetDict({'train':train, 'test':test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 일단 아래와 같은 형태로 분석할 데이터를 정리할 수만 있다면, 나머지는 코드를 복붙해서 일단 돌릴 수 있음. \n",
    "\n",
    "```Python\n",
    "train_dict = {\n",
    "    'text': [\n",
    "        \"I prefer making decisions based on logic and objective facts.\",\n",
    "        \"I always consider how others might feel when making a decision.\",\n",
    "        \"Data and analysis drive most of my decisions.\",\n",
    "        \"I rely on my empathy and personal values to guide my choices.\"\n",
    "    ],\n",
    "    'label': [0, 1, 0, 1]  # 0은 T(사고형), 1은 F(감정형)\n",
    "}\n",
    "\n",
    "test_dict = {\n",
    "    'text': [\n",
    "        \"I find it important to weigh all the pros and cons logically.\",\n",
    "        \"When making decisions, I prioritize harmony and people's emotions.\"\n",
    "    ],\n",
    "    'label': [0, 1]  # 0은 T(사고형), 1은 F(감정형)\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 토크나이저"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 기본적인 사용방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgb3/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "토크나이저 = 데이터전처리하기1 = transformers.AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#토크나이저?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "\n",
    "### $\\star$ `토크나이저` 사용법 (ref: ChatGPT)\n",
    "\n",
    "**주요 파라미터**:\n",
    "\n",
    "1. **text**: \n",
    "   - `Union[str, List[str], List[List[str]]]`\n",
    "   - 주어진 텍스트를 토큰화합니다. 이 텍스트는 문자열일 수도 있고, 문자열의 리스트 또는 리스트 안의 리스트일 수도 있습니다.\n",
    "\n",
    "2. **text_pair**: \n",
    "   - `Union[str, List[str], List[List[str]], NoneType]`\n",
    "   - 두 개의 텍스트를 함께 모델에 입력할 때 사용됩니다. 예를 들어, 질문-답변 쌍 같은 경우 이 두 번째 텍스트를 넣습니다.\n",
    "\n",
    "3. **text_target**: \n",
    "   - `Union[str, List[str], List[List[str]]]`\n",
    "   - 토큰화를 할 때 목표(target) 텍스트에 해당하는 부분입니다. 주로 시퀀스 생성 모델에서 활용됩니다.\n",
    "\n",
    "4. **text_pair_target**: \n",
    "   - `Union[str, List[str], List[List[str]], NoneType]`\n",
    "   - 위의 `text_pair`와 유사하게 목표(target) 텍스트의 두 번째 텍스트를 나타냅니다.\n",
    "\n",
    "5. **add_special_tokens**: \n",
    "   - `bool`\n",
    "   - 문장의 시작, 끝, 구분자 같은 특별한 토큰을 추가할지 여부를 결정합니다. 기본값은 `True`입니다.\n",
    "\n",
    "6. **padding**: \n",
    "   - `Union[bool, str, transformers.utils.generic.PaddingStrategy]`\n",
    "   - 문장 길이가 다를 때 패딩을 넣어 문장의 길이를 동일하게 맞춥니다. 패딩 전략에는 `True`, `False`, `'longest'`, `'max_length'` 등이 있습니다.\n",
    "\n",
    "7. **truncation**: \n",
    "   - `Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy]`\n",
    "   - 문장이 너무 길 경우 지정된 최대 길이에 맞춰 잘라내는 옵션입니다. 전략에는 `True`, `False`, `'longest_first'`, `'only_first'`, `'only_second'` 등이 있습니다.\n",
    "\n",
    "8. **max_length**: \n",
    "   - `Optional[int]`\n",
    "   - 문장의 최대 길이를 설정합니다. `None`일 경우 기본 설정을 따릅니다.\n",
    "\n",
    "9. **stride**: \n",
    "   - `int`\n",
    "   - 텍스트를 자를 때 중첩을 만들기 위한 옵션입니다. 즉, 자른 부분과 다음 부분 사이의 겹치는 범위를 설정합니다.\n",
    "\n",
    "10. **is_split_into_words**: \n",
    "    - `bool`\n",
    "    - 텍스트가 이미 단어 단위로 분리되어 있는지 여부를 나타냅니다. 기본적으로는 `False`로, 텍스트가 단어 단위로 분리되지 않았다고 가정합니다.\n",
    "\n",
    "11. **return_tensors**: \n",
    "    - `Union[str, transformers.utils.generic.TensorType, NoneType]`\n",
    "    - 출력 형식으로 텐서를 반환할지 여부를 설정합니다. `'pt'`(PyTorch), `'tf'`(TensorFlow), `'np'`(NumPy) 등을 지정할 수 있습니다.\n",
    "\n",
    "12. **return_token_type_ids**: \n",
    "    - `Optional[bool]`\n",
    "    - 토큰 타입 ID를 반환할지 여부를 설정합니다. 주로 두 개의 문장을 함께 처리할 때 문장을 구분하기 위해 사용됩니다.\n",
    "\n",
    "13. **return_attention_mask**: \n",
    "    - `Optional[bool]`\n",
    "    - `attention_mask`를 반환할지 여부를 설정합니다. 패딩된 토큰이 모델의 어텐션에 영향을 주지 않도록 마스크를 설정합니다.\n",
    "\n",
    "14. **return_overflowing_tokens**: \n",
    "    - `bool`\n",
    "    - 텍스트가 최대 길이를 초과하는 경우, 잘린 토큰을 반환할지 여부를 결정합니다.\n",
    "\n",
    "15. **return_special_tokens_mask**: \n",
    "    - `bool`\n",
    "    - 특별한 토큰에 대한 마스크를 반환할지 여부를 설정합니다.\n",
    "\n",
    "16. **return_offsets_mapping**: \n",
    "    - `bool`\n",
    "    - 텍스트의 각 토큰이 원본 텍스트에서 어느 위치에 있는지 나타내는 오프셋 맵핑을 반환할지 여부를 설정합니다.\n",
    "\n",
    "17. **return_length**: \n",
    "    - `bool`\n",
    "    - 토큰화된 문장의 길이를 반환할지 여부를 설정합니다.\n",
    "\n",
    "18. **verbose**: \n",
    "    - `bool`\n",
    "    - 디버깅 메시지를 출력할지 여부를 설정합니다. 기본값은 `True`로 설정되어 있습니다.\n",
    "\n",
    "**사용 예시**:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 토크나이저 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 텍스트 토큰화\n",
    "encoding = tokenizer(\n",
    "    text=\"Hello, how are you?\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=10,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(encoding)\n",
    "```\n",
    "\n",
    "이 코드에서는 \"Hello, how are you?\"라는 텍스트를 `bert-base-uncased` 토크나이저로 토큰화하고, 패딩과 트렁케이션을 적용하며, PyTorch 텐서 형식으로 반환하도록 설정했습니다.\n",
    "\n",
    "이러한 파라미터는 주로 자연어 처리(NLP) 모델을 훈련하거나 추론할 때 데이터 전처리 과정에서 많이 사용됩니다.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 기본사용1: 단어별로 다른숫자를 맵핑 + 처음과 끝은 항상 `101`, `102`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7632, 7592, 102], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "토크나이저(\"hi hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7632, 7632, 7592, 102], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "토크나이저(\"hi hi hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7632, 7632, 7592, 7592, 7592, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "토크나이저(\"hi hi hello hello hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 7632, 7592, 102]], 'attention_mask': [[1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "토크나이저([\"hi hello\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 기본사용2: 텍스트 혹은 텍스트의 리스트, 리스트의 리스트를 전달가능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 7632, 7592, 102], [101, 7592, 7592, 102], [101, 7632, 7632, 102]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "토크나이저([\"hi hello\", \"hello hello\", \"hi hi\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` `truncation=True` 의 역할 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7632, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct = 토크나이저('hi hello'*1000,truncation=True)\n",
    "dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dct['input_ids']), len(dct['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` maxlen, padding, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7632, 7592, 102, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct = 토크나이저('hi hello',max_length=10,padding=\"max_length\")\n",
    "dct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 인공지능 ($\\star$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. 1단계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 인공지능에 대한 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 인공지능 불러오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(43052)\n",
    "인공지능 = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", \n",
    "    num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 인공지능의 정체? 엄청나게 많은 숫자들이 포함된 어떠한 물체 (엄청나게 많은 파라메터들이 포함된 네트워크)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "인공지능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0234,  0.0279,  0.0242,  ...,  0.0091, -0.0063, -0.0133],\n",
       "        [ 0.0087,  0.0007, -0.0099,  ...,  0.0183, -0.0007,  0.0295]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "인공지능.classifier.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0122,  0.0030,  0.0301,  ...,  0.0003,  0.0327, -0.0123],\n",
       "        [-0.0041,  0.0008,  0.0169,  ..., -0.0163, -0.0117, -0.0135],\n",
       "        [ 0.0020, -0.0215, -0.0021,  ...,  0.0016,  0.0102, -0.0483],\n",
       "        ...,\n",
       "        [-0.0026, -0.0327,  0.0099,  ...,  0.0341, -0.0184, -0.0109],\n",
       "        [ 0.0088, -0.0345, -0.0011,  ...,  0.0018,  0.0172, -0.0122],\n",
       "        [-0.0148,  0.0147, -0.0184,  ..., -0.0166,  0.0241,  0.0201]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "인공지능.pre_classifier.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 인공지능? \"입력정보 -> 정리된숫자 -> 계산 -> 계산된숫자 -> 출력정보\" 의 과정에서 \"계산\"을 담당\n",
    "\n",
    "- 인공지능이 가지고 있는 숫자들은 계산에 사용되는 숫자들임.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 입력정보에 영화에 대한 부정적 평가를 넣는다면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2023,  3185,  2001,  1037,  4121, 10520,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "입력정보_원시텍스트 = \"This movie was a huge disappointment.\"\n",
    "정리된숫자_토큰화된자료 = 토크나이저(입력정보_원시텍스트,truncation=True,return_tensors='pt')\n",
    "정리된숫자_토큰화된자료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.1173,  0.0261]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "인공지능(**정리된숫자_토큰화된자료)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11731046,  0.02610319]], dtype=float32)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "계산된숫자_로짓 = 인공지능(**정리된숫자_토큰화된자료).logits.detach().numpy()\n",
    "계산된숫자_로짓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46420792, 0.53579205]], dtype=float32)"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "출력정보_확률 = np.exp(계산된숫자_로짓) / np.exp(계산된숫자_로짓).sum()\n",
    "출력정보_확률 # 0일확률, 1일확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "출력정보_확률.argmax() # 부정적 영화평가에 대한 인공지능의 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "계산된숫자_로짓.argmax() # 부정적 영화평가에 대한 인공지능의 예측 <-- 이렇게 구해도됩니다.. 왜??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 입력정보에 영화에 대한 긍정적 평가를 넣는다면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2023,  2001,  1037, 17743,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "입력정보_원시텍스트 = \"This was a masterpiece.\"\n",
    "정리된숫자_토큰화된자료 = 토크나이저(입력정보_원시텍스트,truncation=True,return_tensors='pt')\n",
    "정리된숫자_토큰화된자료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.1080,  0.0264]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "인공지능(**정리된숫자_토큰화된자료)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.10802954,  0.02638793]], dtype=float32)"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "계산된숫자_로짓 = 인공지능(**정리된숫자_토큰화된자료).logits.detach().numpy()\n",
    "계산된숫자_로짓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46644613, 0.53355384]], dtype=float32)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "출력정보_확률 = np.exp(계산된숫자_로짓) / np.exp(계산된숫자_로짓).sum()\n",
    "출력정보_확률 # 0일확률, 1일확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "출력정보_확률.argmax()\n",
    "계산된숫자_로짓.argmax() # 부정적 영화평가에 대한 인공지능의 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 아무숫자나 뱉어내는듯 $\\to$ 멍청한 인공지능.. (옹호: 당연하지 학습전이니까)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 인공지능에 대한 이해1: 인공지능은 \"정리된숫자\"를 입력으로 하고 일련의 계산을 거쳐 \"계산된숫자\"를 출력해주는 함수라 생각할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 인공지능에 대한 이해2: 인공지능은 (1) 많은숫자들과 (2) 고유의 계산방식을 가지고 있음. \n",
    "\n",
    "- 인공지능이 내부에 자체적으로 저장하고 있는 숫자를 **파라메터**라고 부름. \n",
    "- 인공지능은 나름의 법칙에 따라 \"데이터\"와 \"파라메터\"를 계산함. 즉 인공지능은 자체적으로 데이터와 파라메터를 어떻게 계산할지 알고있는데, 이러한 고유의 계산방식을 **아키텍처**라고 말함. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 인공지능에 대한 이해3: 두 인공지능이 서로 다른 고유의 계산방식을 가지고 있다면 두 인공지능은 \"다른 모델\"임. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 인공지능에 대한 이해3': 동일한 생성방식으로 만들어진 인공지능들은 모두 같은 모델임. 예를들면 아래의 인공지능1,2는 같은 모델임 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "인공지능1 = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", \n",
    "    num_labels=2\n",
    ")\n",
    "인공지능2 = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", \n",
    "    num_labels=2\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 인공지능에 대한 이해4: 두 인공지능이 같은 모델이라고 해도, 항상 같은 결과를 주는건 아님. 파라메터에 따라 다른결과를 줄 수 도 있음. (예를들면 위의 인공지능1,2는 같은 모델이지만 다른 파라메터를 가지므로 다른 결과를 줌) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. 2단계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 미니배치를 이해하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 예비학습 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [2, 3],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([[1,2],[2,3],[3,4]])\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33333333, 0.66666667],\n",
       "       [0.4       , 0.6       ],\n",
       "       [0.42857143, 0.57142857]])"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr / arr.sum(axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 예비개념1: 인공지능은 사실 영화평을 하나씩 하나씩 처리하지 않는다. 덩어리로 처리한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 예비개념2: 그렇다고 해서 인공지능이 25000개를 모두 덩어리로 처리하는건 아니다 $\\to$ 16개씩, 혹은 32개씩 묶어서 작은덩어리로 만든 후 처리한다. \n",
    "\n",
    "- 16,32 와 같은 숫자를 `batch_size` 라고 한다.\n",
    "- 16개, 32개로 모인 작은덩어리를 미니배치라고 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 16개의 입력정보를 한번에 처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47823825, 0.5217617 ],\n",
       "       [0.47215328, 0.5278467 ],\n",
       "       [0.4804948 , 0.5195052 ],\n",
       "       [0.46609667, 0.5339033 ],\n",
       "       [0.48814487, 0.5118551 ],\n",
       "       [0.48004225, 0.5199578 ],\n",
       "       [0.49288097, 0.50711906],\n",
       "       [0.49470255, 0.5052974 ],\n",
       "       [0.4941453 , 0.50585467],\n",
       "       [0.49016201, 0.50983804],\n",
       "       [0.495789  , 0.504211  ],\n",
       "       [0.48260576, 0.51739424],\n",
       "       [0.49386355, 0.5061364 ],\n",
       "       [0.49013382, 0.5098662 ],\n",
       "       [0.48240176, 0.5175982 ],\n",
       "       [0.49301967, 0.5069803 ]], dtype=float32)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "입력정보들_원시텍스트 = 데이터['train'][:16]['text']\n",
    "정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\n",
    "계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.data.numpy()\n",
    "출력정보들_확률 = np.exp(계산된숫자들_로짓) / np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\n",
    "출력정보들_확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#출력정보들_확률.argmax(axis=1) \n",
    "계산된숫자들_로짓.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 기억할 것: `정리된숫자들_토큰화된자료` 는 모두 길이가 512임. (그렇게 되도록 패딩함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 512]), torch.Size([16, 512]))"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "정리된숫자들_토큰화된자료['input_ids'].shape, 정리된숫자들_토큰화된자료['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 실제 단어수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([363, 304, 133, 185, 495, 154, 143, 388, 512, 297, 365, 171, 192, 173,\n",
       "        470, 263])"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "정리된숫자들_토큰화된자료['attention_mask'].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 패딩된단어수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([149, 208, 379, 327,  17, 358, 369, 124,   0, 215, 147, 341, 320, 339,\n",
       "         42, 249])"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512 - 정리된숫자들_토큰화된자료['attention_mask'].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 이러한 변환이 이루어지는 이유? 인공지능은 항상 `(n,m)` 차원으로 정리된 정리된숫자들을 입력으로 받아야함. \n",
    "\n",
    "- 왜? 사실 인공지능은 행렬계산을 하도록 설계되어있음.\n",
    "- 그래서 할수없이 패딩처리를 해야하는 것임. (실제로는 행렬로 만들수 없지만 억지로 만들기 위해서..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. 3단계 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 동적패딩을 이해하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 만약에 `batch_size=4` 로 설정하여 처리한다면? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47823825, 0.5217618 ],\n",
       "       [0.47215328, 0.5278467 ],\n",
       "       [0.4804948 , 0.5195052 ],\n",
       "       [0.46609667, 0.5339033 ]], dtype=float32)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "입력정보들_원시텍스트 = 데이터['train'][:4]['text']\n",
    "정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\n",
    "계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.data.numpy()\n",
    "출력정보들_확률 = np.exp(계산된숫자들_로짓) / np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\n",
    "출력정보들_확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1])"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "계산된숫자들_로짓.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` `정리된숫자들_토큰화된자료`의 차원은 어떠할까? (4,512)로 예상되지 않을까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 363]), torch.Size([4, 363]))"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "정리된숫자들_토큰화된자료['input_ids'].shape, 정리된숫자들_토큰화된자료['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 끝차원이 512가 아니라 363이다.. 왜?? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 덩어리의 상태에 따라서 유동적으로 패딩 $\\to$ 그래도 잘 돌아감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 363])\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0847,  0.0024],\n",
      "        [-0.0830,  0.0285],\n",
      "        [-0.0600,  0.0180],\n",
      "        [-0.0919,  0.0440]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "입력정보들_원시데이터 = 데이터['train'][:4]['text']\n",
    "정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시데이터,truncation=True,return_tensors='pt',padding=True)\n",
    "print(정리된숫자들_토큰화된자료['input_ids'].shape)\n",
    "print(인공지능(**정리된숫자들_토큰화된자료))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 495])\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0422,  0.0053],\n",
      "        [-0.0646,  0.0152],\n",
      "        [-0.0172,  0.0112],\n",
      "        [-0.0283, -0.0072]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "입력정보들_원시데이터 = 데이터['train'][4:8]['text']\n",
    "정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시데이터,truncation=True,return_tensors='pt',padding=True)\n",
    "print(정리된숫자들_토큰화된자료['input_ids'].shape)\n",
    "print(인공지능(**정리된숫자들_토큰화된자료))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512])\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0246, -0.0012],\n",
      "        [-0.0594, -0.0200],\n",
      "        [-0.0240, -0.0071],\n",
      "        [-0.0836, -0.0140]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "입력정보들_원시데이터 = 데이터['train'][8:12]['text']\n",
    "정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시데이터,truncation=True,return_tensors='pt',padding=True)\n",
    "print(정리된숫자들_토큰화된자료['input_ids'].shape)\n",
    "print(인공지능(**정리된숫자들_토큰화된자료))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 470])\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0440, -0.0195],\n",
      "        [-0.0469, -0.0074],\n",
      "        [-0.0542,  0.0162],\n",
      "        [-0.0316, -0.0037]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "입력정보들_원시데이터 = 데이터['train'][12:16]['text']\n",
    "정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시데이터,truncation=True,return_tensors='pt',padding=True)\n",
    "print(정리된숫자들_토큰화된자료['input_ids'].shape)\n",
    "print(인공지능(**정리된숫자들_토큰화된자료))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 싹다 512로 통일한것대비 큰 차이없음.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512])\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0847,  0.0024],\n",
      "        [-0.0830,  0.0285],\n",
      "        [-0.0600,  0.0180],\n",
      "        [-0.0919,  0.0440],\n",
      "        [-0.0422,  0.0053],\n",
      "        [-0.0646,  0.0152],\n",
      "        [-0.0172,  0.0112],\n",
      "        [-0.0283, -0.0072],\n",
      "        [-0.0246, -0.0012],\n",
      "        [-0.0594, -0.0200],\n",
      "        [-0.0240, -0.0071],\n",
      "        [-0.0836, -0.0140],\n",
      "        [-0.0440, -0.0195],\n",
      "        [-0.0469, -0.0074],\n",
      "        [-0.0542,  0.0162],\n",
      "        [-0.0316, -0.0037]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "입력정보들_원시데이터 = 데이터['train'][:16]['text']\n",
    "정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시데이터,truncation=True,return_tensors='pt',padding=True)\n",
    "print(정리된숫자들_토큰화된자료['input_ids'].shape)\n",
    "print(인공지능(**정리된숫자들_토큰화된자료))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. 4단계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 손실(=loss)의 개념을 이해하자. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` `정리된숫자들_토큰화된자료`에서 `labels` 를 추가 전달하면 `인공지능(**정리된숫자들_토큰화된자료)`의 결과로 `loss`가 추가계산됨 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.7461, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0847,  0.0024],\n",
       "        [-0.0830,  0.0285],\n",
       "        [-0.0600,  0.0180],\n",
       "        [-0.0919,  0.0440]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "입력정보들_원시데이터 = 데이터['train'][:4]['text']\n",
    "정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시데이터,truncation=True,return_tensors='pt',padding=True)\n",
    "#데이터['train'][:4]['label']\n",
    "정리된숫자들_토큰화된자료['labels'] = torch.tensor([0,0,0,0]) # 정답입력\n",
    "인공지능(**정리된숫자들_토큰화된자료)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 정리를 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제정답: [0, 0, 0, 0]\n",
      "인공지능의예측: [1 1 1 1]\n",
      "인공지능확신정도: [0.5217618 0.5278467 0.5195052 0.5339033]\n",
      "손실: 0.7461\n"
     ]
    }
   ],
   "source": [
    "입력정보들_원시데이터 = 데이터['train'][:4]['text']\n",
    "정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시데이터,truncation=True,return_tensors='pt',padding=True)\n",
    "print(f'실제정답: {데이터['train'][:4]['label']}')\n",
    "정리된숫자들_토큰화된자료['labels'] = torch.tensor(데이터['train'][:4]['label']) # 정답입력\n",
    "계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.detach().numpy()\n",
    "출력정보들_확률 = np.exp(계산된숫자들_로짓) / np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\n",
    "print(f'인공지능의예측: {출력정보들_확률.argmax(axis=1)}')\n",
    "print(f'인공지능확신정도: {출력정보들_확률.max(axis=1)}')\n",
    "print(f'손실: {인공지능(**정리된숫자들_토큰화된자료).loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` loss는 작을수록 좋은 것임. 문제를 많이 틀릴수록 loss가 큼, 문제를 조금 틀릴수록 loss가 작음  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**텍스트0~텍스트3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제정답: [0, 0, 0, 0]\n",
      "인공지능의예측: [1 1 1 1]\n",
      "인공지능확신정도: [0.5217618 0.5278467 0.5195052 0.5339033]\n",
      "손실: 0.7461\n"
     ]
    }
   ],
   "source": [
    "입력정보들_원시데이터 = 데이터['train'][:4]['text']\n",
    "정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시데이터,truncation=True,return_tensors='pt',padding=True)\n",
    "print(f'실제정답: {데이터['train'][:4]['label']}')\n",
    "정리된숫자들_토큰화된자료['labels'] = torch.tensor(데이터['train'][:4]['label']) # 정답입력\n",
    "계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.detach().numpy()\n",
    "출력정보들_확률 = np.exp(계산된숫자들_로짓) / np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\n",
    "print(f'인공지능의예측: {출력정보들_확률.argmax(axis=1)}')\n",
    "print(f'인공지능확신정도: {출력정보들_확률.max(axis=1)}')\n",
    "print(f'손실: {인공지능(**정리된숫자들_토큰화된자료).loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**텍스트12498~텍스트12501**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제정답: [0, 0, 1, 1]\n",
      "인공지능의예측: [1 1 1 1]\n",
      "인공지능확신정도: [0.52731967 0.5243837  0.51578873 0.5351162 ]\n",
      "손실: 0.6950\n"
     ]
    }
   ],
   "source": [
    "입력정보들_원시데이터 = 데이터['train'][12498:12502]['text']\n",
    "정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시데이터,truncation=True,return_tensors='pt',padding=True)\n",
    "print(f'실제정답: {데이터['train'][12498:12502]['label']}')\n",
    "정리된숫자들_토큰화된자료['labels'] = torch.tensor(데이터['train'][12498:12502]['label']) # 정답입력\n",
    "계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.detach().numpy()\n",
    "출력정보들_확률 = np.exp(계산된숫자들_로짓) / np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\n",
    "print(f'인공지능의예측: {출력정보들_확률.argmax(axis=1)}')\n",
    "print(f'인공지능확신정도: {출력정보들_확률.max(axis=1)}')\n",
    "print(f'손실: {인공지능(**정리된숫자들_토큰화된자료).loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 똑같이 틀려도 오답에 대한 확신이 강할수록 loss가 크다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**텍스트0~텍스트1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제정답: [0, 0]\n",
      "인공지능의예측: [1 1]\n",
      "인공지능확신정도: [0.5217617 0.5278467]\n",
      "손실: 0.7440\n"
     ]
    }
   ],
   "source": [
    "입력정보들_원시데이터 = 데이터['train'][:2]['text']\n",
    "정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시데이터,truncation=True,return_tensors='pt',padding=True)\n",
    "print(f'실제정답: {데이터['train'][:2]['label']}')\n",
    "정리된숫자들_토큰화된자료['labels'] = torch.tensor(데이터['train'][:2]['label']) # 정답입력\n",
    "계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.detach().numpy()\n",
    "출력정보들_확률 = np.exp(계산된숫자들_로짓) / np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\n",
    "print(f'인공지능의예측: {출력정보들_확률.argmax(axis=1)}')\n",
    "print(f'인공지능확신정도: {출력정보들_확률.max(axis=1)}')\n",
    "print(f'손실: {인공지능(**정리된숫자들_토큰화된자료).loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**텍스트1~텍스트2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제정답: [0, 0]\n",
      "인공지능의예측: [1 1]\n",
      "인공지능확신정도: [0.5278467 0.5195052]\n",
      "손실: 0.7417\n"
     ]
    }
   ],
   "source": [
    "입력정보들_원시데이터 = 데이터['train'][1:3]['text']\n",
    "정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시데이터,truncation=True,return_tensors='pt',padding=True)\n",
    "print(f'실제정답: {데이터['train'][1:3]['label']}')\n",
    "정리된숫자들_토큰화된자료['labels'] = torch.tensor(데이터['train'][1:3]['label']) # 정답입력\n",
    "계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.detach().numpy()\n",
    "출력정보들_확률 = np.exp(계산된숫자들_로짓) / np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\n",
    "print(f'인공지능의예측: {출력정보들_확률.argmax(axis=1)}')\n",
    "print(f'인공지능확신정도: {출력정보들_확률.max(axis=1)}')\n",
    "print(f'손실: {인공지능(**정리된숫자들_토큰화된자료).loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 손실 = 인공지능의 \"멍청한정도\" 혹은 \"똑똑한정도\"을 숫자화한것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "\n",
    "### $\\star\\star\\star$ 학습이 가능한 이유 (대충 아이디어만) \n",
    "\n",
    "`1`. 랜덤으로 1개의 인공지능을 생성한다. (아래의 코드로 가능)\n",
    "\n",
    "```Python\n",
    "인공지능 = 인공지능생성기()\n",
    "```\n",
    "\n",
    "`2`. 인공지능의 파라메터중 하나를 찍는다. 예를들면 아래와 같은 상황이 있다고 하자. \n",
    "\n",
    "```Python\n",
    "인공지능.classifier.weight\n",
    "```\n",
    "```\n",
    "Parameter containing:\n",
    "tensor([[-0.0234,  0.0279,  0.0242,  ...,  0.0091, -0.0063, -0.0133],\n",
    "        [ 0.0087,  0.0007, -0.0099,  ...,  0.0183, -0.0007,  0.0295]],\n",
    "       requires_grad=True)\n",
    "```\n",
    "\n",
    "하나의 숫자 `-0.0234` 를 선택한다.\n",
    "\n",
    "`3`. `-0.0234`의 값을 조금 변화시켜본다. 예를들면, `-0.0235`, `-0.0233` 와 같은 식으로 변화시켜본뒤 loss를 관찰한다. \n",
    "\n",
    "`4`. 원래값과 변화시킨값들중 loss를 가장 작게 만드는 값으로 `-0.0234`를 update한다. \n",
    "\n",
    "`5`. 다른 모든 파라메터에 대하여 1~4를 반복한다. (과정을 반복할수록 loss가 작아지겠죠, 즉 인공지능은 똑똑해지겠죠)\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 인공지능의 학습은 마법같은 신비한 현상이 아니고 극한의 노가다를 통해 얻어지는 산물일 뿐이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 데이터전처리2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "def 데이터전처리하기2(examples):\n",
    "    return 데이터전처리하기1(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████| 25000/25000 [00:03<00:00, 8260.93 examples/s]\n"
     ]
    }
   ],
   "source": [
    "데이터 = datasets.load_dataset('imdb')\n",
    "전처리된데이터 = 데이터.map(데이터전처리하기2,batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 데이터콜렉터 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 데이터콜렉터 사용방법 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorWithPadding(tokenizer=DistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}, padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "데이터콜렉터 = 동적패딩처리하기 = transformers.DataCollatorWithPadding(tokenizer=토크나이저,return_tensors='pt')\n",
    "데이터콜렉터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'label', 'input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "작은덩어리_딕셔너리 = 전처리된데이터['train'][:4]\n",
    "작은덩어리_딕셔너리.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "작은덩어리_리스트 = [{\n",
    "    'label': 작은덩어리_딕셔너리['label'][i],\n",
    "    'input_ids': 작은덩어리_딕셔너리['input_ids'][i],\n",
    "    'attention_mask': 작은덩어리_딕셔너리['attention_mask'][i]\n",
    "} for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045, 12524,  ...,  5436,  1012,   102],\n",
       "        [  101,  1000,  1045,  ...,     0,     0,     0],\n",
       "        [  101,  2065,  2069,  ...,     0,     0,     0],\n",
       "        [  101,  2023,  2143,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 0, 0, 0])}"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "데이터콜렉터(작은덩어리_리스트)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.7461, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0847,  0.0024],\n",
       "        [-0.0830,  0.0285],\n",
       "        [-0.0600,  0.0180],\n",
       "        [-0.0919,  0.0440]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "인공지능(**데이터콜렉터(작은덩어리_리스트))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 데이터콜렉터의 기능: batch_size 에 따라서 패딩시켜줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` `accuracy.compute`의 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.3333333333333333}"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.compute(references=[0, 0, 0], predictions=[0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 함수내용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def 평가하기(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     predictions = np.argmax(predictions, axis=1)\n",
    "#     accuracy = evaluate.load(\"accuracy\")\n",
    "#     return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def 평가하기(eval_pred):\n",
    "    계산된숫자들_로짓, 실제정답 = eval_pred\n",
    "    인공지능의예측 = np.argmax(계산된숫자들_로짓, axis=1)\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    return accuracy.compute(predictions=인공지능의예측, references=실제정답)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `평가하기` 함수는 `eval_dataset`에 적용됨 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. 트레이너 세부지침"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "트레이너세부지침 = transformers.TrainingArguments(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2, # 전체문제세트를 2번 공부하라..\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\", \n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note} \n",
    "\n",
    "### 옵션 설명:\n",
    "\n",
    "1. **`output_dir=\"my_awesome_model\"`**:\n",
    "   - 학습된 모델과 관련 파일(예: 체크포인트, 로그 등)이 저장될 디렉토리 경로를 지정합니다.\n",
    "\n",
    "2. **`learning_rate=2e-5`**:\n",
    "   - 학습률(learning rate)을 설정합니다. 모델이 가중치를 업데이트할 때 사용하는 스텝 크기로, 작은 값일수록 학습 속도가 느려지지만 안정성이 높습니다.\n",
    "\n",
    "3. **`per_device_train_batch_size=16`**:\n",
    "   - **훈련** 시, 각 GPU 또는 CPU 디바이스에서 사용할 배치 크기를 설정합니다. 이 경우 각 디바이스에서 16개의 샘플을 한 번에 처리합니다.\n",
    "\n",
    "4. **`per_device_eval_batch_size=16`**:\n",
    "   - **평가** 시, 각 GPU 또는 CPU 디바이스에서 사용할 배치 크기를 설정합니다. 평가 시에는 훈련과 같은 배치 크기를 사용하는 것이 일반적입니다.\n",
    "\n",
    "5. **`num_train_epochs=2`**:\n",
    "   - 전체 훈련 데이터셋을 몇 번 반복(에포크)할지 설정합니다. 여기서는 데이터셋 전체를 2번 학습하게 됩니다.\n",
    "\n",
    "6. **`weight_decay=0.01`**:\n",
    "   - 가중치 감쇠(weight decay)를 적용하여 모델의 가중치가 지나치게 커지지 않도록 제어합니다. 0.01의 값은 가중치가 조금씩 감소하게 하여 모델의 일반화 성능을 높이는 데 도움을 줄 수 있습니다.\n",
    "\n",
    "7. **`eval_strategy=\"epoch\"`**:\n",
    "   - 평가 전략을 설정합니다. 여기서는 `epoch`으로 설정되어, 매 에포크가 끝날 때마다 평가가 진행됩니다.\n",
    "   - 다른 값으로는 `steps` (일정한 스텝마다 평가) 등이 있습니다.\n",
    "\n",
    "8. **`save_strategy=\"epoch\"`**:\n",
    "   - 모델을 언제 저장할지 설정합니다. `epoch`으로 설정되면, 매 에포크가 끝날 때마다 체크포인트를 저장합니다.\n",
    "   - 다른 값으로는 `steps` (일정한 스텝마다 저장) 등이 있습니다.\n",
    "\n",
    "9. **`load_best_model_at_end=True`**:\n",
    "   - 학습이 끝난 후, 가장 성능이 좋았던 체크포인트를 불러옵니다. 평가 지표에 따라 가장 성능이 좋았던 모델을 자동으로 불러와 최종 모델로 사용하게 됩니다.\n",
    "\n",
    "10. **`push_to_hub=False`**:\n",
    "    - 모델 학습이 끝난 후 Hugging Face Hub에 모델을 업로드할지 여부를 설정합니다. `False`로 설정하면 업로드하지 않습니다.\n",
    "    - `True`로 설정하면 모델과 관련된 파일들이 Hugging Face Hub에 업로드되어 다른 사람들과 공유할 수 있습니다.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` lr = 학습률 = 답이 틀렸을 경우 혼내는 정도\n",
    "\n",
    "- 정확한 느낌은 경사하강법을 이해해야함. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. 트레이너"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 트레이너를 이용한 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3126' max='3126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3126/3126 13:10, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.221900</td>\n",
       "      <td>0.199376</td>\n",
       "      <td>0.923000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.145700</td>\n",
       "      <td>0.233206</td>\n",
       "      <td>0.931000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3126, training_loss=0.20330691871472223, metrics={'train_runtime': 790.8461, 'train_samples_per_second': 63.223, 'train_steps_per_second': 3.953, 'total_flos': 6556904415524352.0, 'train_loss': 0.20330691871472223, 'epoch': 2.0})"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "트레이너 = transformers.Trainer(\n",
    "    model=인공지능,\n",
    "    args=트레이너세부지침,\n",
    "    train_dataset=전처리된데이터['train'],\n",
    "    eval_dataset=전처리된데이터['test'],\n",
    "    tokenizer=토크나이저, # 왜 필요하지??\n",
    "    data_collator=데이터콜렉터,\n",
    "    compute_metrics=평가하기,\n",
    ")\n",
    "트레이너.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1562.5"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25000 / 16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3126"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1563 * 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 인공지능이 똑똑해졌을까? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0230,  0.0279,  0.0239,  ...,  0.0085, -0.0062, -0.0143],\n",
       "        [ 0.0084,  0.0007, -0.0097,  ...,  0.0189, -0.0008,  0.0304]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "인공지능.classifier.weight # 숫자들은 바뀌어 있음.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0133,  0.0017,  0.0303,  ...,  0.0005,  0.0322, -0.0122],\n",
       "        [-0.0038,  0.0010,  0.0165,  ..., -0.0163, -0.0115, -0.0138],\n",
       "        [ 0.0033, -0.0210, -0.0019,  ...,  0.0014,  0.0100, -0.0486],\n",
       "        ...,\n",
       "        [-0.0043, -0.0345,  0.0094,  ...,  0.0350, -0.0182, -0.0110],\n",
       "        [ 0.0078, -0.0360, -0.0012,  ...,  0.0021,  0.0170, -0.0120],\n",
       "        [-0.0162,  0.0133, -0.0184,  ..., -0.0165,  0.0240,  0.0200]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "인공지능.pre_classifier.weight # 숫자들이 바뀌어 있음.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "인공지능.to(\"cpu\") # 인공지능이 지금 GPU에서 학습했거든요.. 다시 CPU로 내려주는 작업이 필요합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 입력정보에 영화에 대한 부정적 평가를 넣는다면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2023,  3185,  2001,  1037,  4121, 10520,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "입력정보_원시텍스트 = \"This movie was a huge disappointment.\"\n",
    "정리된숫자_토큰화된자료 = 토크나이저(입력정보_원시텍스트,truncation=True,return_tensors='pt')\n",
    "정리된숫자_토큰화된자료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.2626, -2.1934]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "인공지능(**정리된숫자_토큰화된자료)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.262645 , -2.1934297]], dtype=float32)"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "계산된숫자_로짓 = 인공지능(**정리된숫자_토큰화된자료).logits.detach().numpy()\n",
    "계산된숫자_로짓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9885254 , 0.01147464]], dtype=float32)"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "출력정보_확률 = np.exp(계산된숫자_로짓) / np.exp(계산된숫자_로짓).sum()\n",
    "출력정보_확률 # 0일확률, 1일확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "출력정보_확률.argmax() # 부정적 영화평가에 대한 인공지능의 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 입력정보에 영화에 대한 긍정적 평가를 넣는다면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2023,  2001,  1037, 17743,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "입력정보_원시텍스트 = \"This was a masterpiece.\"\n",
    "정리된숫자_토큰화된자료 = 토크나이저(입력정보_원시텍스트,truncation=True,return_tensors='pt')\n",
    "정리된숫자_토큰화된자료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1035,  1.6938]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "인공지능(**정리된숫자_토큰화된자료)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.103469 ,  1.6938232]], dtype=float32)"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "계산된숫자_로짓 = 인공지능(**정리된숫자_토큰화된자료).logits.detach().numpy()\n",
    "계산된숫자_로짓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0219393, 0.9780607]], dtype=float32)"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "출력정보_확률 = np.exp(계산된숫자_로짓) / np.exp(계산된숫자_로짓).sum()\n",
    "출력정보_확률 # 0일확률, 1일확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "출력정보_확률.argmax()\n",
    "계산된숫자_로짓.argmax() # 부정적 영화평가에 대한 인공지능의 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 우리가 가져야할 생각: 신기하다 X // 노가다 많이 했구나.. O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. 파이프라인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 강인공지능? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ref: <https://zdnet.co.kr/view/?no=20160622145838>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.9885253310203552}]\n",
      "[{'label': 'LABEL_1', 'score': 0.978060781955719}]\n"
     ]
    }
   ],
   "source": [
    "강인공지능 = transformers.pipeline(\"sentiment-analysis\", model=\"my_awesome_model/checkpoint-1563\")\n",
    "print(강인공지능(\"This movie was a huge disappointment.\"))\n",
    "print(강인공지능(\"This was a masterpiece.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
